{"0": {"Title": "1.1.1. Ordinary Least Squares", "Text": "LinearRegression fits a linear model with coefficients w=(w1,...,wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Mathematically it solves a problem of the form:\nLinearRegression will take in its fit method arrays X, y and will store the coefficients w of the linear model in its coef_ member:\nThe coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix X have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design.\nThe least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape (n_samples, n_features) this method has a cost of O(nsamplesn\n2\nfeatures\n)\n, assuming that nsamples\u2265nfeatures\n.\nThis method has the same order of complexity as Ordinary Least Squares.\nThe alpha parameter controls the degree of sparsity of the estimated coefficients.\nscikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV. LassoLarsCV is based on the Least Angle Regression algorithm explained below.\nFor high-dimensional datasets with many collinear features, LassoCV is most often preferable. However, LassoLarsCV has the advantage of exploring more relevant values of alpha parameter, and if the number of samples is very small compared to the number of features, it is often faster than LassoCV.\n\nThe algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one\u2019s correlations with the residual.\nInstead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the \u21131\nnorm of the parameter vector. The full coefficients path is stored in the array coef_path_, which has size (n_features, max_features+1). The first column is always zero.\nBayesianRidge estimates a probabilistic model of the regression problem as described above. The prior for the coefficient w\nis given by a spherical Gaussian:\nThe priors over \u03b1\nand \u03bb\nare chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical Ridge.\nThe parameters w\n, \u03b1\nand \u03bb\nare estimated jointly during the fit of the model, the regularization parameters \u03b1\nand \u03bb\nbeing estimated by maximizing the log marginal likelihood. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \u03b1\nand \u03bb\nis done as suggested in (MacKay, 1992).\nThe remaining hyperparameters are the parameters \u03b11\n, \u03b12\n, \u03bb1\nand \u03bb2\nof the gamma priors over \u03b1\nand \u03bb\n. These are usually chosen to be non-informative. By default \u03b11=\u03b12=\u03bb1=\u03bb2=10\u22126\n.\nBayesian Ridge Regression is used for regression:\nAfter being fitted, the model can then be used to predict new values:\nThe coefficients w\nof the model can be accessed:\nDue to the Bayesian framework, the weights found are slightly different to the ones found by Ordinary Least Squares. However, Bayesian Ridge Regression is more robust to ill-posed problems.\nThere are different things to keep in mind when dealing with data corrupted by outliers:\nAn important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data.\nNote that in general, robust fitting in high-dimensional setting (large n_features) is very hard. The robust models here will probably not work in these settings.\nEach iteration performs the following steps:\nThese steps are performed either a maximum number of times (max_trials) or until one of the special stop criteria are met (see stop_n_inliers and stop_score). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.\nThe is_data_valid and is_model_valid functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, is_data_valid should be used as it is called prior to fitting the model and thus leading to better computational performance.\nTheilSenRegressor is comparable to the Ordinary Least Squares (OLS) in terms of asymptotic efficiency and as an unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric method which means it makes no assumption about the underlying distribution of the data. Since Theil-Sen is a median-based estimator, it is more robust against corrupted data aka outliers. In univariate setting, Theil-Sen has a breakdown point of about 29.3% in case of a simple linear regression which means that it can tolerate arbitrary corrupted data of up to 29.3%.\nThe implementation of TheilSenRegressor in scikit-learn follows a generalization to a multivariate linear regression model [10] using the spatial median which is a generalization of the median to multiple dimensions [11].\nIn terms of time and space complexity, Theil-Sen scales according to\nwhich makes it infeasible to be applied exhaustively to problems with a large number of samples and features. Therefore, the magnitude of a subpopulation can be chosen to limit the time and space complexity by considering only a random subset of all possible combinations.\n", "Code_snippet": ">>> from sklearn import linear_model\n>>> reg = linear_model.LinearRegression()\n>>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n...                                       \nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n                 normalize=False)\n>>> reg.coef_\narray([0.5, 0.5])>>> from sklearn import linear_model\n>>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n>>> Y = [0., 1., 2., 3.]\n>>> reg = linear_model.BayesianRidge()\n>>> reg.fit(X, Y)  \nBayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n       normalize=False, tol=0.001, verbose=False)>>> reg.predict([[1, 0.]])\narray([0.50000013])>>> reg.coef_\narray([0.49999993, 0.49999993])", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_bayesian_ridge_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_theilsen_0011.png"]}, "Title": "Getting Data In/Out", "Text": "Reading from an excel file.", "Code_snippet": "In [148]: pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])\nOut[148]: \n    Unnamed: 0          A          B         C          D\n0   2000-01-01   0.266457  -0.399641 -0.219582   1.186860\n1   2000-01-02  -1.170732  -0.345873  1.653061  -0.282953\n2   2000-01-03  -1.734933   0.530468  2.060811  -0.515536\n3   2000-01-04  -1.555121   1.452620  0.239859  -1.156896\n4   2000-01-05   0.578117   0.511371  0.103552  -2.428202\n5   2000-01-06   0.478344   0.449933 -0.741620  -1.962409\n6   2000-01-07   1.235339  -0.091757 -1.543861  -1.084753\n..         ...        ...        ...       ...        ...\n993 2002-09-20 -10.628548  -9.153563 -7.883146  28.313940\n994 2002-09-21 -10.390377  -8.727491 -6.399645  30.914107\n995 2002-09-22  -8.985362  -8.485624 -4.669462  31.367740\n996 2002-09-23  -9.558560  -8.781216 -4.499815  30.518439\n997 2002-09-24  -9.902058  -9.340490 -4.386639  30.105593\n998 2002-09-25 -10.216020  -9.480682 -3.933802  29.758560\n999 2002-09-26 -11.856774 -10.671012 -3.216025  29.369368\n\n[1000 rows x 5 columns]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": [], "1": {"Title": "1.1.2. Ridge Regression", "Text": "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares:\nThe complexity parameter\n\u03b1\ncontrols the amount of shrinkage: the larger the value of\n\u03b1\n, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\nAs with other linear models, Ridge will take in its fit method arrays X, y and will store the coefficients\nw\nof the linear model in its coef_ member:\nRidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation:\nSpecifying the value of the cv attribute will trigger the use of cross-validation with GridSearchCV, for example cv=10 for 10-fold cross-validation, rather than Generalized Cross-Validation.\nAlternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).\nARDRegression is very similar to Bayesian Ridge Regression, but can lead to sparser coefficients\nw\n[1] [2]. ARDRegression poses a different prior over\nw\n, by dropping the assumption of the Gaussian being spherical.\nInstead, the distribution over\nw\nis assumed to be an axis-parallel, elliptical Gaussian distribution.\nThis means each coefficient\nw\nis drawn from a Gaussian distribution, centered on zero and with a precision\n\u03bb\n:\nwith\ndiag\n.\nIn contrast to Bayesian Ridge Regression, each coordinate of\nw\nhas its own standard deviation\n\u03bb\n. The prior over all\n\u03bb\nis chosen to be the same gamma distribution given by hyperparameters\n\u03bb\nand\n\u03bb\n.\nARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine [3] [4].\nRANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers from the complete data set.\nRANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations (see max_trials parameter). It is typically used for linear and non-linear regression problems and is especially popular in the field of photogrammetric computer vision.\nThe algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.\n", "Code_snippet": ">>> from sklearn import linear_model\n>>> reg = linear_model.Ridge(alpha=.5)\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) \nRidge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n>>> reg.coef_\narray([0.34545455, 0.34545455])\n>>> reg.intercept_ \n0.13636...>>> import numpy as np\n>>> from sklearn import linear_model\n>>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       \nRidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n        cv=None, fit_intercept=True, gcv_mode=None, normalize=False,\n        scoring=None, store_cv_values=False)\n>>> reg.alpha_\n0.01", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ard_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ransac_0011.png"]}, "2": {"Title": "1.1.3. Lasso", "Text": "The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero coefficients (see Compressive sensing: tomography reconstruction with L1 prior (Lasso)).\nMathematically, it consists of a linear model with an added regularization term. The objective function to minimize is:\nThe lasso estimate thus solves the minimization of the least-squares penalty with\n\u03b1\nadded, where\n\u03b1\nis a constant and\n|\nis the\n\u2113\n-norm of the coefficient vector.\nThe implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. See Least Angle Regression for another implementation:\nThe function lasso_path is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.\nThe following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.\nThe equivalence between alpha and the regularization parameter of SVM, C is given by alpha = 1 / C or alpha = 1 / (n_samples * C), depending on the estimator and the exact objective function optimized by the model.\nThe TheilSenRegressor estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It loses its robustness properties and becomes no better than an ordinary least squares in high dimension.\n", "Code_snippet": ">>> from sklearn import linear_model\n>>> reg = linear_model.Lasso(alpha=0.1)\n>>> reg.fit([[0, 0], [1, 1]], [0, 1])  \nLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\n>>> reg.predict([[1, 1]])\narray([0.8])", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_model_selection_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_theilsen_0011.png"]}, "3": {"Title": "1.1.4. Multi-task Lasso", "Text": "The MultiTaskLasso is a linear model that estimates sparse coefficients for multiple regression problems jointly: y is a 2D array, of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks.\nThe following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.\n\nFitting a time-series model, imposing that any active feature be active at all times.\nMathematically, it consists of a linear model trained with a mixed\n\u2113\n\u2113\n-norm for regularization. The objective function to minimize is:\nwhere\nFro\nindicates the Frobenius norm\nand\n\u2113\n\u2113\nreads\nThe implementation in the class MultiTaskLasso uses coordinate descent as the algorithm to fit the coefficients.\nThe HuberRegressor is different to Ridge because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from TheilSenRegressor and RANSACRegressor because it does not ignore the effect of the outliers but gives a lesser weight to them.\nThe loss function that HuberRegressor minimizes is given by\nwhere\nIt is advised to set the parameter epsilon to 1.35 to achieve 95% statistical efficiency.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_multi_task_lasso_support_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_huber_vs_ridge_001.png"]}, "4": {"Title": "1.1.5. Elastic-Net", "Text": "ElasticNet is a linear regression model trained with both\n\u2113\nand\n\u2113\n-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of\n\u2113\nand\n\u2113\nusing the l1_ratio parameter.\nElastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\nA practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.\nThe objective function to minimize is in this case\nThe class ElasticNetCV can be used to set the parameters alpha (\n\u03b1\n) and l1_ratio (\n\u03c1\n) by cross-validation.\nThe following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.\nThe HuberRegressor differs from using SGDRegressor with loss set to huber in the following ways.\nNote that this estimator is different from the R implementation of Robust Regression (http://www.ats.ucla.edu/stat/r/dae/rreg.htm) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_coordinate_descent_path_0011.png"]}, "5": {"Title": "1.1.6. Multi-task Elastic-Net", "Text": "The MultiTaskElasticNet is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: Y is a 2D array of shape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks.\nMathematically, it consists of a linear model trained with a mixed\n\u2113\n\u2113\n-norm and\n\u2113\n-norm for regularization. The objective function to minimize is:\nThe implementation in the class MultiTaskElasticNet uses coordinate descent as the algorithm to fit the coefficients.\nThe class MultiTaskElasticNetCV can be used to set the parameters alpha (\n\u03b1\n) and l1_ratio (\n\u03c1\n) by cross-validation.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "6": {"Title": "1.1.7. Least Angle Regression", "Text": "Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.\nThe advantages of LARS are:\nThe disadvantages of the LARS method include:\nThe LARS model can be used using estimator Lars, or its low-level implementation lars_path or lars_path_gram.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "7": {"Title": "1.1.8. LARS Lasso", "Text": "LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients.\nThe Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions lars_path or lars_path_gram.\n", "Code_snippet": ">>> from sklearn import linear_model\n>>> reg = linear_model.LassoLars(alpha=.1)\n>>> reg.fit([[0, 0], [1, 1]], [0, 1])  \nLassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,\n     fit_path=True, max_iter=500, normalize=True, positive=False,\n     precompute='auto', verbose=False)\n>>> reg.coef_    \narray([0.717157..., 0.        ])", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lasso_lars_0011.png"]}, "8": {"Title": "1.1.9. Orthogonal Matching Pursuit (OMP)", "Text": "OrthogonalMatchingPursuit and orthogonal_mp implements the OMP algorithm for approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (ie. the\n\u2113\npseudo-norm).\nBeing a forward feature selection method like Least Angle Regression, orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements:\nAlternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as:\nOMP is based on a greedy algorithm that includes at each step the atom most highly correlated with the current residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is recomputed using an orthogonal projection on the space of the previously chosen dictionary elements.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "9": {"Title": "1.1.10. Bayesian Regression", "Text": "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.\nThis can be done by introducing uninformative priors over the hyper parameters of the model. The\n\u2113\nregularization used in Ridge Regression is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients\nw\nwith precision\n\u03bb\n. Instead of setting lambda manually, it is possible to treat it as a random variable to be estimated from the data.\nTo obtain a fully probabilistic model, the output\ny\nis assumed to be Gaussian distributed around\nX\n:\nwhere\n\u03b1\nis again treated as a random variable that is to be estimated from the data.\nThe advantages of Bayesian Regression are:\nThe disadvantages of Bayesian regression include:\n", "Code_snippet": ">>> from sklearn import linear_model\n>>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n>>> Y = [0., 1., 2., 3.]\n>>> reg = linear_model.BayesianRidge()\n>>> reg.fit(X, Y)  \nBayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n       normalize=False, tol=0.001, verbose=False)>>> reg.predict([[1, 0.]])\narray([0.50000013])>>> reg.coef_\narray([0.49999993, 0.49999993])", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_bayesian_ridge_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ard_0011.png"]}, "10": {"Title": "1.1.11. Logistic regression", "Text": "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\nLogistic regression is implemented in LogisticRegression. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional\n\u2113\n,\n\u2113\nor Elastic-Net regularization. Note that regularization is applied by default.\nAs an optimization problem, binary class\n\u2113\npenalized logistic regression minimizes the following cost function:\nSimilarly,\n\u2113\nregularized logistic regression solves the following optimization problem:\nElastic-Net regularization is a combination of\n\u2113\nand\n\u2113\n, and minimizes the following cost function:\nwhere\n\u03c1\ncontrols the strength of\n\u2113\nregularization vs.\n\u2113\nregularization (it corresponds to the l1_ratio parameter).\nNote that, in this notation, it\u2019s assumed that the target\ny\ntakes values in the set\n\u2212\nat trial\ni\n. We can also see that Elastic-Net is equivalent to\n\u2113\nwhen\n\u03c1\nand equivalent to\n\u2113\nwhen\n\u03c1\n.\nThe solvers implemented in the class LogisticRegression are \u201cliblinear\u201d, \u201cnewton-cg\u201d, \u201clbfgs\u201d, \u201csag\u201d and \u201csaga\u201d:\nThe solver \u201cliblinear\u201d uses a coordinate descent (CD) algorithm, and relies on the excellent C++ LIBLINEAR library, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a \u201cone-vs-rest\u201d fashion so separate binary classifiers are trained for all classes. This happens under the hood, so LogisticRegression instances using this solver behave as multiclass classifiers. For\n\u2113\nregularization sklearn.svm.l1_min_c allows to calculate the lower bound for C in order to get a non \u201cnull\u201d (all feature weights to zero) model.\nThe \u201clbfgs\u201d, \u201csag\u201d and \u201cnewton-cg\u201d solvers only support\n\u2113\nregularization or no regularization, and are found to converge faster for some high-dimensional data. Setting multi_class to \u201cmultinomial\u201d with these solvers learns a true multinomial logistic regression model [5], which means that its probability estimates should be better calibrated than the default \u201cone-vs-rest\u201d setting.\nThe \u201csag\u201d solver uses Stochastic Average Gradient descent [6]. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\nThe \u201csaga\u201d solver [7] is a variant of \u201csag\u201d that also supports the non-smooth penalty=\"l1\". This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports penalty=\"elasticnet\".\nThe \u201clbfgs\u201d is an optimization algorithm that approximates the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm [8], which belongs to quasi-Newton methods. The \u201clbfgs\u201d solver is recommended for use for small data-sets but for larger datasets its performance suffers. [9]\nThe following table summarizes the penalties supported by each solver:\nThe \u201clbfgs\u201d solver is used by default for its robustness. For large datasets the \u201csaga\u201d solver is usually faster. For large dataset, you may also consider using SGDClassifier with \u2018log\u2019 loss, which might be even faster but requires more tuning.\nLogisticRegressionCV implements Logistic Regression with built-in cross-validation support, to find the optimal C and l1_ratio parameters according to the scoring attribute. The \u201cnewton-cg\u201d, \u201csag\u201d, \u201csaga\u201d and \u201clbfgs\u201d solvers are found to be faster for high-dimensional dense data, due to warm-starting (see Glossary).\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "11": {"Title": "1.1.12. Stochastic Gradient Descent - SGD", "Text": "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The partial_fit method allows online/out-of-core learning.\nThe classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with loss=\"log\", SGDClassifier fits a logistic regression model, while with loss=\"hinge\" it fits a linear support vector machine (SVM).\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "12": {"Title": "1.1.13. Perceptron", "Text": "The Perceptron is another simple classification algorithm suitable for large scale learning. By default:\nThe last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "13": {"Title": "1.1.14. Passive Aggressive Algorithms", "Text": "The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C.\nFor classification, PassiveAggressiveClassifier can be used with loss='hinge' (PA-I) or loss='squared_hinge' (PA-II). For regression, PassiveAggressiveRegressor can be used with loss='epsilon_insensitive' (PA-I) or loss='squared_epsilon_insensitive' (PA-II).\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": []}, "14": {"Title": "1.1.15. Robustness regression: outliers and modeling errors", "Text": "Robust regression aims to fit a regression model in the presence of corrupt data: either outliers, or error in the model.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_theilsen_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_fit_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ransac_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_theilsen_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_huber_vs_ridge_001.png"]}, "15": {"Title": "1.1.16. Polynomial regression: extending linear models with basis functions", "Text": "One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\nFor example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\nIf we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\nThe (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new set of features\nWith this re-labeling of the data, our problem can be written\nWe see that the resulting polynomial regression is in the same class of linear models we considered above (i.e. the model is linear in\nw\n) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\nHere is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:\nThis figure is created using the PolynomialFeatures transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:\nThe features of X have been transformed from\n[\nto\n[\n, and can now be used within any linear model.\nThis sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial regression can be created and used as follows:\nThe linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.\nIn some cases it\u2019s not necessary to include higher powers of any single feature, but only the so-called interaction features that multiply together at most\nd\ndistinct features. These can be gotten from PolynomialFeatures with the setting interaction_only=True.\nFor example, when dealing with boolean features,\nx\nfor all\nn\nand is therefore useless; but\nx\nrepresents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:\nAnd the classifier \u201cpredictions\u201d are perfect:\n", "Code_snippet": ">>> from sklearn.preprocessing import PolynomialFeatures\n>>> import numpy as np\n>>> X = np.arange(6).reshape(3, 2)\n>>> X\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(degree=2)\n>>> poly.fit_transform(X)\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])>>> from sklearn.preprocessing import PolynomialFeatures\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.pipeline import Pipeline\n>>> import numpy as np\n>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),\n...                   ('linear', LinearRegression(fit_intercept=False))])\n>>> # fit to an order-3 polynomial data\n>>> x = np.arange(5)\n>>> y = 3 - 2 * x + x ** 2 - x ** 3\n>>> model = model.fit(x[:, np.newaxis], y)\n>>> model.named_steps['linear'].coef_\narray([ 3., -2.,  1., -1.])>>> from sklearn.linear_model import Perceptron\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> import numpy as np\n>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n>>> y = X[:, 0] ^ X[:, 1]\n>>> y\narray([0, 1, 1, 0])\n>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\n>>> X\narray([[1, 0, 0, 0],\n       [1, 0, 1, 0],\n       [1, 1, 0, 0],\n       [1, 1, 1, 1]])\n>>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\n...                  shuffle=False).fit(X, y)>>> clf.predict(X)\narray([0, 1, 1, 0])\n>>> clf.score(X, y)\n1.0", "Url": "https://scikit-learn.org/stable/modules/linear_model.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_polynomial_interpolation_0011.png"]}, "16": {"Title": "1.2.1. Dimensionality reduction using Linear Discriminant Analysis", "Text": "discriminant_analysis.LinearDiscriminantAnalysis can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is, in general, a rather strong dimensionality reduction, and only makes sense in a multiclass setting.\nThis is implemented in discriminant_analysis.LinearDiscriminantAnalysis.transform. The desired dimensionality can be set using the n_components constructor parameter. This parameter has no influence on discriminant_analysis.LinearDiscriminantAnalysis.fit or discriminant_analysis.LinearDiscriminantAnalysis.predict.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/lda_qda.html", "Attachment_Url": []}, "17": {"Title": "1.2.2. Mathematical formulation of the LDA and QDA classifiers", "Text": "Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data\nP\nfor each class\nk\n. Predictions can then be obtained by using Bayes\u2019 rule:\nand we select the class\nk\nwhich maximizes this conditional probability.\nMore specifically, for linear and quadratic discriminant analysis,\nP\nis modeled as a multivariate Gaussian distribution with density:\nwhere\nd\nis the number of features.\nTo use this model as a classifier, we just need to estimate from the training data the class priors\nP\n(by the proportion of instances of class\nk\n), the class means\n\u03bc\n(by the empirical sample class means) and the covariance matrices (either by the empirical sample class covariance matrices, or by a regularized estimator: see the section on shrinkage below).\nIn the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix:\n\u03a3\nfor all\nk\n. This leads to linear decision surfaces, which can be seen by comparing the log-probability ratios\nlog\n:\nIn the case of QDA, there are no assumptions on the covariance matrices\n\u03a3\nof the Gaussians, leading to quadratic decision surfaces. See [3] for more details.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/lda_qda.html", "Attachment_Url": []}, "18": {"Title": "1.2.3. Mathematical formulation of LDA dimensionality reduction", "Text": "To understand the use of LDA in dimensionality reduction, it is useful to start with a geometric reformulation of the LDA classification rule explained above. We write\nK\nfor the total number of target classes. Since in LDA we assume that all classes have the same estimated covariance\n\u03a3\n, we can rescale the data so that this covariance is the identity:\nThen one can show that to classify a data point after scaling is equivalent to finding the estimated class mean\n\u03bc\nwhich is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the\nK\naffine subspace\nH\ngenerated by all the\n\u03bc\nfor all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a\nK\ndimensional space.\nWe can reduce the dimension even more, to a chosen\nL\n, by projecting onto the linear subspace\nH\nwhich maximizes the variance of the\n\u03bc\nafter projection (in effect, we are doing a form of PCA for the transformed class means\n\u03bc\n). This\nL\ncorresponds to the n_components parameter used in the discriminant_analysis.LinearDiscriminantAnalysis.transform method. See [3] for more details.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/lda_qda.html", "Attachment_Url": []}, "19": {"Title": "1.2.4. Shrinkage", "Text": "Shrinkage is a tool to improve estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator. Shrinkage LDA can be used by setting the shrinkage parameter of the discriminant_analysis.LinearDiscriminantAnalysis class to \u2018auto\u2019. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf [4]. Note that currently shrinkage only works when setting the solver parameter to \u2018lsqr\u2019 or \u2018eigen\u2019.\nThe shrinkage parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.\n\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/lda_qda.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_0011.png"]}, "20": {"Title": "1.2.5. Estimation algorithms", "Text": "The default solver is \u2018svd\u2019. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the \u2018svd\u2019 solver cannot be used with shrinkage.\nThe \u2018lsqr\u2019 solver is an efficient algorithm that only works for classification. It supports shrinkage.\nThe \u2018eigen\u2019 solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the \u2018eigen\u2019 solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/lda_qda.html", "Attachment_Url": []}, "21": {"Title": "1.4.1. Classification", "Text": "SVC, NuSVC and LinearSVC are classes capable of performing multi-class classification on a dataset.\nSVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). On the other hand, LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept keyword kernel, as this is assumed to be linear. It also lacks some of the members of SVC and NuSVC, like support_.\nAs other classifiers, SVC, NuSVC and LinearSVC take as input two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array y of class labels (strings or integers), size [n_samples]:\nAfter being fitted, the model can then be used to predict new values:\nSVMs decision function depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in members support_vectors_, support_ and n_support:\nSVC and NuSVC implement the \u201cone-against-one\u201d approach (Knerr et al., 1990) for multi- class classification. If n_class is the number of classes, then n_class * (n_class - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the decision_function_shape option allows to monotically transform the results of the \u201cone-against-one\u201d classifiers to a decision function of shape (n_samples, n_classes).\nOn the other hand, LinearSVC implements \u201cone-vs-the-rest\u201d multi-class strategy, thus training n_class models. If there are only two classes, only one model is trained:\nSee Mathematical formulation for a complete description of the decision function.\nNote that the LinearSVC also implements an alternative multi-class strategy, the so-called multi-class SVM formulated by Crammer and Singer, by using the option multi_class='crammer_singer'. This method is consistent, which is not true for one-vs-rest classification. In practice, one-vs-rest classification is usually preferred, since the results are mostly similar, but the runtime is significantly less.\nFor \u201cone-vs-rest\u201d LinearSVC the attributes coef_ and intercept_ have the shape [n_class, n_features] and [n_class] respectively. Each row of the coefficients corresponds to one of the n_class many \u201cone-vs-rest\u201d classifiers and similar for the intercepts, in the order of the \u201cone\u201d class.\nIn the case of \u201cone-vs-one\u201d SVC, the layout of the attributes is a little more involved. In the case of having a linear kernel, the attributes coef_ and intercept_ have the shape [n_class * (n_class - 1) / 2, n_features] and [n_class * (n_class - 1) / 2] respectively. This is similar to the layout for LinearSVC described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is \u201c0 vs 1\u201d, \u201c0 vs 2\u201d , \u2026 \u201c0 vs n\u201d, \u201c1 vs 2\u201d, \u201c1 vs 3\u201d, \u201c1 vs n\u201d, . . . \u201cn-1 vs n\u201d.\nThe shape of dual_coef_ is [n_class-1, n_SV] with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the n_class * (n_class - 1) / 2 \u201cone-vs-one\u201d classifiers. Each of the support vectors is used in n_class - 1 classifiers. The n_class - 1 entries in each row correspond to the dual coefficients for these classifiers.\nThis might be made more clear by an example:\nConsider a three class problem with class 0 having three support vectors\nv\nand class 1 and 2 having two support vectors\nv\nand\nv\nrespectively. For each support vector\nv\n, there are two dual coefficients. Let\u2019s call the coefficient of support vector\nv\nin the classifier between classes\ni\nand\nk\n\u03b1\n. Then dual_coef_ looks like this:\nYou can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix.\nClassifiers with custom kernels behave the same way as any other classifiers, except that:\nYou can also use your own defined kernels by passing a function to the keyword kernel in the constructor.\nYour kernel must take as arguments two matrices of shape (n_samples_1, n_features), (n_samples_2, n_features) and return a kernel matrix of shape (n_samples_1, n_samples_2).\nThe following code defines a linear kernel and creates a classifier instance that will use that kernel:\nGiven training vectors\nx\n, i=1,\u2026, n, in two classes, and a vector\ny\n, SVC solves the following primal problem:\nIts dual is\nwhere\ne\nis the vector of all ones,\nC\nis the upper bound,\nQ\nis an\nn\nby\nn\npositive semidefinite matrix,\nQ\n, where\nK\nis the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function\n\u03d5\n.\nThe decision function is:\nThis parameters can be accessed through the members dual_coef_ which holds the product\ny\n, support_vectors_ which holds the support vectors, and intercept_ which holds the independent term\n\u03c1\n:\n", "Code_snippet": ">>> from sklearn import svm\n>>> X = [[0, 0], [1, 1]]\n>>> y = [0, 1]\n>>> clf = svm.SVC(gamma='scale')\n>>> clf.fit(X, y)  \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)>>> clf.predict([[2., 2.]])\narray([1])>>> # get support vectors\n>>> clf.support_vectors_\narray([[0., 0.],\n       [1., 1.]])\n>>> # get indices of support vectors\n>>> clf.support_ \narray([0, 1]...)\n>>> # get number of support vectors for each class\n>>> clf.n_support_ \narray([1, 1]...)>>> X = [[0], [1], [2], [3]]\n>>> Y = [0, 1, 2, 3]\n>>> clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n>>> clf.fit(X, Y) \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\n>>> dec = clf.decision_function([[1]])\n>>> dec.shape[1] # 4 classes: 4*3/2 = 6\n6\n>>> clf.decision_function_shape = \"ovr\"\n>>> dec = clf.decision_function([[1]])\n>>> dec.shape[1] # 4 classes\n4>>> lin_clf = svm.LinearSVC()\n>>> lin_clf.fit(X, Y) \nLinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)\n>>> dec = lin_clf.decision_function([[1]])\n>>> dec.shape[1]\n4>>> import numpy as np\n>>> from sklearn import svm\n>>> def my_kernel(X, Y):\n...     return np.dot(X, Y.T)\n...\n>>> clf = svm.SVC(kernel=my_kernel)>>> import numpy as np\n>>> from sklearn import svm\n>>> X = np.array([[0, 0], [1, 1]])\n>>> y = [0, 1]\n>>> clf = svm.SVC(kernel='precomputed')\n>>> # linear kernel computation\n>>> gram = np.dot(X, X.T)\n>>> clf.fit(gram, y) \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n    kernel='precomputed', max_iter=-1, probability=False,\n    random_state=None, shrinking=True, tol=0.001, verbose=False)\n>>> # predict on training examples\n>>> clf.predict(gram)\narray([0, 1])", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_unbalanced_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_weighted_samples_0011.png"]}, "22": {"Title": "1.4.2. Regression", "Text": "The decision_function method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled. In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM\u2019s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per Wu et al. (2004).\nNeedless to say, the cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores, in the sense that the \u201cargmax\u201d of the scores may not be the argmax of the probabilities. (E.g., in binary classification, a sample may be labeled by predict as belonging to a class that has probability <\u00bd according to predict_proba.) Platt\u2019s method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set probability=False and use decision_function instead of predict_proba.\nThe method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.\nThe model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.\nThere are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers linear kernels, while NuSVR implements a slightly different formulation than SVR and LinearSVR. See Implementation details for further details.\nAs with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:\nSet kernel='precomputed' and pass the Gram matrix instead of X in the fit method. At the moment, the kernel values between all training vectors and the test vectors must be provided.\nWe introduce a new parameter\n\u03bd\nwhich controls the number of support vectors and training errors. The parameter\n\u03bd\nis an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.\nIt can be shown that the\n\u03bd\n-SVC formulation is a reparameterization of the\nC\n-SVC and therefore mathematically equivalent.\n", "Code_snippet": ">>> from sklearn import svm\n>>> X = [[0, 0], [2, 2]]\n>>> y = [0.5, 2.5]\n>>> clf = svm.SVR()\n>>> clf.fit(X, y) \nSVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n    tol=0.001, verbose=False)\n>>> clf.predict([[1, 1]])\narray([1.5])>>> import numpy as np\n>>> from sklearn import svm\n>>> X = np.array([[0, 0], [1, 1]])\n>>> y = [0, 1]\n>>> clf = svm.SVC(kernel='precomputed')\n>>> # linear kernel computation\n>>> gram = np.dot(X, X.T)\n>>> clf.fit(gram, y) \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n    kernel='precomputed', max_iter=-1, probability=False,\n    random_state=None, shrinking=True, tol=0.001, verbose=False)\n>>> # predict on training examples\n>>> clf.predict(gram)\narray([0, 1])", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": []}, "23": {"Title": "1.4.3. Density estimation, novelty detection", "Text": "In problems where it is desired to give more importance to certain classes or certain individual samples keywords class_weight and sample_weight can be used.\nSVC (but not NuSVC) implement a keyword class_weight in the fit method. It\u2019s a dictionary of the form {class_label : value}, where value is a floating point number > 0 that sets the parameter C of class class_label to C * value.\nSVC, NuSVC, SVR, NuSVR and OneClassSVM implement also weights for individual samples in method fit through keyword sample_weight. Similar to class_weight, these set the parameter C for the i-th example to C * sample_weight[i].\nThe class OneClassSVM implements a One-Class SVM which is used in outlier detection.\nSee Novelty and Outlier Detection for the description and usage of OneClassSVM.\nWhen training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\nProper choice of C and gamma is critical to the SVM\u2019s performance. One is advised to use sklearn.model_selection.GridSearchCV with C and gamma spaced exponentially far apart to choose good values.\nGiven training vectors\nx\n, i=1,\u2026, n, and a vector\ny\n\u03b5\n-SVR solves the following primal problem:\nIts dual is\nwhere\ne\nis the vector of all ones,\nC\nis the upper bound,\nQ\nis an\nn\nby\nn\npositive semidefinite matrix,\nQ\nis the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function\n\u03d5\n.\nThe decision function is:\nThese parameters can be accessed through the members dual_coef_ which holds the difference\n\u03b1\n, support_vectors_ which holds the support vectors, and intercept_ which holds the independent term\n\u03c1\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_unbalanced_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_weighted_samples_0011.png"]}, "24": {"Title": "1.4.4. Complexity", "Text": "Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by this libsvm-based implementation scales between\nO\nand\nO\ndepending on how efficiently the libsvm cache is used in practice (dataset dependent). If the data is very sparse\nn\nshould be replaced by the average number of non-zero features in a sample vector.\nAlso note that for the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": []}, "25": {"Title": "1.4.5. Tips on Practical Use", "Text": "", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": []}, "26": {"Title": "1.4.6. Kernel functions", "Text": "The kernel function can be any of the following:\nDifferent kernels are specified by keyword kernel at initialization:\n", "Code_snippet": ">>> linear_svc = svm.SVC(kernel='linear')\n>>> linear_svc.kernel\n'linear'\n>>> rbf_svc = svm.SVC(kernel='rbf')\n>>> rbf_svc.kernel\n'rbf'>>> import numpy as np\n>>> from sklearn import svm\n>>> def my_kernel(X, Y):\n...     return np.dot(X, Y.T)\n...\n>>> clf = svm.SVC(kernel=my_kernel)>>> import numpy as np\n>>> from sklearn import svm\n>>> X = np.array([[0, 0], [1, 1]])\n>>> y = [0, 1]\n>>> clf = svm.SVC(kernel='precomputed')\n>>> # linear kernel computation\n>>> gram = np.dot(X, X.T)\n>>> clf.fit(gram, y) \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n    kernel='precomputed', max_iter=-1, probability=False,\n    random_state=None, shrinking=True, tol=0.001, verbose=False)\n>>> # predict on training examples\n>>> clf.predict(gram)\narray([0, 1])", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": []}, "27": {"Title": "1.4.7. Mathematical formulation", "Text": "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_0011.png"]}, "28": {"Title": "1.4.8. Implementation details", "Text": "Internally, we use libsvm and liblinear to handle all computations. These libraries are wrapped using C and Cython.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/svm.html", "Attachment_Url": []}, "29": {"Title": "1.5.1. Classification", "Text": "The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.\nAs other classifiers, SGD has to be fitted with two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:\nAfter being fitted, the model can then be used to predict new values:\nSGD fits a linear model to the training data. The member coef_ holds the model parameters:\nMember intercept_ holds the intercept (aka offset or bias):\nWhether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter fit_intercept.\nTo get the signed distance to the hyperplane use SGDClassifier.decision_function:\nThe concrete loss function can be set via the loss parameter. SGDClassifier supports the following loss functions:\nThe first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.\nUsing loss=\"log\" or loss=\"modified_huber\" enables the predict_proba method, which gives a vector of probability estimates\nP\nper sample\nx\n:\nThe concrete penalty can be set via the penalty parameter. SGD supports the following penalties:\nThe default setting is penalty=\"l2\". The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter l1_ratio controls the convex combination of L1 and L2 penalty.\nSGDClassifier supports multi-class classification by combining multiple binary classifiers in a \u201cone versus all\u201d (OVA) scheme. For each of the\nK\nclasses, a binary classifier is learned that discriminates between that and all other\nK\nclasses. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers.\nIn the case of multi-class classification coef_ is a two-dimensional array of shape=[n_classes, n_features] and intercept_ is a one-dimensional array of shape=[n_classes]. The i-th row of coef_ holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute classes_). Note that, in principle, since they allow to create a probability model, loss=\"log\" and loss=\"modified_huber\" are more suitable for one-vs-all classification.\nSGDClassifier supports both weighted classes and weighted instances via the fit parameters class_weight and sample_weight. See the examples below and the docstring of SGDClassifier.fit for further information.\nSGDClassifier supports averaged SGD (ASGD). Averaging can be enabled by setting `average=True`. ASGD works by averaging the coefficients of the plain SGD over each iteration over a sample. When using ASGD the learning rate can be larger and even constant leading on some datasets to a speed up in training time.\nFor classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in LogisticRegression.\nStochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of\nE\nby considering a single training example at a time.\nThe class SGDClassifier implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by\nwhere\n\u03b7\nis the learning rate which controls the step-size in the parameter space. The intercept\nb\nis updated similarly but without regularization.\nThe learning rate\n\u03b7\ncan be either constant or gradually decaying. For classification, the default learning rate schedule (learning_rate='optimal') is given by\nwhere\nt\nis the time step (there are a total of n_samples * n_iter time steps),\nt\nis determined based on a heuristic proposed by L\u00e9on Bottou such that the expected initial updates are comparable with the expected size of the weights (this assuming that the norm of the training samples is approx. 1). The exact definition can be found in _init_t in BaseSGD.\nFor regression the default learning rate schedule is inverse scaling (learning_rate='invscaling'), given by\nwhere\ne\nand\np\nare hyperparameters chosen by the user via eta0 and power_t, resp.\nFor a constant learning rate use learning_rate='constant' and use eta0 to specify the learning rate.\nFor an adaptively decreasing learning rate, use learning_rate='adaptive' and use eta0 to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.\nThe model parameters can be accessed through the members coef_ and intercept_:\n", "Code_snippet": ">>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0., 0.], [1., 1.]]\n>>> y = [0, 1]\n>>> clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n>>> clf.fit(X, y)   \nSGDClassifier(alpha=0.0001, average=False, class_weight=None,\n           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n           random_state=None, shuffle=True, tol=0.001,\n           validation_fraction=0.1, verbose=0, warm_start=False)>>> clf.predict([[2., 2.]])\narray([1])>>> clf.coef_                                         \narray([[9.9..., 9.9...]])>>> clf.intercept_                                    \narray([-9.9...])>>> clf.decision_function([[2., 2.]])                 \narray([29.6...])>>> clf = SGDClassifier(loss=\"log\", max_iter=5).fit(X, y)\n>>> clf.predict_proba([[1., 1.]])                      \narray([[0.00..., 0.99...]])", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_iris_0011.png"]}, "30": {"Title": "1.5.2. Regression", "Text": "The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. SGDRegressor is well suited for regression problems with a large number of training samples (> 10.000), for other problems we recommend Ridge, Lasso, or ElasticNet.\nThe concrete loss function can be set via the loss parameter. SGDRegressor supports the following loss functions:\nThe Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter epsilon. This parameter depends on the scale of the target variables.\nSGDRegressor supports averaged SGD as SGDClassifier. Averaging can be enabled by setting `average=True`.\nFor regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in Ridge.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": []}, "31": {"Title": "1.5.3. Stochastic Gradient Descent for sparse data", "Text": "There is built-in support for sparse data given in any matrix in a format supported by scipy.sparse. For maximum efficiency, however, use the CSR matrix format as defined in scipy.sparse.csr_matrix.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": []}, "32": {"Title": "1.5.4. Complexity", "Text": "The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of\nO\n, where k is the number of iterations (epochs) and\np\nis the average number of non-zero attributes per sample.\nRecent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": []}, "33": {"Title": "1.5.5. Stopping criterion", "Text": "The classes SGDClassifier and SGDRegressor provide two criteria to stop the algorithm when a given level of convergence is reached:\nIn both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve n_iter_no_change times in a row. The improvement is evaluated with a tolerance tol, and the algorithm stops in any case after a maximum number of iteration max_iter.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": []}, "34": {"Title": "1.5.6. Tips on Practical Use", "Text": "", "Code_snippet": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)  # Don't cheat - fit only on training data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)  # apply same transformation to test data", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": []}, "35": {"Title": "1.5.7. Mathematical formulation", "Text": "Given a set of training examples\n(\nwhere\nx\nand\ny\n, our goal is to learn a linear scoring function\nf\nwith model parameters\nw\nand intercept\nb\n. In order to make predictions, we simply look at the sign of\nf\n. A common choice to find the model parameters is by minimizing the regularized training error given by\nwhere\nL\nis a loss function that measures model (mis)fit and\nR\nis a regularization term (aka penalty) that penalizes model complexity;\n\u03b1\nis a non-negative hyperparameter.\nDifferent choices for\nL\nentail different classifiers such as\nAll of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.\nPopular choices for the regularization term\nR\ninclude:\nThe Figure below shows the contours of the different regularization terms in the parameter space when\nR\n.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_loss_functions_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_penalties_0011.png"]}, "36": {"Title": "1.5.8. Implementation details", "Text": "The implementation of SGD is influenced by the Stochastic Gradient SVM of L\u00e9on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a \u201cone versus all\u201d approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/sgd.html", "Attachment_Url": []}, "37": {"Title": "1.6.1. Unsupervised Nearest Neighbors", "Text": "NearestNeighbors implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in sklearn.metrics.pairwise. The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute']. When the default value 'auto' is passed, the algorithm attempts to determine the best approach from the training data. For a discussion of the strengths and weaknesses of each option, see Nearest Neighbor Algorithms.\nFor the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within sklearn.neighbors can be used:\nBecause the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero.\nIt is also possible to efficiently produce a sparse graph showing the connections between neighboring points:\nThe dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see sklearn.manifold.Isomap, sklearn.manifold.LocallyLinearEmbedding, and sklearn.cluster.SpectralClustering.\nFast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for N samples in D dimensions, this approach scales as O[DN2]. Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples N grows, the brute-force approach quickly becomes infeasible. In the classes within sklearn.neighbors, brute-force neighbors searches are specified using the keyword algorithm = 'brute', and are computed using the routines available in sklearn.metrics.pairwise.\nThe NearestCentroid classifier has a shrink_threshold parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by shrink_threshold. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.\nIn the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.\n\nCombined with a nearest neighbors classifier (KNeighborsClassifier), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user.\nNCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries.\nTo use this model for classification, one needs to combine a NeighborhoodComponentsAnalysis instance that learns the optimal transformation with a KNeighborsClassifier instance that performs the classification in the projected space. Here is an example using the two classes:\n\nThe plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes.\nNCA can be seen as learning a (squared) Mahalanobis distance metric:\nwhere\nM\nis a symmetric positive semi-definite matrix of size (n_features, n_features).\nNCA stores a matrix of pairwise distances, taking n_samples ** 2 memory. Time complexity depends on the number of iterations done by the optimisation algorithm. However, one can set the maximum number of iterations with the argument max_iter. For each iteration, time complexity is O(n_components x n_samples x min(n_samples, n_features)).\n", "Code_snippet": ">>> from sklearn.neighbors import NearestNeighbors\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n>>> distances, indices = nbrs.kneighbors(X)\n>>> indices                                           \narray([[0, 1],\n       [1, 0],\n       [2, 1],\n       [3, 4],\n       [4, 3],\n       [5, 4]]...)\n>>> distances\narray([[0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.41421356],\n       [0.        , 1.        ],\n       [0.        , 1.        ],\n       [0.        , 1.41421356]])>>> nbrs.kneighbors_graph(X).toarray()\narray([[1., 1., 0., 0., 0., 0.],\n       [1., 1., 0., 0., 0., 0.],\n       [0., 1., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 1., 0.],\n       [0., 0., 0., 1., 1., 0.],\n       [0., 0., 0., 0., 1., 1.]])>>> from sklearn.neighbors import KDTree\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kdt = KDTree(X, leaf_size=30, metric='euclidean')\n>>> kdt.query(X, k=2, return_distance=False)          \narray([[0, 1],\n       [1, 0],\n       [2, 1],\n       [3, 4],\n       [4, 3],\n       [5, 4]]...)>>> from sklearn.neighbors import (NeighborhoodComponentsAnalysis,\n... KNeighborsClassifier)\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import Pipeline\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n>>> nca_pipe.fit(X_train, y_train) \nPipeline(...)\n>>> print(nca_pipe.score(X_test, y_test)) \n0.96190476...", "Url": "https://scikit-learn.org/stable/modules/neighbors.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_0021.png"]}, "38": {"Title": "1.6.2. Nearest Neighbors Classification", "Text": "Alternatively, one can use the KDTree or BallTree classes directly to find nearest neighbors. This is the functionality wrapped by the NearestNeighbors class used above. The Ball Tree and KD Tree have the same interface; we\u2019ll show an example of using the KD Tree here:\nRefer to the KDTree and BallTree class documentation for more information on the options available for nearest neighbors searches, including specification of query strategies, distance metrics, etc. For a list of available metrics, see the documentation of the DistanceMetric class.\nNeighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\nscikit-learn implements two different nearest neighbors classifiers: KNeighborsClassifier implements learning based on the\nk\nnearest neighbors of each query point, where\nk\nis an integer value specified by the user. RadiusNeighborsClassifier implements learning based on the number of neighbors within a fixed radius\nr\nof each training point, where\nr\nis a floating-point value specified by the user.\nThe\nk\n-neighbors classification in KNeighborsClassifier is the most commonly used technique. The optimal choice of the value\nk\nis highly data-dependent: in general a larger\nk\nsuppresses the effects of noise, but makes the classification boundaries less distinct.\nIn cases where the data is not uniformly sampled, radius-based neighbors classification in RadiusNeighborsClassifier can be a better choice. The user specifies a fixed radius\nr\n, such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called \u201ccurse of dimensionality\u201d.\nThe basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns uniform weights to each neighbor. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.\n\nTo address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point\nA\nis very distant from point\nB\n, and point\nB\nis very close to point\nC\n, then we know that points\nA\nand\nC\nare very distant, without having to explicitly calculate their distance. In this way, the computational cost of a nearest neighbors search can be reduced to\nO\nor better. This is a significant improvement over brute-force for large\nN\n.\nAn early approach to taking advantage of this aggregate information was the KD tree data structure (short for K-dimensional tree), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no\nD\n-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only\nO\ndistance computations. Though the KD tree approach is very fast for low-dimensional (\nD\n) neighbors searches, it becomes inefficient as\nD\ngrows very large: this is one manifestation of the so-called \u201ccurse of dimensionality\u201d. In scikit-learn, KD tree neighbors searches are specified using the keyword algorithm = 'kd_tree', and are computed using the class KDTree.\nNCA can be used to perform supervised dimensionality reduction. The input data are projected onto a linear subspace consisting of the directions which minimize the NCA objective. The desired dimensionality can be set using the parameter n_components. For instance, the following figure shows a comparison of dimensionality reduction with Principal Component Analysis (sklearn.decomposition.PCA), Linear Discriminant Analysis (sklearn.discriminant_analysis.LinearDiscriminantAnalysis) and Neighborhood Component Analysis (NeighborhoodComponentsAnalysis) on the Digits dataset, a dataset with size\nn\nand\nn\n. The data set is split into a training and a test set of equal size, then standardized. For evaluation the 3-nearest neighbor classification accuracy is computed on the 2-dimensional projected points found by each method. Each data sample belongs to one of 10 classes.\n\nHere the transform operation returns\nL\n, therefore its time complexity equals n_components * n_features * n_samples_test. There is no added space complexity in the operation.\n", "Code_snippet": ">>> from sklearn.neighbors import KDTree\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kdt = KDTree(X, leaf_size=30, metric='euclidean')\n>>> kdt.query(X, k=2, return_distance=False)          \narray([[0, 1],\n       [1, 0],\n       [2, 1],\n       [3, 4],\n       [4, 3],\n       [5, 4]]...)", "Url": "https://scikit-learn.org/stable/modules/neighbors.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_classification_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_classification_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_0031.png"]}, "39": {"Title": "1.6.3. Nearest Neighbors Regression", "Text": "Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors.\nscikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the\nk\nnearest neighbors of each query point, where\nk\nis an integer value specified by the user. RadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius\nr\nof the query point, where\nr\nis a floating-point value specified by the user.\nThe basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns equal weights to all points. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.\nThe use of multi-output nearest neighbors for regression is demonstrated in Face completion with a multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.\nTo address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.\nA ball tree recursively divides the data into nodes defined by a centroid\nC\nand radius\nr\n, such that each point in the node lies within the hyper-sphere defined by\nr\nand\nC\n. The number of candidate points for a neighbor search is reduced through use of the triangle inequality:\nWith this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword algorithm = 'ball_tree', and are computed using the class sklearn.neighbors.BallTree. Alternatively, the user can work with the BallTree class directly.\nThe goal of NCA is to learn an optimal linear transformation matrix of size (n_components, n_features), which maximises the sum over all samples\ni\nof the probability\np\nthat\ni\nis correctly classified, i.e.:\nwith\nN\n= n_samples and\np\nthe probability of sample\ni\nbeing correctly classified according to a stochastic nearest neighbors rule in the learned embedded space:\nwhere\nC\nis the set of points in the same class as sample\ni\n, and\np\nis the softmax over Euclidean distances in the embedded space:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neighbors.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_regression_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_multioutput_face_completion_0011.png"]}, "40": {"Title": "1.6.4. Nearest Neighbor Algorithms", "Text": "The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:\nCurrently, algorithm = 'auto' selects 'kd_tree' if\nk\nand the 'effective_metric_' is in the 'VALID_METRICS' list of 'kd_tree'. It selects 'ball_tree' if\nk\nand the 'effective_metric_' is in the 'VALID_METRICS' list of 'ball_tree'. It selects 'brute' if\nk\nand the 'effective_metric_' is not in the 'VALID_METRICS' list of 'kd_tree' or 'ball_tree'. It selects 'brute' if\nk\n. This choice is based on the assumption that the number of query points is at least the same order as the number of training points, and that leaf_size is close to its default value of 30.\nThis implementation follows what is explained in the original paper [1]. For the optimisation method, it currently uses scipy\u2019s L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning.\nSee the examples below and the docstring of NeighborhoodComponentsAnalysis.fit for further information.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neighbors.html", "Attachment_Url": []}, "41": {"Title": "1.6.5. Nearest Centroid Classifier", "Text": "As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter leaf_size. This parameter choice has many effects:\nleaf_size is not referenced for brute force queries.\nThe NearestCentroid classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the sklearn.KMeans algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (sklearn.discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis) for more complex methods that do not make this assumption. Usage of the default NearestCentroid is simple:\n", "Code_snippet": ">>> from sklearn.neighbors.nearest_centroid import NearestCentroid\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> y = np.array([1, 1, 1, 2, 2, 2])\n>>> clf = NearestCentroid()\n>>> clf.fit(X, y)\nNearestCentroid(metric='euclidean', shrink_threshold=None)\n>>> print(clf.predict([[-0.8, -1]]))\n[1]", "Url": "https://scikit-learn.org/stable/modules/neighbors.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_0021.png"]}, "42": {"Title": "1.6.6. Neighborhood Components Analysis", "Text": "Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification.\n\nIn the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the mathematical formulation for more details.\n", "Code_snippet": ">>> from sklearn.neighbors import (NeighborhoodComponentsAnalysis,\n... KNeighborsClassifier)\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import Pipeline\n>>> X, y = load_iris(return_X_y=True)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n... stratify=y, test_size=0.7, random_state=42)\n>>> nca = NeighborhoodComponentsAnalysis(random_state=42)\n>>> knn = KNeighborsClassifier(n_neighbors=3)\n>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n>>> nca_pipe.fit(X_train, y_train) \nPipeline(...)\n>>> print(nca_pipe.score(X_test, y_test)) \n0.96190476...", "Url": "https://scikit-learn.org/stable/modules/neighbors.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_illustration_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_illustration_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_classification_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_nca_dim_reduction_0031.png"]}, "43": {"Title": "1.7.1. Gaussian Process Regression (GPR)", "Text": "The GaussianProcessRegressor implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for normalize_y=False) or the training data\u2019s mean (for normalize_y=True). The prior\u2019s covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed optimizer. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, None can be passed as optimizer.\nThe noise level in the targets can be specified by passing it via the parameter alpha, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).\nThe implementation is based on Algorithm 2.1 of [RW2006]. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:\nThis example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.\nThe first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.\nThe second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.\nThis example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).\nWhile the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC.\nThe second figure shows the log-marginal-likelihood for different choices of the kernel\u2019s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.\nThe main usage of a Kernel is to compute the GP\u2019s covariance between datapoints. For this, the method __call__ of the kernel can be called. This method can either be used to compute the \u201cauto-covariance\u201d of all pairs of datapoints in a 2d array X, or the \u201ccross-covariance\u201d of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the WhiteKernel): k(X) == K(X, Y=X)\nIf only the diagonal of the auto-covariance is being used, the method diag() of a kernel can be called, which is more computationally efficient than the equivalent call to __call__: np.diag(k(X, X)) == k.diag(X)\nKernels are parameterized by a vector\n\u03b8\nof hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel\u2019s auto-covariance with respect to\n\u03b8\nvia setting eval_gradient=True in the __call__ method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of\n\u03b8\n, which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of\n\u03b8\ncan be get and set via the property theta of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property bounds of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of Hyperparameter in the respective kernel. Note that a kernel using a hyperparameter with name \u201cx\u201d must have the attributes self.x and self.x_bounds.\nThe abstract base class for all kernels is Kernel. Kernel implements a similar interface as Estimator, providing the methods get_params(), set_params(), and clone(). This allows setting kernel values also via meta-estimators such as Pipeline or GridSearch. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with k1__ and parameters of the right operand with k2__. An additional convenience method is clone_with_theta(theta), which returns a cloned version of the kernel but with the hyperparameters set to theta. An illustrative example:\nAll Gaussian process kernels are interoperable with sklearn.metrics.pairwise and vice versa: instances of subclasses of Kernel can be passed as metric to pairwise_kernels from sklearn.metrics.pairwise. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class PairwiseKernel. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter gamma is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.\n", "Code_snippet": ">>> from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n>>> kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))\n>>> for hyperparameter in kernel.hyperparameters: print(hyperparameter)\nHyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\nHyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\nHyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n>>> params = kernel.get_params()\n>>> for key in sorted(params): print(\"%s : %s\" % (key, params[key]))\nk1 : 1**2 * RBF(length_scale=0.5)\nk1__k1 : 1**2\nk1__k1__constant_value : 1.0\nk1__k1__constant_value_bounds : (0.0, 10.0)\nk1__k2 : RBF(length_scale=0.5)\nk1__k2__length_scale : 0.5\nk1__k2__length_scale_bounds : (0.0, 10.0)\nk2 : RBF(length_scale=2)\nk2__length_scale : 2.0\nk2__length_scale_bounds : (0.0, 10.0)\n>>> print(kernel.theta)  # Note: log-transformed\n[ 0.         -0.69314718  0.69314718]\n>>> print(kernel.bounds)  # Note: log-transformed\n[[      -inf 2.30258509]\n [      -inf 2.30258509]\n [      -inf 2.30258509]]", "Url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_0021.png"]}, "44": {"Title": "1.7.2. GPR examples", "Text": "Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the \u201ckernel trick\u201d. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.\nA major difference is that GPR can choose the kernel\u2019s hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions.\nThe following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel\u2019s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.\nThe figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly\n2\n(6.28), while KRR chooses the doubled periodicity\n4\n. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (\u201ccurse of dimensionality\u201d). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.\nThis example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as RBF often obtain better results.\nThe ConstantKernel kernel can be used as part of a Product kernel where it scales the magnitude of the other factor (kernel) or as part of a Sum kernel, where it modifies the mean of the Gaussian process. It depends on a parameter\nc\n. It is defined as:\nThe main use-case of the WhiteKernel kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter\nn\ncorresponds to estimating the noise-level. It is defined as:\n", "Code_snippet": "34.4**2 * RBF(length_scale=41.8)\n+ 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,\n                                                   periodicity=1)\n+ 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)\n+ 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)", "Url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_compare_gpr_krr_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_co2_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_xor_0011.png"]}, "45": {"Title": "1.7.3. Gaussian Process Classification (GPC)", "Text": "This example is based on Section 5.4.3 of [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.\nThe kernel is composed of several terms that are responsible for explaining different properties of the signal:\nMaximizing the log-marginal-likelihood after subtracting the target\u2019s mean yields the following kernel with an LML of -83.214:\nThus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015\nThe GaussianProcessClassifier implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function\nf\n, which is then squashed through a link function to obtain the probabilistic classification. The latent function\nf\nis a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and\nf\nis removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.\nIn contrast to the regression setting, the posterior of the latent function\nf\nis not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [RW2006].\nThe GP prior mean is assumed to be zero. The prior\u2019s covariance is specified by passing a kernel object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed optimizer. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying n_restarts_optimizer. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, None can be passed as optimizer.\nGaussianProcessClassifier supports multi-class classification by performing either one-versus-rest or one-versus-one based training and prediction. In one-versus-rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In \u201cone_vs_one\u201d, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. See the section on multi-class classification for more details.\nIn the case of Gaussian process classification, \u201cone_vs_one\u201d might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that \u201cone_vs_one\u201d does not support predicting probability estimates but only plain predictions. Moreover, note that GaussianProcessClassifier does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.\nThis example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.\nKernel operators take one or two base kernels and combine them into a new kernel. The Sum kernel takes two kernels\nk\nand\nk\nand combines them via\nk\n. The Product kernel takes two kernels\nk\nand\nk\nand combines them via\nk\n. The Exponentiation kernel takes one base kernel and a scalar parameter\ne\nand combines them via\nk\n.\n", "Code_snippet": "34.4**2 * RBF(length_scale=41.8)\n+ 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,\n                                                   periodicity=1)\n+ 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)\n+ 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)", "Url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_co2_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_iris_0011.png"]}, "46": {"Title": "1.7.4. GPC examples", "Text": "The RBF kernel is a stationary kernel. It is also known as the \u201csquared exponential\u201d kernel. It is parameterized by a length-scale parameter\nl\n, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs\nx\n(anisotropic variant of the kernel). The kernel is given by:\nThis kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_xor_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_iris_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0011.png"]}, "47": {"Title": "1.7.5. Kernels for Gaussian Processes", "Text": "Kernels (also called \u201ccovariance functions\u201d in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the \u201csimilarity\u201d of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values\nk\nand are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [RW2006].\nThe Matern kernel is a stationary kernel and a generalization of the RBF kernel. It has an additional parameter\n\u03bd\nwhich controls the smoothness of the resulting function. It is parameterized by a length-scale parameter\nl\n, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs\nx\n(anisotropic variant of the kernel). The kernel is given by:\nAs\n\u03bd\n, the Mat\u00e9rn kernel converges to the RBF kernel. When\n\u03bd\n, the Mat\u00e9rn kernel becomes identical to the absolute exponential kernel, i.e.,\nIn particular,\n\u03bd\n:\nand\n\u03bd\n:\nare popular choices for learning functions that are not infinitely differentiable (as assumed by the RBF kernel) but at least once (\n\u03bd\n) or twice differentiable (\n\u03bd\n).\nThe flexibility of controlling the smoothness of the learned function via\n\u03bd\nallows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat\u00e9rn kernel are shown in the following figure:\nSee [RW2006], pp84 for further details regarding the different variants of the Mat\u00e9rn kernel.\n", "Code_snippet": ">>> from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n>>> kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))\n>>> for hyperparameter in kernel.hyperparameters: print(hyperparameter)\nHyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\nHyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\nHyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\n>>> params = kernel.get_params()\n>>> for key in sorted(params): print(\"%s : %s\" % (key, params[key]))\nk1 : 1**2 * RBF(length_scale=0.5)\nk1__k1 : 1**2\nk1__k1__constant_value : 1.0\nk1__k1__constant_value_bounds : (0.0, 10.0)\nk1__k2 : RBF(length_scale=0.5)\nk1__k2__length_scale : 0.5\nk1__k2__length_scale_bounds : (0.0, 10.0)\nk2 : RBF(length_scale=2)\nk2__length_scale : 2.0\nk2__length_scale_bounds : (0.0, 10.0)\n>>> print(kernel.theta)  # Note: log-transformed\n[ 0.         -0.69314718  0.69314718]\n>>> print(kernel.bounds)  # Note: log-transformed\n[[      -inf 2.30258509]\n [      -inf 2.30258509]\n [      -inf 2.30258509]]", "Url": "https://scikit-learn.org/stable/modules/gaussian_process.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0041.png"]}, "48": {"Title": "1.9.1. Gaussian Naive Bayes", "Text": "GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\nThe parameters \u03c3y\nand \u03bcy\nare estimated using maximum likelihood.\n", "Code_snippet": ">>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> from sklearn.naive_bayes import GaussianNB\n>>> gnb = GaussianNB()\n>>> y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n>>> print(\"Number of mislabeled points out of a total %d points : %d\"\n...       % (iris.data.shape[0],(iris.target != y_pred).sum()))\nNumber of mislabeled points out of a total 150 points : 6", "Url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "Attachment_Url": []}, "49": {"Title": "1.9.2. Multinomial Naive Bayes", "Text": "MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors for each class , where is the number of features (in text classification, the size of the vocabulary) and is the probability of feature appearing in a sample belonging to class .\nThe parameters\n\u03b8\nis estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\nwhere\nN\nis the number of times feature\ni\nappears in a sample of class\ny\nin the training set\nT\n, and\nN\nis the total count of all features for class\ny\n.\nThe smoothing priors\n\u03b1\naccounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting\n\u03b1\nis called Laplace smoothing, while\n\u03b1\nis called Lidstone smoothing.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "Attachment_Url": []}, "50": {"Title": "1.9.3. Complement Naive Bayes", "Text": "ComplementNB implements the complement naive Bayes (CNB) algorithm. CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets. Specifically, CNB uses statistics from the complement of each class to compute the model\u2019s weights. The inventors of CNB show empirically that the parameter estimates for CNB are more stable than those for MNB. Further, CNB regularly outperforms MNB (often by a considerable margin) on text classification tasks. The procedure for calculating the weights is as follows:\nwhere the summations are over all documents\nj\nnot in class\nc\n,\nd\nis either the count or tf-idf value of term\ni\nin document\nj\n,\n\u03b1\nis a smoothing hyperparameter like that found in MNB, and\n\u03b1\n. The second normalization addresses the tendency for longer documents to dominate parameter estimates in MNB. The classification rule is:\ni.e., a document is assigned to the class that is the poorest complement match.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "Attachment_Url": []}, "51": {"Title": "1.9.4. Bernoulli Naive Bayes", "Text": "BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).\nThe decision rule for Bernoulli naive Bayes is based on\nwhich differs from multinomial NB\u2019s rule in that it explicitly penalizes the non-occurrence of a feature\ni\nthat is an indicator for class\ny\n, where the multinomial variant would simply ignore a non-occurring feature.\nIn the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "Attachment_Url": []}, "52": {"Title": "1.9.5. Out-of-core naive Bayes model fitting", "Text": "Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents. All naive Bayes classifiers support sample weighting.\nContrary to the fit method, the first call to partial_fit needs to be passed the list of all the expected class labels.\nFor an overview of available strategies in scikit-learn, see also the out-of-core learning documentation.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/naive_bayes.html", "Attachment_Url": []}, "53": {"Title": "1.10.1. Classification", "Text": "DecisionTreeClassifier is a class capable of performing multi-class classification on a dataset.\nAs with other classifiers, DecisionTreeClassifier takes as input two arrays: an array X, sparse or dense, of size [n_samples, n_features] holding the training samples, and an array Y of integer values, size [n_samples], holding the class labels for the training samples:\nAfter being fitted, the model can then be used to predict the class of samples:\nAlternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:\nDecisionTreeClassifier is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, \u2026, K-1]) classification.\nUsing the Iris dataset, we can construct a tree as follows:\nOnce trained, you can plot the tree with the plot_tree function:\nWe can also export the tree in Graphviz format using the export_graphviz exporter. If you use the conda package manager, the graphviz binaries\nand the python package can be installed with\nAlternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with pip install graphviz.\nBelow is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file iris.pdf:\nThe export_graphviz exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:\nAlternatively, the tree can also be exported in textual format with the function export_text. This method doesn\u2019t require the installation of external libraries and is more compact:\nIf a target is a classification outcome taking on values 0,1,\u2026,K-1, for node\nm\n, representing a region\nR\nwith\nN\nobservations, let\nbe the proportion of class k observations in node\nm\nCommon measures of impurity are Gini\nEntropy\nand Misclassification\nwhere\nX\nis the training data in node\nm\n", "Code_snippet": ">>> from sklearn import tree\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = tree.DecisionTreeClassifier()\n>>> clf = clf.fit(X, Y)>>> clf.predict([[2., 2.]])\narray([1])>>> clf.predict_proba([[2., 2.]])\narray([[0., 1.]])>>> from sklearn.datasets import load_iris\n>>> from sklearn import tree\n>>> iris = load_iris()\n>>> clf = tree.DecisionTreeClassifier()\n>>> clf = clf.fit(iris.data, iris.target)>>> tree.plot_tree(clf.fit(iris.data, iris.target)) >>> import graphviz \n>>> dot_data = tree.export_graphviz(clf, out_file=None) \n>>> graph = graphviz.Source(dot_data) \n>>> graph.render(\"iris\") >>> dot_data = tree.export_graphviz(clf, out_file=None, \n...                      feature_names=iris.feature_names,  \n...                      class_names=iris.target_names,  \n...                      filled=True, rounded=True,  \n...                      special_characters=True)  \n>>> graph = graphviz.Source(dot_data)  \n>>> graph >>> from sklearn.datasets import load_iris\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.tree.export import export_text\n>>> iris = load_iris()\n>>> X = iris['data']\n>>> y = iris['target']\n>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n>>> decision_tree = decision_tree.fit(X, y)\n>>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n>>> print(r)\n|--- petal width (cm) <= 0.80\n|   |--- class: 0\n|--- petal width (cm) >  0.80\n|   |--- petal width (cm) <= 1.75\n|   |   |--- class: 1\n|   |--- petal width (cm) >  1.75\n|   |   |--- class: 2", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_0021.png", "https://scikit-learn.org/stable/_images/iris.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_0011.png"]}, "54": {"Title": "1.10.2. Regression", "Text": "Decision trees can also be applied to regression problems, using the DecisionTreeRegressor class.\nAs in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:\nIf the target is a continuous value, then for node\nm\n, representing a region\nR\nwith\nN\nobservations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes.\nMean Squared Error:\nMean Absolute Error:\nwhere\nX\nis the training data in node\nm\n", "Code_snippet": ">>> from sklearn import tree\n>>> X = [[0, 0], [2, 2]]\n>>> y = [0.5, 2.5]\n>>> clf = tree.DecisionTreeRegressor()\n>>> clf = clf.fit(X, y)\n>>> clf.predict([[1, 1]])\narray([0.5])", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_0011.png"]}, "55": {"Title": "1.10.3. Multi-output problems", "Text": "A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of size [n_samples, n_outputs].\nWhen there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased.\nWith regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:\nThis module offers support for multi-output problems by implementing this strategy in both DecisionTreeClassifier and DecisionTreeRegressor. If a decision tree is fit on an output array Y of size [n_samples, n_outputs] then the resulting estimator will:\nThe use of multi-output trees for regression is demonstrated in Multi-output Decision Tree Regression. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.\nThe use of multi-output trees for classification is demonstrated in Face completion with a multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_multioutput_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_multioutput_face_completion_0011.png"]}, "56": {"Title": "1.10.4. Complexity", "Text": "In general, the run time cost to construct a balanced binary tree is\nO\nand query time\nO\n. Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through\nO\nto find the feature that offers the largest reduction in entropy. This has a cost of\nO\nat each node, leading to a total cost over the entire trees (by summing the cost at each node) of\nO\n.\nScikit-learn offers a more efficient implementation for the construction of decision trees. A naive implementation (as above) would recompute the class label histograms (for classification) or the means (for regression) at for each new split point along a given feature. Presorting the feature over all relevant samples, and retaining a running label count, will reduce the complexity at each node to\nO\n, which results in a total cost of\nO\n. This is an option for all tree based algorithms. By default it is turned on for gradient boosting, where in general it makes training faster, but turned off for all other algorithms as it tends to slow down training when training deep trees.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": []}, "57": {"Title": "1.10.5. Tips on practical use", "Text": "", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": []}, "58": {"Title": "1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART", "Text": "What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?\nID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\nC4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule\u2019s precondition if the accuracy of the rule improves without it.\nC5.0 is Quinlan\u2019s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\nCART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\nscikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": []}, "59": {"Title": "1.10.7. Mathematical formulation", "Text": "Given training vectors\nx\n, i=1,\u2026, l and a label vector\ny\n, a decision tree recursively partitions the space such that the samples with the same labels are grouped together.\nLet the data at node\nm\nbe represented by\nQ\n. For each candidate split\n\u03b8\nconsisting of a feature\nj\nand threshold\nt\n, partition the data into\nQ\nand\nQ\nsubsets\nThe impurity at\nm\nis computed using an impurity function\nH\n, the choice of which depends on the task being solved (classification or regression)\nSelect the parameters that minimises the impurity\nRecurse for subsets\nQ\nand\nQ\nuntil the maximum allowable depth is reached,\nN\nor\nN\n.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/tree.html", "Attachment_Url": []}, "60": {"Title": "1.11.1. Bagging meta-estimator", "Text": "In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).\nBagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:\nIn scikit-learn, bagging methods are offered as a unified BaggingClassifier meta-estimator (resp. BaggingRegressor), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, max_samples and max_features control the size of the subsets (in terms of samples and features), while bootstrap and bootstrap_features control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. As an example, the snippet below illustrates how to instantiate a bagging ensemble of KNeighborsClassifier base estimators, each built on random subsets of 50% of the samples and 50% of the features.\nIn random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.\nIn contrast to the original publication [B2001], the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.\nThe following example shows how to fit an AdaBoost classifier with 100 weak learners:\nThe number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the base_estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples to consider a split min_samples_split).\nGradientBoostingClassifier supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners:\nThe number of weak learners (i.e. regression trees) is controlled by the parameter n_estimators; The size of each tree can be controlled either by setting the tree depth via max_depth or by setting the number of leaf nodes via max_leaf_nodes. The learning_rate is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage .\nThe following loss functions are supported and can be specified using the parameter loss:\n[F2001] proposed a simple regularization strategy that scales the contribution of each weak learner by a factor :\nThe parameter is also called the learning rate because it scales the step length the gradient descent procedure; it can be set via the learning_rate parameter.\nThe parameter learning_rate strongly interacts with the parameter n_estimators, the number of weak learners to fit. Smaller values of learning_rate require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of learning_rate favor better test error. [HTF2009] recommend to set the learning rate to a small constant (e.g. learning_rate <= 0.1) and choose n_estimators by early stopping. For a more detailed discussion of the interaction between learning_rate and n_estimators see [R2007].\nOften features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response?\nIndividual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree (see Feature importance evaluation for more details).\nThe feature importance scores of a fit gradient boosting model can be accessed via the feature_importances_ property:\nIn majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.\nE.g., if the prediction for a given sample is\nthe VotingClassifier (with voting='hard') would classify the sample as \u201cclass 1\u201d based on the majority class label.\nIn the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E.g., in the following scenario\nthe class label 1 will be assigned to the sample.\nThe following example shows how to fit the majority rule classifier:\nIn order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support predict_proba method):\nOptionally, weights can be provided for the individual classifiers:\n", "Code_snippet": ">>> from sklearn.ensemble import BaggingClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> bagging = BaggingClassifier(KNeighborsClassifier(),\n...                             max_samples=0.5, max_features=0.5)>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import AdaBoostClassifier\n\n>>> iris = load_iris()\n>>> clf = AdaBoostClassifier(n_estimators=100)\n>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n>>> scores.mean()                             \n0.9...>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)                 \n0.913...>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X, y)\n>>> clf.feature_importances_  \narray([0.10..., 0.10..., 0.11..., ...>>> from sklearn import datasets\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.ensemble import VotingClassifier\n\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data[:, 1:3], iris.target\n\n>>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n...                           random_state=1)\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n>>> clf3 = GaussianNB()\n\n>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n\n>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n...     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\nAccuracy: 0.94 (+/- 0.04) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.04) [Ensemble]>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...                         voting='soft', weights=[2, 5, 1])", "Url": "https://scikit-learn.org/stable/modules/ensemble.html", "Attachment_Url": []}, "61": {"Title": "1.11.2. Forests of randomized trees", "Text": "The sklearn.ensemble module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques [B1998] specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\nAs other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:\nLike decision trees, forests of trees also extend to multi-output problems (if Y is an array of size [n_samples, n_outputs]).\nIn extremely randomized trees (see ExtraTreesClassifier and ExtraTreesRegressor classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:\nGradientBoostingRegressor supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares ('ls').\nThe figure below shows the results of applying GradientBoostingRegressor with least squares loss and 500 base learners to the Boston house price dataset (sklearn.datasets.load_boston). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the train_score_ attribute of the gradient boosting model. The test error at each iterations can be obtained via the staged_predict method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. n_estimators) by early stopping. The plot on the right shows the feature importances which can be obtained via the feature_importances_ property.\n[F1999] proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction subsample of the available training data. The subsample is drawn without replacement. A typical value of subsample is 0.5.\nThe figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.\nAnother strategy to reduce the variance is by subsampling the features analogous to the random splits in RandomForestClassifier . The number of subsampled features can be controlled via the max_features parameter.\nStochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute oob_improvement_. oob_improvement_[i] holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming.\nIn contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.\nSpecific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.\nTo illustrate this with a simple example, let\u2019s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.\nThe weighted average probabilities for a sample would then be calculated as follows:\nHere, the predicted class label is 2, since it has the highest average probability.\nThe following example illustrates how the decision regions may change when a soft VotingClassifier is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:\n", "Code_snippet": ">>> from sklearn.ensemble import RandomForestClassifier\n>>> X = [[0, 0], [1, 1]]\n>>> Y = [0, 1]\n>>> clf = RandomForestClassifier(n_estimators=10)\n>>> clf = clf.fit(X, Y)>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.datasets import make_blobs\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.tree import DecisionTreeClassifier\n\n>>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n...     random_state=0)\n\n>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n...     random_state=0)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores.mean()                               \n0.98...\n\n>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n...     min_samples_split=2, random_state=0)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores.mean()                               \n0.999...\n\n>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n...     min_samples_split=2, random_state=0)\n>>> scores = cross_val_score(clf, X, y, cv=5)\n>>> scores.mean() > 0.999\nTrue>>> import numpy as np\n>>> from sklearn.metrics import mean_squared_error\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n>>> X_train, X_test = X[:200], X[200:]\n>>> y_train, y_test = y[:200], y[200:]\n>>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n>>> mean_squared_error(y_test, est.predict(X_test))    \n5.00...>>> from sklearn import datasets\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.svm import SVC\n>>> from itertools import product\n>>> from sklearn.ensemble import VotingClassifier\n\n>>> # Loading some example data\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, [0, 2]]\n>>> y = iris.target\n\n>>> # Training classifiers\n>>> clf1 = DecisionTreeClassifier(max_depth=4)\n>>> clf2 = KNeighborsClassifier(n_neighbors=7)\n>>> clf3 = SVC(gamma='scale', kernel='rbf', probability=True)\n>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n...                         voting='soft', weights=[2, 1, 2])\n\n>>> clf1 = clf1.fit(X, y)\n>>> clf2 = clf2.fit(X, y)\n>>> clf3 = clf3.fit(X, y)\n>>> eclf = eclf.fit(X, y)", "Url": "https://scikit-learn.org/stable/modules/ensemble.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_iris_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regression_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_decision_regions_0011.png"]}, "62": {"Title": "1.11.3. AdaBoost", "Text": "The main parameters to adjust when using these methods is n_estimators and max_features. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are max_features=n_features for regression problems, and max_features=sqrt(n_features) for classification tasks (where n_features is the number of features in the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=2 (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (bootstrap=True) while the default strategy for extra-trees is to use the whole dataset (bootstrap=False). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting oob_score=True.\nThe module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995].\nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights\nw\n,\nw\n, \u2026,\nw\nto each of the training samples. Initially, those weights are all set to\nw\n, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence [HTF].\nAdaBoost can be used both for classification and regression problems:\nBoth GradientBoostingRegressor and GradientBoostingClassifier support warm_start=True which allows you to add more estimators to an already fitted model.\nThe VotingClassifier can also be used together with GridSearchCV in order to tune the hyperparameters of the individual estimators:\n", "Code_snippet": ">>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.ensemble import AdaBoostClassifier\n\n>>> iris = load_iris()\n>>> clf = AdaBoostClassifier(n_estimators=100)\n>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n>>> scores.mean()                             \n0.9...>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\n>>> mean_squared_error(y_test, est.predict(X_test))    \n3.84...>>> from sklearn.model_selection import GridSearchCV\n>>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n...                           random_state=1)\n>>> clf2 = RandomForestClassifier(random_state=1)\n>>> clf3 = GaussianNB()\n>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\n>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n\n>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n>>> grid = grid.fit(iris.data, iris.target)>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...                         voting='soft', weights=[2, 5, 1])", "Url": "https://scikit-learn.org/stable/modules/ensemble.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png"]}, "63": {"Title": "1.11.4. Gradient Tree Boosting", "Text": "Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the n_jobs parameter. If n_jobs=k then computations are partitioned into k jobs, and run on k cores of the machine. If n_jobs=-1 then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using k jobs will unfortunately not be k times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).\nGradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.\nThe advantages of GBRT are:\nThe disadvantages of GBRT are:\nThe module sklearn.ensemble provides methods for both classification and regression via gradient boosted regression trees.\nThe size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth h can capture interactions of order h . There are two ways in which the size of the individual regression trees can be controlled.\nIf you specify max_depth=h then complete binary trees of depth h will be grown. Such trees will have (at most) 2**h leaf nodes and 2**h - 1 split nodes.\nAlternatively, you can control the tree size by specifying the number of leaf nodes via the parameter max_leaf_nodes. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with max_leaf_nodes=k has k - 1 split nodes and thus can model interactions of up to order max_leaf_nodes - 1 .\nWe found that max_leaf_nodes=k gives comparable results to max_depth=k-1 but is significantly faster to train at the expense of a slightly higher training error. The parameter max_leaf_nodes corresponds to the variable J in the chapter on gradient boosting in [F2001] and is related to the parameter interaction.depth in R\u2019s gbm package where max_leaf_nodes == interaction.depth + 1 .\n", "Code_snippet": ">>> # explicitly require this experimental feature\n>>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n>>> # now you can import normally from ensemble\n>>> from sklearn.ensemble import HistGradientBoostingClassifier>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> X_train, X_test = X[:2000], X[2000:]\n>>> y_train, y_test = y[:2000], y[2000:]\n\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)                 \n0.913...>>> import numpy as np\n>>> from sklearn.metrics import mean_squared_error\n>>> from sklearn.datasets import make_friedman1\n>>> from sklearn.ensemble import GradientBoostingRegressor\n\n>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n>>> X_train, X_test = X[:200], X[200:]\n>>> y_train, y_test = y[:200], y[200:]\n>>> est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n>>> mean_squared_error(y_test, est.predict(X_test))    \n5.00...>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est\n>>> mean_squared_error(y_test, est.predict(X_test))    \n3.84...>>> from sklearn.datasets import make_hastie_10_2\n>>> from sklearn.ensemble import GradientBoostingClassifier\n\n>>> X, y = make_hastie_10_2(random_state=0)\n>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n...     max_depth=1, random_state=0).fit(X, y)\n>>> clf.feature_importances_  \narray([0.10..., 0.10..., 0.11..., ...", "Url": "https://scikit-learn.org/stable/modules/ensemble.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regression_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_0011.png"]}, "64": {"Title": "1.11.5. Voting Classifier", "Text": "The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.\nBy averaging the estimates of predictive ability over several randomized trees one can reduce the variance of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to [L2014] for more information on MDI and feature importance evaluation with Random Forests.\nThe following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a ExtraTreesClassifier model.\nIn practice those estimates are stored as an attribute named feature_importances_ on the fitted model. This is an array with shape (n_features,) whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.\nGBRT considers additive models of the following form:\nwhere\nh\nare the basis functions which are usually called weak learners in the context of boosting. Gradient Tree Boosting uses decision trees of fixed size as weak learners. Decision trees have a number of abilities that make them valuable for boosting, namely the ability to handle data of mixed type and the ability to model complex functions.\nSimilar to other boosting algorithms, GBRT builds the additive model in a greedy fashion:\nwhere the newly added tree\nh\ntries to minimize the loss\nL\n, given the previous ensemble\nF\n:\nThe initial model\nF\nis problem specific, for least-squares regression one usually chooses the mean of the target values.\nGradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model\nF\nwhich can be calculated for any differentiable loss function:\nWhere the step length\n\u03b3\nis chosen using line search:\nThe algorithms for regression and classification only differ in the concrete loss function used.\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n", "Code_snippet": ">>> from sklearn import datasets\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.ensemble import VotingClassifier\n\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data[:, 1:3], iris.target\n\n>>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n...                           random_state=1)\n>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n>>> clf3 = GaussianNB()\n\n>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n\n>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n...     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\nAccuracy: 0.94 (+/- 0.04) [Random Forest]\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\nAccuracy: 0.95 (+/- 0.04) [Ensemble]>>> from sklearn import datasets\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.svm import SVC\n>>> from itertools import product\n>>> from sklearn.ensemble import VotingClassifier\n\n>>> # Loading some example data\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, [0, 2]]\n>>> y = iris.target\n\n>>> # Training classifiers\n>>> clf1 = DecisionTreeClassifier(max_depth=4)\n>>> clf2 = KNeighborsClassifier(n_neighbors=7)\n>>> clf3 = SVC(gamma='scale', kernel='rbf', probability=True)\n>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n...                         voting='soft', weights=[2, 1, 2])\n\n>>> clf1 = clf1.fit(X, y)\n>>> clf2 = clf2.fit(X, y)\n>>> clf3 = clf3.fit(X, y)\n>>> eclf = eclf.fit(X, y)>>> from sklearn.model_selection import GridSearchCV\n>>> clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n...                           random_state=1)\n>>> clf2 = RandomForestClassifier(random_state=1)\n>>> clf3 = GaussianNB()\n>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\n>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n\n>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n>>> grid = grid.fit(iris.data, iris.target)>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')>>> eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n...                         voting='soft', weights=[2, 5, 1])", "Url": "https://scikit-learn.org/stable/modules/ensemble.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_decision_regions_0011.png"]}, "65": {"Title": "1.11.6. Voting Regressor", "Text": "RandomTreesEmbedding implements an unsupervised transformation of the data. Using a forest of completely random trees, RandomTreesEmbedding encodes the data by the indices of the leaves a data point ends up in. This index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be computed very efficiently and can then be used as a basis for other learning tasks. The size and sparsity of the code can be influenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the coding contains one entry of one. The size of the coding is at most n_estimators * 2 ** max_depth, the maximum number of leaves in the forest.\nAs neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.\nThe idea behind the VotingRegressor is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.\nThe following example shows how to fit the VotingRegressor:\n", "Code_snippet": ">>> from sklearn import datasets\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.ensemble import VotingRegressor\n\n>>> # Loading some example data\n>>> boston = datasets.load_boston()\n>>> X = boston.data\n>>> y = boston.target\n\n>>> # Training classifiers\n>>> reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n>>> reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n>>> reg3 = LinearRegression()\n>>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n>>> ereg = ereg.fit(X, y)", "Url": "https://scikit-learn.org/stable/modules/ensemble.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_regressor_0011.png"]}, "66": {"Title": "1.12.1. Multilabel classification format", "Text": "In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]]) represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.\nProducing multilabel data as a list of sets of labels may be more intuitive. The MultiLabelBinarizer transformer can be used to convert between a collection of collections of labels and the indicator format.\nBelow is an example of multiclass learning using OvR:\nBelow is an example of multiclass learning using OvO:\nBelow is an example of multiclass learning using Output-Codes:\n", "Code_snippet": ">>> from sklearn.preprocessing import MultiLabelBinarizer\n>>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]\n>>> MultiLabelBinarizer().fit_transform(y)\narray([[0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 0, 0]])>>> from sklearn import datasets\n>>> from sklearn.multiclass import OneVsRestClassifier\n>>> from sklearn.svm import LinearSVC\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) \narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])>>> from sklearn import datasets\n>>> from sklearn.multiclass import OneVsOneClassifier\n>>> from sklearn.svm import LinearSVC\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) \narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])>>> from sklearn import datasets\n>>> from sklearn.multiclass import OutputCodeClassifier\n>>> from sklearn.svm import LinearSVC\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> clf = OutputCodeClassifier(LinearSVC(random_state=0),\n...                            code_size=2, random_state=0)\n>>> clf.fit(X, y).predict(X) \narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "67": {"Title": "1.12.2. One-Vs-The-Rest", "Text": "This strategy, also known as one-vs-all, is implemented in OneVsRestClassifier. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.\nOneVsRestClassifier also supports multilabel classification. To use this feature, feed the classifier an indicator matrix, in which cell [i, j] indicates the presence of label j in sample i.\n", "Code_snippet": ">>> from sklearn import datasets\n>>> from sklearn.multiclass import OneVsRestClassifier\n>>> from sklearn.svm import LinearSVC\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) \narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_multilabel_0011.png"]}, "68": {"Title": "1.12.3. One-Vs-One", "Text": "OneVsOneClassifier constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers.\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don\u2019t scale well with n_samples. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used n_classes times. The decision function is the result of a monotonic transformation of the one-versus-one classification.\n", "Code_snippet": ">>> from sklearn import datasets\n>>> from sklearn.multiclass import OneVsOneClassifier\n>>> from sklearn.svm import LinearSVC\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X) \narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "69": {"Title": "1.12.4. Error-Correcting Output-Codes", "Text": "Output-code based strategies are fairly different from one-vs-the-rest and one-vs-one. With these strategies, each class is represented in a Euclidean space, where each dimension can only be 0 or 1. Another way to put it is that each class is represented by a binary code (an array of 0 and 1). The matrix which keeps track of the location/code of each class is called the code book. The code size is the dimensionality of the aforementioned space. Intuitively, each class should be represented by a code as unique as possible and a good code book should be designed to optimize classification accuracy. In this implementation, we simply use a randomly-generated code book as advocated in [3] although more elaborate methods may be added in the future.\nAt fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen.\nIn OutputCodeClassifier, the code_size attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.\nA number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since log2(n_classes) is much smaller than n_classes.\nA number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name \u201cerror-correcting\u201d. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging.\n", "Code_snippet": ">>> from sklearn import datasets\n>>> from sklearn.multiclass import OutputCodeClassifier\n>>> from sklearn.svm import LinearSVC\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> clf = OutputCodeClassifier(LinearSVC(random_state=0),\n...                            code_size=2, random_state=0)\n>>> clf.fit(X, y).predict(X) \narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "70": {"Title": "1.12.5. Multioutput regression", "Text": "Multioutput regression support can be added to any regressor with MultiOutputRegressor. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As MultiOutputRegressor fits one regressor per target it can not take advantage of correlations between targets.\nBelow is an example of multioutput regression:\n", "Code_snippet": ">>> from sklearn.datasets import make_regression\n>>> from sklearn.multioutput import MultiOutputRegressor\n>>> from sklearn.ensemble import GradientBoostingRegressor\n>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\narray([[-154.75474165, -147.03498585,  -50.03812219],\n       [   7.12165031,    5.12914884,  -81.46081961],\n       [-187.8948621 , -100.44373091,   13.88978285],\n       [-141.62745778,   95.02891072, -191.48204257],\n       [  97.03260883,  165.34867495,  139.52003279],\n       [ 123.92529176,   21.25719016,   -7.84253   ],\n       [-122.25193977,  -85.16443186, -107.12274212],\n       [ -30.170388  ,  -94.80956739,   12.16979946],\n       [ 140.72667194,  176.50941682,  -17.50447799],\n       [ 149.37967282,  -81.15699552,   -5.72850319]])", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "71": {"Title": "1.12.6. Multioutput classification", "Text": "Multioutput classification support can be added to any classifier with MultiOutputClassifier. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3\u2026,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3\u2026,yn).\nBelow is an example of multioutput classification:\n", "Code_snippet": ">>> from sklearn.datasets import make_classification\n>>> from sklearn.multioutput import MultiOutputClassifier\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> from sklearn.utils import shuffle\n>>> import numpy as np\n>>> X, y1 = make_classification(n_samples=10, n_features=100, n_informative=30, n_classes=3, random_state=1)\n>>> y2 = shuffle(y1, random_state=1)\n>>> y3 = shuffle(y1, random_state=2)\n>>> Y = np.vstack((y1, y2, y3)).T\n>>> n_samples, n_features = X.shape # 10,100\n>>> n_outputs = Y.shape[1] # 3\n>>> n_classes = 3\n>>> forest = RandomForestClassifier(n_estimators=100, random_state=1)\n>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n>>> multi_target_forest.fit(X, Y).predict(X)\narray([[2, 2, 0],\n       [1, 2, 1],\n       [2, 1, 0],\n       [0, 0, 2],\n       [0, 2, 1],\n       [0, 0, 2],\n       [1, 1, 0],\n       [1, 1, 1],\n       [0, 0, 2],\n       [2, 0, 0]])", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "72": {"Title": "1.12.7. Classifier Chain", "Text": "Classifier chains (see ClassifierChain) are a way of combining a number of binary classifiers into a single multi-label model that is capable of exploiting correlations among targets.\nFor a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.\nWhen predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features.\nClearly the order of the chain is important. The first model in the chain has no information about the other labels while the last model in the chain has features indicating the presence of all of the other labels. In general one does not know the optimal ordering of the models in the chain so typically many randomly ordered chains are fit and their predictions are averaged together.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "73": {"Title": "1.12.8. Regressor Chain", "Text": "Regressor chains (see RegressorChain) is analogous to ClassifierChain as a way of combining a number of regressions into a single multi-target model that is capable of exploiting correlations among targets.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/multiclass.html", "Attachment_Url": []}, "74": {"Title": "1.13.1. Removing features with low variance", "Text": "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\nAs an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\nso we can select using the threshold .8 * (1 - .8):\nAs expected, VarianceThreshold has removed the first column, which has a probability p=5/6>.8 of containing a zero.\nLinear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with feature_selection.SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the linear_model.Lasso for regression, and of linear_model.LogisticRegression and svm.LinearSVC for classification:\nWith SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.\n", "Code_snippet": ">>> from sklearn.feature_selection import VarianceThreshold\n>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n>>> sel.fit_transform(X)\narray([[0, 1],\n       [1, 0],\n       [0, 0],\n       [1, 1],\n       [1, 0],\n       [1, 1]])>>> from sklearn.svm import LinearSVC\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.feature_selection import SelectFromModel\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X.shape\n(150, 4)\n>>> lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n>>> model = SelectFromModel(lsvc, prefit=True)\n>>> X_new = model.transform(X)\n>>> X_new.shape\n(150, 3)", "Url": "https://scikit-learn.org/stable/modules/feature_selection.html", "Attachment_Url": []}, "75": {"Title": "1.13.2. Univariate feature selection", "Text": "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method:\nFor instance, we can perform a\n\u03c7\ntest to the samples to retrieve only the two best features as follows:\nThese objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile):\nThe methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\nTree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer):\n", "Code_snippet": ">>> from sklearn.datasets import load_iris\n>>> from sklearn.feature_selection import SelectKBest\n>>> from sklearn.feature_selection import chi2\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X.shape\n(150, 4)\n>>> X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n>>> X_new.shape\n(150, 2)>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.feature_selection import SelectFromModel\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X.shape\n(150, 4)\n>>> clf = ExtraTreesClassifier(n_estimators=50)\n>>> clf = clf.fit(X, y)\n>>> clf.feature_importances_  \narray([ 0.04...,  0.05...,  0.4...,  0.4...])\n>>> model = SelectFromModel(clf, prefit=True)\n>>> X_new = model.transform(X)\n>>> X_new.shape               \n(150, 2)", "Url": "https://scikit-learn.org/stable/modules/feature_selection.html", "Attachment_Url": []}, "76": {"Title": "1.13.3. Recursive feature elimination", "Text": "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\nRFECV performs RFE in a cross-validation loop to find the optimal number of features.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/feature_selection.html", "Attachment_Url": []}, "77": {"Title": "1.13.4. Feature selection using SelectFromModel", "Text": "SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ attribute after fitting. The features are considered unimportant and removed, if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are \u201cmean\u201d, \u201cmedian\u201d and float multiples of these like \u201c0.1*mean\u201d.\nFor examples on how it is to be used refer to the sections below.\n", "Code_snippet": ">>> from sklearn.svm import LinearSVC\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.feature_selection import SelectFromModel\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X.shape\n(150, 4)\n>>> lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n>>> model = SelectFromModel(lsvc, prefit=True)\n>>> X_new = model.transform(X)\n>>> X_new.shape\n(150, 3)>>> from sklearn.ensemble import ExtraTreesClassifier\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.feature_selection import SelectFromModel\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X.shape\n(150, 4)\n>>> clf = ExtraTreesClassifier(n_estimators=50)\n>>> clf = clf.fit(X, y)\n>>> clf.feature_importances_  \narray([ 0.04...,  0.05...,  0.4...,  0.4...])\n>>> model = SelectFromModel(clf, prefit=True)\n>>> X_new = model.transform(X)\n>>> X_new.shape               \n(150, 2)", "Url": "https://scikit-learn.org/stable/modules/feature_selection.html", "Attachment_Url": []}, "78": {"Title": "1.13.5. Feature selection as part of a pipeline", "Text": "Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a sklearn.pipeline.Pipeline:\nIn this snippet we make use of a sklearn.svm.LinearSVC coupled with sklearn.feature_selection.SelectFromModel to evaluate feature importances and select the most relevant features. Then, a sklearn.ensemble.RandomForestClassifier is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the sklearn.pipeline.Pipeline examples for more details.\n", "Code_snippet": "clf = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n  ('classification', RandomForestClassifier())\n])\nclf.fit(X, y)", "Url": "https://scikit-learn.org/stable/modules/feature_selection.html", "Attachment_Url": []}, "79": {"Title": "1.14.1. Label Propagation", "Text": "Label propagation denotes a few variations of semi-supervised graph inference algorithms.\nscikit-learn provides two label propagation models: LabelPropagation and LabelSpreading. Both work by constructing a similarity graph over all items in the input dataset.\nLabelPropagation and LabelSpreading differ in modifications to the similarity matrix that graph and the clamping effect on the label distributions. Clamping allows the algorithm to change the weight of the true ground labeled data to some degree. The LabelPropagation algorithm performs hard clamping of input labels, which means . This clamping factor can be relaxed, to say , which means that we will always retain 80 percent of our original label distribution, but the algorithm gets to change its confidence of the distribution within 20 percent.\nLabelPropagation uses the raw similarity matrix constructed from the data with no modifications. In contrast, LabelSpreading minimizes a loss function that has regularization properties, as such it is often more robust to noise. The algorithm iterates on a modified version of the original graph and normalizes the edge weights by computing the normalized graph Laplacian matrix. This procedure is also used in Spectral clustering.\nLabel propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:\nThe RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/label_propagation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_label_propagation_structure_0011.png"]}, "80": {"Title": "1.17.1. Multi-layer Perceptron", "Text": "Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function f(\u22c5):Rm\u2192Ro by training on a dataset, where m\nis the number of dimensions for input and o\nis the number of dimensions for output. Given a set of features X=x1,x2,...,xm\nand a target y\n, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output.\nThe leftmost layer, known as the input layer, consists of a set of neurons {xi|x1,x2,...,xm} representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation w1x1+w2x2+...+wmxm, followed by a non-linear activation function g(\u22c5):R\u2192R - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.\nThe module contains the public attributes coefs_ and intercepts_. coefs_ is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1. intercepts_ is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1.\nThe advantages of Multi-layer Perceptron are:\nThe disadvantages of Multi-layer Perceptron (MLP) include:\nPlease see Tips on Practical Use section that addresses some of these disadvantages.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/multilayerperceptron_network.png"]}, "81": {"Title": "1.17.2. Classification", "Text": "Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:\nAfter fitting (training), the model can predict labels for new samples:\nMLP can fit a non-linear model to the training data. clf.coefs_ contains the weight matrices that constitute the model parameters:\nCurrently, MLPClassifier supports only the Cross-Entropy loss function, which allows probability estimates by running the predict_proba method.\nMLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates\nP\nper sample\nx\n:\nMLPClassifier supports multi-class classification by applying Softmax as the output function.\nFurther, the model supports multi-label classification in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to 0.5 are rounded to 1, otherwise to 0. For a predicted output of a sample, the indices where the value is 1 represents the assigned classes of that sample:\nSee the examples below and the docstring of MLPClassifier.fit for further information.\n", "Code_snippet": ">>> from sklearn.neural_network import MLPClassifier\n>>> X = [[0., 0.], [1., 1.]]\n>>> y = [0, 1]\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n...                     hidden_layer_sizes=(5, 2), random_state=1)\n...\n>>> clf.fit(X, y)                         \nMLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n              beta_1=0.9, beta_2=0.999, early_stopping=False,\n              epsilon=1e-08, hidden_layer_sizes=(5, 2),\n              learning_rate='constant', learning_rate_init=0.001,\n              max_iter=200, momentum=0.9, n_iter_no_change=10,\n              nesterovs_momentum=True, power_t=0.5, random_state=1,\n              shuffle=True, solver='lbfgs', tol=0.0001,\n              validation_fraction=0.1, verbose=False, warm_start=False)>>> clf.predict([[2., 2.], [-1., -2.]])\narray([1, 0])>>> [coef.shape for coef in clf.coefs_]\n[(2, 5), (5, 2), (2, 1)]>>> clf.predict_proba([[2., 2.], [1., 2.]])  \narray([[1.967...e-04, 9.998...-01],\n       [1.967...e-04, 9.998...-01]])>>> X = [[0., 0.], [1., 1.]]\n>>> y = [[0, 1], [1, 1]]\n>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n...                     hidden_layer_sizes=(15,), random_state=1)\n...\n>>> clf.fit(X, y)                         \nMLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n              beta_1=0.9, beta_2=0.999, early_stopping=False,\n              epsilon=1e-08, hidden_layer_sizes=(15,),\n              learning_rate='constant', learning_rate_init=0.001,\n              max_iter=200, momentum=0.9, n_iter_no_change=10,\n              nesterovs_momentum=True, power_t=0.5,  random_state=1,\n              shuffle=True, solver='lbfgs', tol=0.0001,\n              validation_fraction=0.1, verbose=False, warm_start=False)\n>>> clf.predict([[1., 2.]])\narray([[1, 1]])\n>>> clf.predict([[0., 0.]])\narray([[0, 1]])", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "82": {"Title": "1.17.3. Regression", "Text": "Class MLPRegressor implements a multi-layer perceptron (MLP) that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values.\nMLPRegressor also supports multi-output regression, in which a sample can have more than one target.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "83": {"Title": "1.17.4. Regularization", "Text": "Both MLPRegressor and MLPClassifier use parameter alpha for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha.\nSee the examples below for further information.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_mlp_alpha_0011.png"]}, "84": {"Title": "1.17.5. Algorithms", "Text": "MLP trains using Stochastic Gradient Descent, Adam, or L-BFGS. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.\nwhere\n\u03b7\nis the learning rate which controls the step-size in the parameter space search.\nL\nis the loss function used for the network.\nMore details can be found in the documentation of SGD\nAdam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments.\nWith SGD or Adam, training supports online and mini-batch learning.\nL-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of L-BFGS.\nIf the selected solver is \u2018L-BFGS\u2019, training does not support online nor mini-batch learning.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "85": {"Title": "1.17.6. Complexity", "Text": "Suppose there are\nn\ntraining samples,\nm\nfeatures,\nk\nhidden layers, each containing\nh\nneurons - for simplicity, and\no\noutput neurons. The time complexity of backpropagation is\nO\n, where\ni\nis the number of iterations. Since backpropagation has a high time complexity, it is advisable to start with smaller number of hidden neurons and few hidden layers for training.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "86": {"Title": "1.17.7. Mathematical formulation", "Text": "Given a set of training examples\n(\nwhere\nx\nand\ny\n, a one hidden layer one hidden neuron MLP learns the function\nf\nwhere\nW\nand\nW\nare model parameters.\nW\nrepresent the weights of the input layer and hidden layer, respectively; and\nb\nrepresent the bias added to the hidden layer and the output layer, respectively.\ng\nis the activation function, set by default as the hyperbolic tan. It is given as,\nFor binary classification,\nf\npasses through the logistic function\ng\nto obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.\nIf there are more than two classes,\nf\nitself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,\nwhere\nz\nrepresents the\ni\nth element of the input to softmax, which corresponds to class\ni\n, and\nK\nis the number of classes. The result is a vector containing the probabilities that sample\nx\nbelong to each class. The output is the class with the highest probability.\nIn regression, the output remains as\nf\n; therefore, output activation function is just the identity function.\nMLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,\nwhere\n\u03b1\nis an L2-regularization term (aka penalty) that penalizes complex models; and\n\u03b1\nis a non-negative hyperparameter that controls the magnitude of the penalty.\nFor regression, MLP uses the Square Error loss function; written as,\nStarting from initial random weights, multi-layer perceptron (MLP) minimizes the loss function by repeatedly updating these weights. After computing the loss, a backward pass propagates it from the output layer to the previous layers, providing each weight parameter with an update value meant to decrease the loss.\nIn gradient descent, the gradient\n\u2207\nof the loss with respect to the weights is computed and deducted from\nW\n. More formally, this is expressed as,\nwhere\ni\nis the iteration step, and\n\u03f5\nis the learning rate with a value larger than 0.\nThe algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "87": {"Title": "1.17.8. Tips on Practical Use", "Text": "", "Code_snippet": ">>> from sklearn.preprocessing import StandardScaler  \n>>> scaler = StandardScaler()  \n>>> # Don't cheat - fit only on training data\n>>> scaler.fit(X_train)  \n>>> X_train = scaler.transform(X_train)  \n>>> # apply same transformation to test data\n>>> X_test = scaler.transform(X_test)  ", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "88": {"Title": "1.17.9. More control with warm_start", "Text": "If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using warm_start=True and max_iter=1 and iterating yourself can be helpful:\n", "Code_snippet": ">>> X = [[0., 0.], [1., 1.]]\n>>> y = [0, 1]\n>>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)\n>>> for i in range(10):\n...     clf.fit(X, y)\n...     # additional monitoring / inspection \nMLPClassifier(...", "Url": "https://scikit-learn.org/stable/modules/neural_networks_supervised.html", "Attachment_Url": []}, "89": {"Title": "2.1.1. Gaussian Mixture", "Text": "The GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A GaussianMixture.fit method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the GaussianMixture.predict method.\nThe GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.\nVariational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical.\nDue to its Bayesian nature, the variational algorithm needs more hyper- parameters than expectation-maximization, the most important of these being the concentration parameter weight_concentration_prior. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.\nThe parameters implementation of the BayesianGaussianMixture class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.\nThe next figure compares the results obtained for the different type of the weight concentration prior (parameter weight_concentration_prior_type) for different values of weight_concentration_prior. Here, we can see the value of the weight_concentration_prior parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is \u2018dirichlet_distribution\u2019 while this is not necessarily the case for the \u2018dirichlet_process\u2019 type (used by default).\n\nThe examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected n_components=5 which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.\nOn the following figure we are fitting a dataset not well-depicted by a Gaussian mixture. Adjusting the weight_concentration_prior, parameter of the BayesianGaussianMixture controls the number of components used to fit this data. We also present on the last two plots a random sampling generated from the two resulting mixtures.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/mixture.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_covariances_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_selection_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_concentration_prior_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_concentration_prior_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_sin_0011.png"]}, "90": {"Title": "2.1.2. Variational Bayesian Gaussian Mixture", "Text": "The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a Variational Bayesian Gaussian mixture avoids the specification of the number of components for a Gaussian mixture model.\nThe BayesianGaussianMixture object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar as the one defined by GaussianMixture.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/mixture.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_selection_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_concentration_prior_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_concentration_prior_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_gmm_sin_0011.png"]}, "91": {"Title": "2.2.1. Introduction", "Text": "High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.\nThe simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.\n\nTo address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an \u201cinteresting\u201d linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data.\n\nManifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.\nThe manifold learning implementations available in scikit-learn are summarized below\nThe Isomap algorithm comprises three stages:\nThe overall complexity of Isomap is\nO\n.\nThe standard LLE algorithm comprises three stages:\nThe overall complexity of standard LLE is\nO\n.\nThe MLLE algorithm comprises three stages:\nThe overall complexity of MLLE is\nO\n.\nThe HLLE algorithm comprises three stages:\nThe overall complexity of standard HLLE is\nO\n.\nThe Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:\nThe overall complexity of spectral embedding is\nO\n.\nThe LTSA algorithm comprises three stages:\nThe overall complexity of standard LTSA is\nO\n.\nThe simplest metric MDS model, called absolute MDS, disparities are defined by\nd\n. With absolute MDS, the value\nS\nshould then correspond exactly to the distance between point\ni\nand\nj\nin the embedding point.\nMost commonly, disparities are set to\nd\n.\nThe main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.\nOptimizing the KL divergence can be a little bit tricky sometimes. There are five parameters that control the optimization of t-SNE and therefore possibly the quality of the resulting embedding:\nThe perplexity is defined as\nk\nwhere\nS\nis the Shannon entropy of the conditional probability distribution. The perplexity of a\nk\n-sided die is\nk\n, so that\nk\nis effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.\nThe maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten\u2019s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.\n\u201cHow to Use t-SNE Effectively\u201d provides a good discussion of the effects of the various parameters, as well as interactive plots to explore the effects of different parameters.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0041.png"]}, "92": {"Title": "2.2.2. Isomap", "Text": "One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping. Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points. Isomap can be performed with the object Isomap.\nNon metric MDS focuses on the ordination of the data. If\nS\n, then the embedding should enforce\nd\n. A simple algorithm to enforce that is to use a monotonic regression of\nd\non\nS\n, yielding disparities\nd\nin the same order as\nS\n.\nA trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities\nd\nare normalized.\nThe Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is\nO\n, where\nd\nis the number of output dimensions and\nN\nis the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is\nO\n, but has several other notable differences:\nFor visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.\nAlso note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not low enough to accurately represents the internal structure of the data.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mds_0011.png"]}, "93": {"Title": "2.2.3. Locally Linear Embedding", "Text": "Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.\nLocally linear embedding can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0061.png"]}, "94": {"Title": "2.2.4. Modified Locally Linear Embedding", "Text": "One well-known issue with LLE is the regularization problem. When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient. To address this, standard LLE applies an arbitrary regularization parameter\nr\n, which is chosen relative to the trace of the local weight matrix. Though it can be shown formally that as\nr\n, the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for\nr\n. This problem manifests itself in embeddings which distort the underlying geometry of the manifold.\nOne method to address the regularization problem is to use multiple weight vectors in each neighborhood. This is the essence of modified locally linear embedding (MLLE). MLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'modified'. It requires n_neighbors > n_components.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0071.png"]}, "95": {"Title": "2.2.5. Hessian Eigenmapping", "Text": "Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, sklearn implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'hessian'. It requires n_neighbors > n_components * (n_components + 3) / 2.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0081.png"]}, "96": {"Title": "2.2.6. Spectral Embedding", "Text": "Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be performed with the function spectral_embedding or its object-oriented counterpart SpectralEmbedding.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": []}, "97": {"Title": "2.2.7. Local Tangent Space Alignment", "Text": "Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function locally_linear_embedding or its object-oriented counterpart LocallyLinearEmbedding, with the keyword method = 'ltsa'.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0091.png"]}, "98": {"Title": "2.2.8. Multi-dimensional Scaling (MDS)", "Text": "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\nIn general, is a technique used for analyzing similarity or dissimilarity data. MDS attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.\nThere exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class MDS implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.\nLet\nS\nbe the similarity matrix, and\nX\nthe coordinates of the\nn\ninput points. Disparities\nd\nare transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by\ns\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0101.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mds_0011.png"]}, "99": {"Title": "2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)", "Text": "t-SNE (TSNE) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student\u2019s t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:\nWhile Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.\nThe Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.\nThe disadvantages to using t-SNE are roughly:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_0131.png"]}, "100": {"Title": "2.2.10. Tips on practical use", "Text": "", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/manifold.html", "Attachment_Url": []}, "101": {"Title": "2.3.1. Overview of clustering methods", "Text": "Non-flat geometry clustering is useful when the clusters have a specific shape, i.e. a non-flat manifold, and the standard euclidean distance is not the right metric. This case arises in the two top rows of the figure above.\nGaussian mixture models, useful for clustering, are described in another chapter of the documentation dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.\nThe MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.\nThe algorithm iterates between two major steps, similar to vanilla k-means. In the first step, samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.\nMiniBatchKMeans converges faster than KMeans, but the quality of the results is reduced. In practice this difference in quality can be quite small, as shown in the example and cited reference.\nDifferent label assignment strategies can be used, corresponding to the assign_labels parameter of SpectralClustering. The \"kmeans\" strategy can match finer details of the data, but it can be more unstable. In particular, unless you control the random_state, it may not be reproducible from run-to-run, as it depends on a random initialization. On the other hand, the \"discretize\" strategy is 100% reproducible, but it tends to create parcels of fairly even and geometrical shape.\nAgglomerativeClustering supports Ward, single, average, and complete linkage strategies.\nAgglomerative cluster has a \u201crich get richer\u201d behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data.\nGiven the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments of the same samples labels_pred, the adjusted Rand index is a function that measures the similarity of the two assignments, ignoring permutations and with chance normalization:\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3, and get the same score:\nFurthermore, adjusted_rand_score is symmetric: swapping the argument does not change the score. It can thus be used as a consensus measure:\nPerfect labeling is scored 1.0:\nBad (e.g. independent labelings) have negative or close to 0.0 scores:\n", "Code_snippet": ">>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)  \n0.24...>>> labels_pred = [1, 1, 0, 0, 3, 3]\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)  \n0.24...>>> metrics.adjusted_rand_score(labels_pred, labels_true)  \n0.24...>>> labels_pred = labels_true[:]\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\n1.0>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)  \n-0.12...", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mini_batch_kmeans_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_0011.png"]}, "102": {"Title": "2.3.2. K-means", "Text": "The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\nThe k-means algorithm divides a set of\nN\nsamples\nX\ninto\nK\ndisjoint clusters\nC\n, each described by the mean\n\u03bc\nof the samples in the cluster. The means are commonly called the cluster \u201ccentroids\u201d; note that they are not, in general, points from\nX\n, although they live in the same space.\nThe K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion:\nInertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\nK-means is often referred to as Lloyd\u2019s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose\nk\nsamples from the dataset\nX\n. After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.\nK-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.\nThe algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.\nGiven enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='k-means++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\nThe algorithm supports sample weights, which can be given by a parameter sample_weight. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset\nX\n.\nA parameter can be given to allow K-means to be run in parallel, called n_jobs. Giving this parameter a positive value uses that many processors (default: 1). A value of -1 uses all available processors, with -2 using one less, and so on. Parallelization generally speeds up computation at the cost of memory (in this case, multiple copies of centroids need to be stored, one for each job).\nK-means can be used for vector quantization. This is achieved using the transform method of a trained model of KMeans.\nSpectral Clustering can also be used to cluster graphs by their spectral embeddings. In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with affinity='precomputed':\nAn interesting aspect of AgglomerativeClustering is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll.\n\nThese constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.\nThe connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using sklearn.neighbors.kneighbors_graph to restrict merging to nearest neighbors as in this example, or using sklearn.feature_extraction.image.grid_to_graph to enable only merging of neighboring pixels on an image, as in the coin example.\nGiven the knowledge of the ground truth class assignments labels_true and our clustering algorithm assignments of the same samples labels_pred, the Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, Normalized Mutual Information (NMI) and Adjusted Mutual Information (AMI). NMI is often used in the literature, while AMI was proposed more recently and is normalized against chance:\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:\nAll, mutual_info_score, adjusted_mutual_info_score and normalized_mutual_info_score are symmetric: swapping the argument does not change the score. Thus they can be used as a consensus measure:\nPerfect labeling is scored 1.0:\nThis is not true for mutual_info_score, which is therefore harder to judge:\nBad (e.g. independent labelings) have non-positive scores:\n", "Code_snippet": ">>> from sklearn.cluster import SpectralClustering\n>>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,\n...                         assign_labels='discretize')\n>>> sc.fit_predict(adjacency_matrix)  >>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n0.22504...>>> labels_pred = [1, 1, 0, 0, 3, 3]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n0.22504...>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \n0.22504...>>> labels_pred = labels_true[:]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n1.0\n\n>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  \n1.0>>> metrics.mutual_info_score(labels_true, labels_pred)  \n0.69...>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n-0.10526...", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_digits_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mini_batch_kmeans_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_adjusted_for_chance_measures_0011.png"]}, "103": {"Title": "2.3.3. Affinity Propagation", "Text": "AffinityPropagation creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given.\nAffinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.\nThe main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order\nO\n, where\nN\nis the number of samples and\nT\nis the number of iterations until convergence. Further, the memory complexity is of the order\nO\nif a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.\nAlgorithm description: The messages sent between points belong to one of two categories. The first is the responsibility\nr\n, which is the accumulated evidence that sample\nk\nshould be the exemplar for sample\ni\n. The second is the availability\na\nwhich is the accumulated evidence that sample\ni\nshould choose sample\nk\nto be its exemplar, and considers the values for all other samples that\nk\nshould be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves.\nMore formally, the responsibility of a sample\nk\nto be the exemplar of sample\ni\nis given by:\nWhere\ns\nis the similarity between samples\ni\nand\nk\n. The availability of sample\nk\nto be the exemplar of sample\ni\nis given by:\nTo begin with, all values for\nr\nand\na\nare set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor\n\u03bb\nis introduced to iteration process:\nwhere\nt\nindicates the iteration times.\nSingle, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance (l2), Manhattan distance (or Cityblock, or l1), cosine distance, or any precomputed affinity matrix.\nThe guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.\nIf C is a ground truth class assignment and K the clustering, let us define\na\nand\nb\nas:\nThe raw (unadjusted) Rand index is then given by:\nWhere\nC\nis the total number of possible pairs in the dataset (without ordering).\nHowever the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).\nTo counter this effect we can discount the expected RI\nE\nof random labelings by defining the adjusted Rand index as follows:\nAssume two label assignments (of the same N objects),\nU\nand\nV\n. Their entropy is the amount of uncertainty for a partition set, defined by:\nwhere\nP\nis the probability that an object picked at random from\nU\nfalls into class\nU\n. Likewise for\nV\n:\nWith\nP\n. The mutual information (MI) between\nU\nand\nV\nis calculated by:\nwhere\nP\nis the probability that an object picked at random falls into both classes\nU\nand\nV\n.\nIt also can be expressed in set cardinality formulation:\nThe normalized mutual information is defined as\nThis value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of \u201cmutual information\u201d between the label assignments.\nThe expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation,\na\n(the number of elements in\nU\n) and\nb\n(the number of elements in\nV\n).\nUsing the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:\nFor normalized mutual information and adjusted mutual information, the normalizing value is typically some generalized mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides \u201cqualitatively similar behaviours\u201d [YAT2016]. In our implementation, this is controlled by the average_method parameter.\nVinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]. Their \u2018sqrt\u2019 and \u2018sum\u2019 averages are the geometric and arithmetic means; we use these more broadly common names.\nGiven the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.\nIn particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:\nWe can turn those concept as scores homogeneity_score and completeness_score. Both are bounded below by 0.0 and above by 1.0 (higher is better):\nTheir harmonic mean called V-measure is computed by v_measure_score:\nThis function\u2019s formula is as follows::\nbeta defaults to a value of 1.0, but for using a value less than 1 for beta:\nmore weight will be attributed to homogeneity, and using a value greater than 1:\nmore weight will be attributed to completeness.\nThe V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean [B2011].\nHomogeneity, completeness and V-measure can be computed at once using homogeneity_completeness_v_measure as follows:\nThe following clustering assignment is slightly better, since it is homogeneous but not complete:\nHomogeneity and completeness scores are formally given by:\nwhere\nH\nis the conditional entropy of the classes given the cluster assignments and is given by:\nand\nH\nis the entropy of the classes and is given by:\nwith\nn\nthe total number of samples,\nn\nand\nn\nthe number of samples respectively belonging to class\nc\nand cluster\nk\n, and finally\nn\nthe number of samples from class\nc\nassigned to cluster\nk\n.\nThe conditional entropy of clusters given class\nH\nand the entropy of clusters\nH\nare defined in a symmetric manner.\nRosenberg and Hirschberg further define V-measure as the harmonic mean of homogeneity and completeness:\n", "Code_snippet": ">>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.homogeneity_score(labels_true, labels_pred)  \n0.66...\n\n>>> metrics.completeness_score(labels_true, labels_pred) \n0.42...>>> metrics.v_measure_score(labels_true, labels_pred)    \n0.51..... math:: v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)    \n0.54...>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)    \n0.48...>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n...                                                      \n(0.66..., 0.42..., 0.51...)>>> labels_pred = [0, 0, 0, 1, 2, 2]\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n...                                                      \n(1.0, 0.68..., 0.81...)homogeneity_score(a, b) == completeness_score(b, a)", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_affinity_propagation_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_0061.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_0071.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_adjusted_for_chance_measures_0011.png"]}, "104": {"Title": "2.3.4. Mean Shift", "Text": "MeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\nGiven a candidate centroid\nx\nfor iteration\nt\n, the candidate is updated according to the following equation:\nWhere\nN\nis the neighborhood of samples within a given distance around\nx\nand\nm\nis the mean shift vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:\nThe algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided estimate_bandwidth function, which is called if the bandwidth is not set.\nThe algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.\nLabelling a new sample is performed by finding the nearest centroid for a given sample.\nThe Fowlkes-Mallows index (sklearn.metrics.fowlkes_mallows_score) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:\nWhere TP is the number of True Positive (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), FP is the number of False Positive (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and FN is the number of False Negative (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).\nThe score ranges from 0 to 1. A high value indicates a good similarity between two clusters.\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score:\nPerfect labeling is scored 1.0:\nBad (e.g. independent labelings) have zero scores:\n", "Code_snippet": ">>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n0.47140...>>> labels_pred = [1, 1, 0, 0, 3, 3]\n\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n0.47140...>>> labels_pred = labels_true[:]\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n1.0>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n0.0", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_mean_shift_0011.png"]}, "105": {"Title": "2.3.5. Spectral clustering", "Text": "SpectralClustering does a low-dimension embedding of the affinity matrix between samples, followed by a KMeans in the low dimensional space. It is especially efficient if the affinity matrix is sparse and the pyamg module is installed. SpectralClustering requires the number of clusters to be specified. It works well for a small number of clusters but is not advised when using many clusters.\nFor two clusters, it solves a convex relaxation of the normalised cuts problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.\n\nIf the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\nThe Silhouette Coefficient s for a single sample is then given as:\nThe Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.\nIn normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.\n", "Code_snippet": "similarity = np.exp(-beta * distance / distance.std())>>> from sklearn.cluster import SpectralClustering\n>>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,\n...                         assign_labels='discretize')\n>>> sc.fit_predict(adjacency_matrix)  >>> from sklearn import metrics\n>>> from sklearn.metrics import pairwise_distances\n>>> from sklearn import datasets\n>>> dataset = datasets.load_iris()\n>>> X = dataset.data\n>>> y = dataset.target>>> import numpy as np\n>>> from sklearn.cluster import KMeans\n>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n>>> labels = kmeans_model.labels_\n>>> metrics.silhouette_score(X, labels, metric='euclidean')\n...                                                      \n0.55...", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_segmentation_toy_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_segmentation_toy_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_segmentation_0021.png"]}, "106": {"Title": "2.3.6. Hierarchical clustering", "Text": "Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the Wikipedia page for more details.\nThe AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:\nAgglomerativeClustering can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges.\nIf the ground truth labels are not known, the Calinski-Harabasz index (sklearn.metrics.calinski_harabasz_score) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters.\nFor\nk\nclusters, the Calinski-Harabasz score\ns\nis given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:\nwhere\nB\nis the between group dispersion matrix and\nW\nis the within-cluster dispersion matrix defined by:\nwith\nN\nbe the number of points in our data,\nC\nbe the set of points in cluster\nq\n,\nc\nbe the center of cluster\nq\n,\nc\nbe the center of\nE\n,\nn\nbe the number of points in cluster\nq\n.\nIn normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis.\n", "Code_snippet": ">>> from sklearn import metrics\n>>> from sklearn.metrics import pairwise_distances\n>>> from sklearn import datasets\n>>> dataset = datasets.load_iris()\n>>> X = dataset.data\n>>> y = dataset.target>>> import numpy as np\n>>> from sklearn.cluster import KMeans\n>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n>>> labels = kmeans_model.labels_\n>>> metrics.calinski_harabasz_score(X, labels)  \n561.62...", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_0061.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_clustering_metrics_0071.png"]}, "107": {"Title": "2.3.7. DBSCAN", "Text": "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster.\nMore formally, we define a core sample as being a sample in the dataset such that there exist min_samples other samples within a distance of eps, which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.\nAny core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least eps in distance from any core sample, is considered an outlier by the algorithm.\nWhile the parameter min_samples primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desiable to increase this parameter), the parameter eps is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as -1 for \u201cnoise\u201d). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below).\nIn the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.\n\nIf the ground truth labels are not known, the Davies-Bouldin index (sklearn.metrics.davies_bouldin_score) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.\nThe index is defined as the average similarity between each cluster\nC\nfor\ni\nand its most similar one\nC\n. In the context of this index, similarity is defined as a measure\nR\nthat trades off:\nA simple choice to construct\nR\nso that it is nonnegative and symmetric is:\nThen the Davies-Bouldin index is defined as:\nZero is the lowest possible score. Values closer to zero indicate a better partition.\nIn normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:\n", "Code_snippet": ">>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> X = iris.data\n>>> from sklearn.cluster import KMeans\n>>> from sklearn.metrics import davies_bouldin_score\n>>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)\n>>> labels = kmeans.labels_\n>>> davies_bouldin_score(X, labels)  \n0.6619...", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_0011.png"]}, "108": {"Title": "2.3.8. OPTICS", "Text": "The OPTICS algorithm shares many similarities with the DBSCAN algorithm, and can be considered a generalization of DBSCAN that relaxes the eps requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a reachability_ distance, and a spot within the cluster ordering_ attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for max_eps, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given eps value using the cluster_optics_dbscan method. Setting max_eps to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points.\n\nThe reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set ordering_ produces a reachability plot, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. \u2018Cutting\u2019 the reachability plot at a single value produces DBSCAN like results; all points above the \u2018cut\u2019 are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter xi. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the cluster_hierarchy_ parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.\nContingency matrix (sklearn.metrics.cluster.contingency_matrix) reports the intersection cardinality for every true/predicted cluster pair. The contingency matrix provides sufficient statistics for all clustering metrics where the samples are independent and identically distributed and one doesn\u2019t need to account for some instances not being clustered.\nHere is an example:\nThe first row of output array indicates that there are three samples whose true cluster is \u201ca\u201d. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is \u201cb\u201d. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.\nA confusion matrix for classification is a square contingency matrix where the order of rows and columns correspond to a list of classes.\n", "Code_snippet": ">>> from sklearn.metrics.cluster import contingency_matrix\n>>> x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n>>> y = [0, 0, 1, 1, 2, 2]\n>>> contingency_matrix(x, y)\narray([[2, 1, 0],\n       [0, 1, 2]])", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_optics_0011.png"]}, "109": {"Title": "2.3.9. Birch", "Text": "The Birch builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.\nThe CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:\nThe Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.\nThis algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by n_clusters. If n_clusters is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.\nAlgorithm description:\nBirch or MiniBatchKMeans?\nHow to use partial_fit?\nTo avoid the computation of global clustering, for every call of partial_fit the user is advised\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_birch_vs_minibatchkmeans_0011.png"]}, "110": {"Title": "2.3.10. Clustering performance evaluation", "Text": "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar that members of different classes according to some similarity metric.\n", "Code_snippet": ">>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)  \n0.24...>>> labels_pred = [1, 1, 0, 0, 3, 3]\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)  \n0.24...>>> metrics.adjusted_rand_score(labels_pred, labels_true)  \n0.24...>>> labels_pred = labels_true[:]\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)\n1.0>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.adjusted_rand_score(labels_true, labels_pred)  \n-0.12...>>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n0.22504...>>> labels_pred = [1, 1, 0, 0, 3, 3]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n0.22504...>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  \n0.22504...>>> labels_pred = labels_true[:]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n1.0\n\n>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  \n1.0>>> metrics.mutual_info_score(labels_true, labels_pred)  \n0.69...>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  \n-0.10526...>>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]\n\n>>> metrics.homogeneity_score(labels_true, labels_pred)  \n0.66...\n\n>>> metrics.completeness_score(labels_true, labels_pred) \n0.42...>>> metrics.v_measure_score(labels_true, labels_pred)    \n0.51..... math:: v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)    \n0.54...>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)    \n0.48...>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n...                                                      \n(0.66..., 0.42..., 0.51...)>>> labels_pred = [0, 0, 0, 1, 2, 2]\n>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\n...                                                      \n(1.0, 0.68..., 0.81...)homogeneity_score(a, b) == completeness_score(b, a)>>> from sklearn import metrics\n>>> labels_true = [0, 0, 0, 1, 1, 1]\n>>> labels_pred = [0, 0, 1, 1, 2, 2]>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n0.47140...>>> labels_pred = [1, 1, 0, 0, 3, 3]\n\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n0.47140...>>> labels_pred = labels_true[:]\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n1.0>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]\n>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\n>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)  \n0.0>>> from sklearn import metrics\n>>> from sklearn.metrics import pairwise_distances\n>>> from sklearn import datasets\n>>> dataset = datasets.load_iris()\n>>> X = dataset.data\n>>> y = dataset.target>>> import numpy as np\n>>> from sklearn.cluster import KMeans\n>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n>>> labels = kmeans_model.labels_\n>>> metrics.silhouette_score(X, labels, metric='euclidean')\n...                                                      \n0.55...>>> from sklearn import metrics\n>>> from sklearn.metrics import pairwise_distances\n>>> from sklearn import datasets\n>>> dataset = datasets.load_iris()\n>>> X = dataset.data\n>>> y = dataset.target>>> import numpy as np\n>>> from sklearn.cluster import KMeans\n>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n>>> labels = kmeans_model.labels_\n>>> metrics.calinski_harabasz_score(X, labels)  \n561.62...>>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> X = iris.data\n>>> from sklearn.cluster import KMeans\n>>> from sklearn.metrics import davies_bouldin_score\n>>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)\n>>> labels = kmeans.labels_\n>>> davies_bouldin_score(X, labels)  \n0.6619...>>> from sklearn.metrics.cluster import contingency_matrix\n>>> x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n>>> y = [0, 0, 1, 1, 2, 2]\n>>> contingency_matrix(x, y)\narray([[2, 1, 0],\n       [0, 1, 2]])", "Url": "https://scikit-learn.org/stable/modules/clustering.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_adjusted_for_chance_measures_0011.png"]}, "111": {"Title": "2.4.1. Spectral Co-Clustering", "Text": "The SpectralCoclustering algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:\nAn approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix A has shape m\u00d7n, the Laplacian matrix for the corresponding bipartite graph has shape (m+n)\u00d7(m+n). However, in this case it is possible to work directly with A, which is smaller and more efficient.\nThe input matrix A is preprocessed as follows:\nWhere is the diagonal matrix with entry equal to and is the diagonal matrix with entry equal to .\nThe singular value decomposition, , provides the partitions of the rows and columns of . A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.\nThe\nsingular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix\nZ\n:\nwhere the columns of\nU\nare\nu\n, and similarly for\nV\n.\nThen the rows of\nZ\nare clustered using k-means. The first n_rows labels provide the row partitioning, and the remaining n_columns labels provide the column partitioning.\nThe input matrix\nA\nis first normalized to make the checkerboard pattern more obvious. There are three possible methods:\nAfter normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.\nIf log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors,\nu\nand\nv\n. are discarded. From now on, the \u201cfirst\u201d singular vectors refers to\nu\nand\nv\nexcept in the case of log normalization.\nGiven these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.\nFor instance, if\np\nsingular vectors were calculated, the\nq\nbest are found as described, where\nq\n. Let\nU\nbe the matrix with columns the\nq\nbest left singular vectors, and similarly\nV\nfor the right. To partition the rows, the rows of\nA\nare projected to a\nq\ndimensional space:\nA\n. Treating the\nm\nrows of this\nm\nmatrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to\nA\nand clustering this\nn\nmatrix yields the column labels.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/biclustering.html", "Attachment_Url": []}, "112": {"Title": "2.4.2. Spectral Biclustering", "Text": "The SpectralBiclustering algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.\nThe algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/biclustering.html", "Attachment_Url": []}, "113": {"Title": "2.4.3. Biclustering evaluation", "Text": "There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.\nTo compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score.\nTo compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:\nwhere\nA\nand\nB\nare biclusters,\n|\nis the number of elements in their intersection. The Jaccard index achieves its minimum of 0 when the biclusters to not overlap at all and its maximum of 1 when they are identical.\nSeveral methods have been developed to compare two sets of biclusters. For now, only consensus_score (Hochreiter et. al., 2010) is available:\nThe minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/biclustering.html", "Attachment_Url": []}, "114": {"Title": "2.5.1. Principal component analysis (PCA)", "Text": "PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns n components in its fit method, and can be used on new data to project it on these components.\nPCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.\nBelow is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:\nThe PCA object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a score method that can be used in cross-validation:\nThe SparseCoder object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a fit method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the transform_method initialization parameter:\nThresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction.\nThe dictionary learning objects offer, via the split_code parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.\nThe split code for a single sample has length 2 * n_components and is constructed using the following rule: First, the regular code of length n_components is computed. Then, the first n_components entries of the split_code are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.\nNMF [1] is an alternative approach to decomposition that assumes that the data and the components are non-negative. NMF can be plugged in instead of PCA or its variants, in the cases where the data matrix does not contain negative values. It finds a decomposition of samples X into two matrices W and H of non-negative elements, by optimizing the distance d between X and the matrix product WH. The most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:\nUnlike PCA, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text.\nIt has been observed in [Hoyer, 2004] [2] that, when carefully constrained, NMF can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by NMF from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.\n\nThe init attribute determines the initialization method applied, which has a great impact on the performance of the method. NMF implements the method Nonnegative Double Singular Value Decomposition. NNDSVD [4] is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.\nNote that the Multiplicative Update (\u2018mu\u2019) solver cannot update zeros present in the initialization, so it leads to poorer results when used jointly with the basic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or NNDSVDar should be preferred.\nNMF can also be initialized with correctly scaled random non-negative matrices by setting init=\"random\". An integer seed or a RandomState can also be passed to random_state to control reproducibility.\nIn NMF, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in ElasticNet, we control the combination of L1 and L2 with the l1_ratio (\n\u03c1\n) parameter, and the intensity of the regularization with the alpha (\n\u03b1\n) parameter. Then the priors terms are:\nand the regularized objective function is:\nNMF regularizes both W and H. The public function non_negative_factorization allows a finer control through the regularization attribute, and may regularize only W, only H, or both.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0031.png"]}, "115": {"Title": "2.5.2. Truncated singular value decomposition and latent semantic analysis", "Text": "The PCA object is very useful, but has certain limitations for large datasets. The biggest limitation is that PCA only supports batch processing, which means all of the data to be processed must fit in main memory. The IncrementalPCA object uses a different form of processing and allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion. IncrementalPCA makes it possible to implement out-of-core Principal Component Analysis either by:\nIncrementalPCA only stores estimates of component and noise variances, in order update explained_variance_ratio_ incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset.\nAs in PCA, IncrementalPCA centers but does not scale the input data for each feature before applying the SVD.\nTruncatedSVD implements a variant of singular value decomposition (SVD) that only computes the\nk\nlargest singular values, where\nk\nis a user-specified parameter.\nWhen truncated SVD is applied to term-document matrices (as returned by CountVectorizer or TfidfVectorizer), this transformation is known as latent semantic analysis (LSA), because it transforms such matrices to a \u201csemantic\u201d space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.\nMathematically, truncated SVD applied to training samples\nX\nproduces a low-rank approximation\nX\n:\nAfter this operation,\nU\nis the transformed training set with\nk\nfeatures (called n_components in the API).\nTo also transform a test set\nX\n, we multiply it with\nV\n:\nTruncatedSVD is very similar to PCA, but differs in that it works on sample matrices\nX\ndirectly instead of their covariance matrices. When the columnwise (per-feature) means of\nX\nare subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. In practical terms, this means that the TruncatedSVD transformer accepts scipy.sparse matrices without the need to densify them, as densifying may fill up memory even for medium-sized document collections.\nWhile the TruncatedSVD transformer works with any (sparse) feature matrix, using it on tf\u2013idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (sublinear_tf=True, use_idf=True) to bring the feature values closer to a Gaussian distribution, compensating for LSA\u2019s erroneous assumptions about textual data.\nDictionary learning (DictionaryLearning) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data.\nRepresenting data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks.\nDictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code.\n\nAfter using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see Sparse coding with a precomputed dictionary).\nIt is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.\n\n\nThe following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.\nAs described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:\nOther distance functions can be used in NMF as, for example, the (generalized) Kullback-Leibler (KL) divergence, also referred as I-divergence:\nOr, the Itakura-Saito (IS) divergence:\nThese three distances are special cases of the beta-divergence family, with\n\u03b2\nrespectively [6]. The beta-divergence are defined by :\nNote that this definition is not valid if\n\u03b2\n, yet it can be continuously extended to the definitions of\nd\nand\nd\nrespectively.\nNMF implements two solvers, using Coordinate Descent (\u2018cd\u2019) [5], and Multiplicative Update (\u2018mu\u2019) [6]. The \u2018mu\u2019 solver can optimize every beta-divergence, including of course the Frobenius norm (\n\u03b2\n), the (generalized) Kullback-Leibler divergence (\n\u03b2\n) and the Itakura-Saito divergence (\n\u03b2\n). Note that for\n\u03b2\n, the \u2018mu\u2019 solver is significantly faster than for other values of\n\u03b2\n. Note also that with a negative (or 0, i.e. \u2018itakura-saito\u2019)\n\u03b2\n, the input matrix cannot contain zero values.\nThe \u2018cd\u2019 solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function.\nNMF is best used with the fit_transform method, which returns the matrix W. The matrix H is stored into the fitted model in the components_ attribute; the method transform will decompose a new matrix X_new based on these stored components:\n", "Code_snippet": ">>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_\n>>> X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\n>>> W_new = model.transform(X_new)", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0061.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0111.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0121.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0131.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0141.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_image_denoising_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_beta_divergence_0011.png"]}, "116": {"Title": "2.5.3. Dictionary Learning", "Text": "It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.\nFor instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.\nThe class PCA used with the optional parameter svd_solver='randomized' is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.\nFor instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size\nn\nand\nn\n, the computation time is less than 1s:\n\nIf we note\nn\nand\nn\n, the time complexity of the randomized PCA is\nO\ninstead of\nO\nfor the exact method implemented in PCA.\nThe memory footprint of randomized PCA is also proportional to\n2\ninstead of\nn\nfor the exact method.\nNote: the implementation of inverse_transform in PCA with svd_solver='randomized' is not the exact inverse transform of transform even when whiten=False (default).\nMiniBatchDictionaryLearning implements a faster, but less accurate version of the dictionary learning algorithm that is better suited for large datasets.\nBy default, MiniBatchDictionaryLearning divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition.\nThe estimator also implements partial_fit, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0061.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0111.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0121.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0131.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0141.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_image_denoising_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_dict_face_patches_0011.png"]}, "117": {"Title": "2.5.4. Factor Analysis", "Text": "KernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels (see Pairwise metrics, Affinities and Kernels). It has many applications including denoising, compression and structured prediction (kernel dependency estimation). KernelPCA supports both transform and inverse_transform.\nIn unsupervised learning we only have a dataset\nX\n. How can this dataset be described mathematically? A very simple continuous latent variable model for\nX\nis\nThe vector\nh\nis called \u201clatent\u201d because it is unobserved.\n\u03f5\nis considered a noise term distributed according to a Gaussian with mean 0 and covariance\n\u03a8\n(i.e.\n\u03f5\n),\n\u03bc\nis some arbitrary offset vector. Such a model is called \u201cgenerative\u201d as it describes how\nx\nis generated from\nh\n. If we use all the\nx\n\u2018s as columns to form a matrix\nX\nand all the\nh\n\u2018s as columns of a matrix\nH\nthen we can write (with suitably defined\nM\nand\nE\n):\nIn other words, we decomposed matrix\nX\n.\nIf\nh\nis given, the above equation automatically implies the following probabilistic interpretation:\nFor a complete probabilistic model we also need a prior distribution for the latent variable\nh\n. The most straightforward assumption (based on the nice properties of the Gaussian distribution) is\nh\n. This yields a Gaussian as the marginal distribution of\nx\n:\nNow, without any further assumptions the idea of having a latent variable\nh\nwould be superfluous \u2013\nx\ncan be completely modelled with a mean and a covariance. We need to impose some more specific structure on one of these two parameters. A simple additional assumption regards the structure of the error covariance\n\u03a8\n:\nBoth models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. FastICA) if non-Gaussian priors on the latent variables are assumed.\nFactor analysis can produce similar components (the columns of its loading matrix) to PCA. However, one can not make any general statements about these components (e.g. whether they are orthogonal):\n\nThe main advantage for Factor Analysis over PCA is that it can model the variance in every direction of the input space independently (heteroscedastic noise):\nThis allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0091.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0081.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_0021.png"]}, "118": {"Title": "2.5.5. Independent component analysis (ICA)", "Text": "SparsePCA is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data.\nMini-batch sparse PCA (MiniBatchSparsePCA) is a variant of SparsePCA that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.\nPrincipal component analysis (PCA) has the disadvantage that the components extracted by this method have exclusively dense expressions, i.e. they have non-zero coefficients when expressed as linear combinations of the original variables. This can make interpretation difficult. In many cases, the real underlying components can be more naturally imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces.\nSparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples.\nThe following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector\nh\n, and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see [Jen09] for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.\n\nNote that there are many different formulations for the Sparse PCA problem. The one implemented here is based on [Mrl09] . The optimization problem solved is a PCA problem (dictionary learning) with an\n\u2113\npenalty on the components:\nThe sparsity-inducing\n\u2113\nnorm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter alpha. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.\nIndependent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the Fast ICA algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.\nIt is classically used to separate mixed signals (a problem known as blind source separation), as in the example below:\nICA can also be used as yet another non linear decomposition that finds components with some sparsity:\n\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_ica_blind_source_separation_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0041.png"]}, "119": {"Title": "2.5.6. Non-negative matrix factorization (NMF or NNMF)", "Text": "", "Code_snippet": ">>> import numpy as np\n>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n>>> from sklearn.decomposition import NMF\n>>> model = NMF(n_components=2, init='random', random_state=0)\n>>> W = model.fit_transform(X)\n>>> H = model.components_\n>>> X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\n>>> W_new = model.transform(X_new)", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_beta_divergence_0011.png"]}, "120": {"Title": "2.5.7. Latent Dirichlet Allocation (LDA)", "Text": "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.\nThe graphical model of LDA is a three-level generative model:\nNote on notations presented in the graphical model above, which can be found in Hoffman et al. (2013):\nIn the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure.\nWhen modeling text corpora, the model assumes the following generative process for a corpus with\nD\ndocuments and\nK\ntopics, with\nK\ncorresponding to n_components in the API:\nFor parameter estimation, the posterior distribution is:\nSince the posterior is intractable, variational Bayesian method uses a simpler distribution\nq\nto approximate it, and those variational parameters\n\u03bb\n,\n\u03d5\n,\n\u03b3\nare optimized to maximize the Evidence Lower Bound (ELBO):\nMaximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between\nq\nand the true posterior\np\n.\nLatentDirichletAllocation implements the online variational Bayes algorithm and supports both online and batch update methods. While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points.\nWhen LatentDirichletAllocation is applied on a \u201cdocument-term\u201d matrix, the matrix will be decomposed into a \u201ctopic-term\u201d matrix and a \u201cdocument-topic\u201d matrix. While \u201ctopic-term\u201d matrix is stored as components_ in the model, \u201cdocument-topic\u201d matrix can be calculated from transform method.\nLatentDirichletAllocation also implements partial_fit method. This is used when data can be fetched sequentially.\nSee also Dimensionality reduction for dimensionality reduction with Neighborhood Components Analysis.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/decomposition.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/lda_model_graph.png"]}, "121": {"Title": "2.6.1. Empirical covariance", "Text": "The covariance matrix of a data set is known to be well approximated by the classical maximum likelihood estimator (or \u201cempirical covariance\u201d), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population\u2019s covariance matrix.\nThe empirical covariance matrix of a sample can be computed using the empirical_covariance function of the package, or by fitting an EmpiricalCovariance object to the data sample with the EmpiricalCovariance.fit method. Be careful that results depend on whether the data are centered, so one may want to use the assume_centered parameter accurately. More precisely, if assume_centered=False, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and assume_centered=True should be used.\nDespite being an unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good estimator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate. Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the shrinkage.\nIn scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the shrunk_covariance method. Also, a shrunk estimator of the covariance can be fitted to data with a ShrunkCovariance object and its ShrunkCovariance.fit method. Again, results depend on whether the data are centered, so one may want to use the assume_centered parameter accurately.\nMathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation :\n\u03a3\n.\nChoosing the amount of shrinkage,\n\u03b1\namounts to setting a bias/variance trade-off, and is discussed below.\nThe Minimum Covariance Determinant estimator is a robust estimator of a data set\u2019s covariance introduced by P.J. Rousseeuw in [3]. The idea is to find a given proportion (h) of \u201cgood\u201d observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (\u201cconsistency step\u201d). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (\u201creweighting step\u201d).\nRousseeuw and Van Driessen [4] developed the FastMCD algorithm in order to compute the Minimum Covariance Determinant. This algorithm is used in scikit-learn when fitting an MCD object to data. The FastMCD algorithm also computes a robust estimate of the data set location at the same time.\nRaw estimates can be accessed as raw_location_ and raw_covariance_ attributes of a MinCovDet robust covariance estimator object.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/covariance.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_vs_empirical_covariance_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_0011.png"]}, "122": {"Title": "2.6.2. Shrunk Covariance", "Text": "In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient\n\u03b1\nthat minimizes the Mean Squared Error between the estimated and the real covariance matrix.\nThe Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the ledoit_wolf function of the sklearn.covariance package, or it can be otherwise obtained by fitting a LedoitWolf object to the same sample.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/covariance.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_covariance_estimation_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lw_vs_oas_0011.png"]}, "123": {"Title": "2.6.3. Sparse inverse covariance", "Text": "Under the assumption that the data are Gaussian distributed, Chen et al. [2] derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf\u2019s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.\nThe OAS estimator of the covariance matrix can be computed on a sample with the oas function of the sklearn.covariance package, or it can be otherwise obtained by fitting an OAS object to the same sample.\nThe matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as covariance selection.\nIn the small-samples situation, in which n_samples is on the order of n_features or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure.\nThe GraphicalLasso estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its alpha parameter, the more sparse the precision matrix. The corresponding GraphicalLassoCV object uses cross-validation to automatically set the alpha parameter.\nThe mathematical formulation is the following:\nWhere\nK\nis the precision matrix to be estimated, and\nS\nis the sample covariance matrix.\n\u2016\nis the sum of the absolute values of off-diagonal coefficients of\nK\n. The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R glasso package.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/covariance.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_covariance_estimation_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lw_vs_oas_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_sparse_cov_0011.png"]}, "124": {"Title": "2.6.4. Robust Covariance Estimation", "Text": "Real data sets are often subject to measurement or recording errors. Regular but uncommon observations may also appear for a variety of reasons. Observations which are very uncommon are called outliers. The empirical covariance estimator and the shrunk covariance estimators presented above are very sensitive to the presence of outliers in the data. Therefore, one should use robust covariance estimators to estimate the covariance of its real data sets. Alternatively, robust covariance estimators can be used to perform outlier detection and discard/downweight some observations according to further processing of the data.\nThe sklearn.covariance package implements a robust estimator of covariance, the Minimum Covariance Determinant [3].\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/covariance.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_robust_vs_empirical_covariance_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_0011.png"]}, "125": {"Title": "2.7.1. Overview of outlier detection methods", "Text": "A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection.\nensemble.IsolationForest and neighbors.LocalOutlierFactor perform reasonably well on the data sets considered here. The svm.OneClassSVM is known to be sensitive to outliers and thus does not perform very well for outlier detection. Finally, covariance.EllipticEnvelope assumes the data is Gaussian and learns an ellipse. For more details on the different estimators refer to the example Comparing anomaly detection algorithms for outlier detection on toy datasets and the sections hereunder.\nOne common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the \u201cshape\u201d of the data, and can define outlying observations as observations which stand far enough from the fit shape.\nThe scikit-learn provides an object covariance.EllipticEnvelope that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.\nFor instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_anomaly_comparison_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_0011.png"]}, "126": {"Title": "2.7.2. Novelty Detection", "Text": "Consider a data set of\nn\nobservations from the same distribution described by\np\nfeatures. Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods.\nIn general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding\np\n-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.\nThe One-Class SVM has been introduced by Sch\u00f6lkopf et al. for that purpose and implemented in the Support Vector Machines module in the svm.OneClassSVM object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The\n\u03bd\nparameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.\nOne efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The ensemble.IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\nThe implementation of ensemble.IsolationForest is based on an ensemble of tree.ExtraTreeRegressor. Following Isolation Forest original paper, the maximum depth of each tree is set to\nwhere\nn\nis the number of samples used to build the tree (see (Liu et al., 2008) for more details).\nThis algorithm is illustrated below.\nThe ensemble.IsolationForest supports warm_start=True which allows you to add more trees to an already fitted model:\n", "Code_snippet": ">>> from sklearn.ensemble import IsolationForest\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n>>> clf = IsolationForest(n_estimators=10, warm_start=True)\n>>> clf.fit(X)  # fit 10 trees  \n>>> clf.set_params(n_estimators=20)  # add 10 more trees  \n>>> clf.fit(X)  # fit the added trees  ", "Url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_oneclass_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_isolation_forest_0011.png"]}, "127": {"Title": "2.7.3. Outlier Detection", "Text": "Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called outliers. Yet, in the case of outlier detection, we don\u2019t have a clean data set representing the population of regular observations that can be used to train any tool.\nAnother efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm.\nThe neighbors.LocalOutlierFactor (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.\nIn practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.\nThe number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).\nThe strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.\nWhen applying LOF for outlier detection, there are no predict, decision_function and score_samples methods but only a fit_predict method. The scores of abnormality of the training samples are accessible through the negative_outlier_factor_ attribute. Note that predict, decision_function and score_samples can be used on new unseen data when LOF is applied for novelty detection, i.e. when the novelty parameter is set to True. See Novelty detection with Local Outlier Factor.\nThis strategy is illustrated below.\n", "Code_snippet": ">>> from sklearn.ensemble import IsolationForest\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n>>> clf = IsolationForest(n_estimators=10, warm_start=True)\n>>> clf.fit(X)  # fit 10 trees  \n>>> clf.set_params(n_estimators=20)  # add 10 more trees  \n>>> clf.fit(X)  # fit the added trees  ", "Url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_isolation_forest_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_lof_outlier_detection_0011.png"]}, "128": {"Title": "2.7.4. Novelty detection with Local Outlier Factor", "Text": "To use neighbors.LocalOutlierFactor for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the novelty parameter set to True before fitting the estimator:\nNote that fit_predict is not available in this case.\nNovelty detection with Local Outlier Factor is illustrated below.\n", "Code_snippet": "lof = LocalOutlierFactor(novelty=True)\nlof.fit(X_train)", "Url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_lof_novelty_detection_0011.png"]}, "129": {"Title": "2.8.1. Density Estimation: Histograms", "Text": "A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure:\n\nA major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data.\nIntuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.\nThis visualization is an example of a kernel density estimation, in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/density.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_kde_1d_0011.png"]}, "130": {"Title": "2.8.2. Kernel Density Estimation", "Text": "Kernel density estimation in scikit-learn is implemented in the sklearn.neighbors.KernelDensity estimator, which uses the Ball Tree or KD Tree for efficient queries (see Nearest Neighbors for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.\nIn the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels:\n\nIt\u2019s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:\nHere we have used kernel='gaussian', as seen above. Mathematically, a kernel is a positive function\nK\nwhich is controlled by the bandwidth parameter\nh\n. Given this kernel form, the density estimate at a point\ny\nwithin a group of points\nis given by:\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.\nsklearn.neighbors.KernelDensity implements several common kernel forms, which are shown in the following figure:\n\nThe form of these kernels is as follows:\nThe kernel density estimator can be used with any of the valid distance metrics (see sklearn.neighbors.DistanceMetric for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the Haversine distance which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:\n\nOne other useful application of kernel density estimation is to learn a non-parametric generative model of a dataset in order to efficiently draw new samples from this generative model. Here is an example of using this process to create a new set of hand-written digits, using a Gaussian kernel learned on a PCA projection of the data:\n\nThe \u201cnew\u201d data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model.\n", "Code_snippet": ">>> from sklearn.neighbors.kde import KernelDensity\n>>> import numpy as np\n>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)\n>>> kde.score_samples(X)\narray([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\n       -0.41076071])", "Url": "https://scikit-learn.org/stable/modules/density.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_kde_1d_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_kde_1d_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_species_kde_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_digits_kde_sampling_0011.png"]}, "131": {"Title": "2.9.1. Restricted Boltzmann machines", "Text": "Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron.\nThe model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides BernoulliRBM, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.\nThe RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (Stochastic Maximum Likelihood) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.\nThe method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.\nThe graphical model of an RBM is a fully-connected bipartite graph.\nThe nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.\nThe energy function measures the quality of a joint assignment:\nIn the formula above,\nb\nand\nc\nare the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:\nThe word restricted refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:\nThe bipartite structure allows for the use of efficient block Gibbs sampling for inference.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_rbm_logistic_classification_0011.png", "https://scikit-learn.org/stable/_images/rbm_graph.png"]}, "132": {"Title": "3.1.1. Computing cross-validated metrics", "Text": "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\nThe following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):\nThe mean score and the 95% confidence interval of the score estimate are hence given by:\nBy default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:\nSee The scoring parameter: defining model evaluation rules for details. In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal.\nWhen the cv argument is an integer, cross_val_score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.\nIt is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:\nAnother option is to use an iterable yielding (train, test) splits as arrays of indices, for example:\nThe cross_validate function differs from cross_val_score in two ways:\nFor single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - ['test_score', 'fit_time', 'score_time']\nAnd for multiple metric evaluation, the return value is a dict with the following keys - ['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']\nreturn_train_score is set to False by default to save computation time. To evaluate the scores on the training set as well you need to be set to True.\nYou may also retain the estimator fitted on each training set by setting return_estimator=True.\nThe multiple metrics can be specified either as a list, tuple or set of predefined scorer names:\nOr as a dict mapping scorer name to a predefined or custom scoring function:\nHere is an example of cross_validate using a single metric:\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.\nThe following cross-validators can be used in such cases.\nNOTE\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it\u2019s safer to use a time-series aware cross-validation scheme Similarly if we know that the generative process has a group structure (samples from collected from different subjects, experiments, measurement devices) it safer to use group-wise cross-validation.\nKFold divides all the samples in\nk\ngroups of samples, called folds (if\nk\n, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using\nk\nfolds, and the fold left out is used for test.\nExample of 2-fold cross-validation on a dataset with 4 samples:\nHere is a visualization of the cross-validation behavior. Note that KFold is not affected by classes or groups.\nEach fold is constituted by two arrays: the first one is related to the training set, and the second one to the test set. Thus, one can create the training/test sets using numpy indexing:\nStratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\nExample of stratified 3-fold cross-validation on a dataset with 10 samples from two slightly unbalanced classes:\nHere is a visualization of the cross-validation behavior.\nRepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times with different randomization in each repetition.\nGroupKFold is a variation of k-fold which ensures that the same group is not represented in both testing and training sets. For example if the data is obtained from different subjects with several samples per-subject and if the model is flexible enough to learn from highly person specific features it could fail to generalize to new subjects. GroupKFold makes it possible to detect this kind of overfitting situations.\nImagine you have three subjects, each with an associated number from 1 to 3:\nEach subject is in a different testing fold, and the same subject is never in both testing and training. Notice that the folds do not have exactly the same size due to the imbalance in the data.\nHere is a visualization of the cross-validation behavior.\nTimeSeriesSplit is a variation of k-fold which returns first\nk\nfolds as train set and the\n(\nth fold as test set. Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them. Also, it adds all surplus data to the first training partition, which is always used to train the model.\nThis class can be used to cross-validate time series data samples that are observed at fixed time intervals.\nExample of 3-split time series cross-validation on a dataset with 6 samples:\nHere is a visualization of the cross-validation behavior.\n", "Code_snippet": ">>> from sklearn.model_selection import cross_val_score\n>>> clf = svm.SVC(kernel='linear', C=1)\n>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n>>> scores                                              \narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])>>> print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\nAccuracy: 0.98 (+/- 0.03)>>> from sklearn import metrics\n>>> scores = cross_val_score(\n...     clf, iris.data, iris.target, cv=5, scoring='f1_macro')\n>>> scores                                              \narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])>>> from sklearn.model_selection import ShuffleSplit\n>>> n_samples = iris.data.shape[0]\n>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n>>> cross_val_score(clf, iris.data, iris.target, cv=cv)  \narray([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])>>> def custom_cv_2folds(X):\n...     n = X.shape[0]\n...     i = 1\n...     while i <= 2:\n...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n...         yield idx, idx\n...         i += 1\n...\n>>> custom_cv = custom_cv_2folds(iris.data)\n>>> cross_val_score(clf, iris.data, iris.target, cv=custom_cv)\narray([1.        , 0.973...])>>> from sklearn import preprocessing\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     iris.data, iris.target, test_size=0.4, random_state=0)\n>>> scaler = preprocessing.StandardScaler().fit(X_train)\n>>> X_train_transformed = scaler.transform(X_train)\n>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n>>> X_test_transformed = scaler.transform(X_test)\n>>> clf.score(X_test_transformed, y_test)  \n0.9333...>>> from sklearn.pipeline import make_pipeline\n>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n>>> cross_val_score(clf, iris.data, iris.target, cv=cv)\n...                                                 \narray([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])>>> from sklearn.model_selection import cross_validate\n>>> from sklearn.metrics import recall_score\n>>> scoring = ['precision_macro', 'recall_macro']\n>>> clf = svm.SVC(kernel='linear', C=1, random_state=0)\n>>> scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,\n...                         cv=5)\n>>> sorted(scores.keys())\n['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\n>>> scores['test_recall_macro']                       \narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])>>> from sklearn.metrics.scorer import make_scorer\n>>> scoring = {'prec_macro': 'precision_macro',\n...            'rec_macro': make_scorer(recall_score, average='macro')}\n>>> scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,\n...                         cv=5, return_train_score=True)\n>>> sorted(scores.keys())                 \n['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n 'train_prec_macro', 'train_rec_macro']\n>>> scores['train_rec_macro']                         \narray([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])>>> scores = cross_validate(clf, iris.data, iris.target,\n...                         scoring='precision_macro', cv=5,\n...                         return_estimator=True)\n>>> sorted(scores.keys())\n['estimator', 'fit_time', 'score_time', 'test_score']>>> import numpy as np\n>>> from sklearn.model_selection import KFold\n\n>>> X = [\"a\", \"b\", \"c\", \"d\"]\n>>> kf = KFold(n_splits=2)\n>>> for train, test in kf.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[0 1] [2 3]>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n>>> y = np.array([0, 1, 0, 1])\n>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]>>> import numpy as np\n>>> from sklearn.model_selection import RepeatedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> random_state = 12883823\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n>>> for train, test in rkf.split(X):\n...     print(\"%s %s\" % (train, test))\n...\n[2 3] [0 1]\n[0 1] [2 3]\n[0 2] [1 3]\n[1 3] [0 2]>>> from sklearn.model_selection import LeaveOneOut\n\n>>> X = [1, 2, 3, 4]\n>>> loo = LeaveOneOut()\n>>> for train, test in loo.split(X):\n...     print(\"%s %s\" % (train, test))\n[1 2 3] [0]\n[0 2 3] [1]\n[0 1 3] [2]\n[0 1 2] [3]>>> from sklearn.model_selection import LeavePOut\n\n>>> X = np.ones(4)\n>>> lpo = LeavePOut(p=2)\n>>> for train, test in lpo.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[1 3] [0 2]\n[1 2] [0 3]\n[0 3] [1 2]\n[0 2] [1 3]\n[0 1] [2 3]>>> from sklearn.model_selection import ShuffleSplit\n>>> X = np.arange(10)\n>>> ss = ShuffleSplit(n_splits=5, test_size=0.25,\n...     random_state=0)\n>>> for train_index, test_index in ss.split(X):\n...     print(\"%s %s\" % (train_index, test_index))\n[9 1 6 7 3 0 5] [2 8 4]\n[2 9 8 0 6 7 4] [3 5 1]\n[4 5 1 0 6 9 7] [2 3 8]\n[2 7 5 8 0 3 4] [6 1 9]\n[4 1 0 6 8 9 3] [5 2 7]>>> from sklearn.model_selection import StratifiedKFold\n\n>>> X = np.ones(10)\n>>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n>>> skf = StratifiedKFold(n_splits=3)\n>>> for train, test in skf.split(X, y):\n...     print(\"%s %s\" % (train, test))\n[2 3 6 7 8 9] [0 1 4 5]\n[0 1 3 4 5 8 9] [2 6 7]\n[0 1 2 4 5 6 7] [3 8 9]>>> from sklearn.model_selection import GroupKFold\n\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n>>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\n>>> gkf = GroupKFold(n_splits=3)\n>>> for train, test in gkf.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[0 1 2 3 4 5] [6 7 8 9]\n[0 1 2 6 7 8 9] [3 4 5]\n[3 4 5 6 7 8 9] [0 1 2]>>> from sklearn.model_selection import TimeSeriesSplit\n\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> tscv = TimeSeriesSplit(n_splits=3)\n>>> print(tscv)  \nTimeSeriesSplit(max_train_size=None, n_splits=3)\n>>> for train, test in tscv.split(X):\n...     print(\"%s %s\" % (train, test))\n[0 1 2] [3]\n[0 1 2 3] [4]\n[0 1 2 3 4] [5]", "Url": "https://scikit-learn.org/stable/modules/cross_validation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0061.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0071.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png"]}, "133": {"Title": "3.1.2. Cross validation iterators", "Text": "The function cross_val_predict has a similar interface to cross_val_score, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).\nThe available cross validation iterators are introduced in the following section.\nThe following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.\nRepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition.\nExample of 2-fold K-Fold repeated 2 times:\nSimilarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times with different randomization in each repetition.\nSome classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in StratifiedKFold and StratifiedShuffleSplit to ensure that relative class frequencies is approximately preserved in each train and validation fold.\nStratifiedShuffleSplit is a variation of ShuffleSplit, which returns stratified splits, i.e which creates splits by preserving the same percentage for each target class as in the complete set.\nHere is a visualization of the cross-validation behavior.\nLeaveOneGroupOut is a cross-validation scheme which holds out the samples according to a third-party provided array of integer groups. This group information can be used to encode arbitrary domain specific pre-defined cross-validation folds.\nEach training set is thus constituted by all the samples except the ones related to a specific group.\nFor example, in the cases of multiple experiments, LeaveOneGroupOut can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:\nAnother common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn.model_selection import KFold\n\n>>> X = [\"a\", \"b\", \"c\", \"d\"]\n>>> kf = KFold(n_splits=2)\n>>> for train, test in kf.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[0 1] [2 3]>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n>>> y = np.array([0, 1, 0, 1])\n>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]>>> import numpy as np\n>>> from sklearn.model_selection import RepeatedKFold\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> random_state = 12883823\n>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n>>> for train, test in rkf.split(X):\n...     print(\"%s %s\" % (train, test))\n...\n[2 3] [0 1]\n[0 1] [2 3]\n[0 2] [1 3]\n[1 3] [0 2]>>> from sklearn.model_selection import LeaveOneOut\n\n>>> X = [1, 2, 3, 4]\n>>> loo = LeaveOneOut()\n>>> for train, test in loo.split(X):\n...     print(\"%s %s\" % (train, test))\n[1 2 3] [0]\n[0 2 3] [1]\n[0 1 3] [2]\n[0 1 2] [3]>>> from sklearn.model_selection import LeavePOut\n\n>>> X = np.ones(4)\n>>> lpo = LeavePOut(p=2)\n>>> for train, test in lpo.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[1 3] [0 2]\n[1 2] [0 3]\n[0 3] [1 2]\n[0 2] [1 3]\n[0 1] [2 3]>>> from sklearn.model_selection import ShuffleSplit\n>>> X = np.arange(10)\n>>> ss = ShuffleSplit(n_splits=5, test_size=0.25,\n...     random_state=0)\n>>> for train_index, test_index in ss.split(X):\n...     print(\"%s %s\" % (train_index, test_index))\n[9 1 6 7 3 0 5] [2 8 4]\n[2 9 8 0 6 7 4] [3 5 1]\n[4 5 1 0 6 9 7] [2 3 8]\n[2 7 5 8 0 3 4] [6 1 9]\n[4 1 0 6 8 9 3] [5 2 7]>>> from sklearn.model_selection import StratifiedKFold\n\n>>> X = np.ones(10)\n>>> y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n>>> skf = StratifiedKFold(n_splits=3)\n>>> for train, test in skf.split(X, y):\n...     print(\"%s %s\" % (train, test))\n[2 3 6 7 8 9] [0 1 4 5]\n[0 1 3 4 5 8 9] [2 6 7]\n[0 1 2 4 5 6 7] [3 8 9]>>> from sklearn.model_selection import GroupKFold\n\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n>>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\n>>> gkf = GroupKFold(n_splits=3)\n>>> for train, test in gkf.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[0 1 2 3 4 5] [6 7 8 9]\n[0 1 2 6 7 8 9] [3 4 5]\n[3 4 5 6 7 8 9] [0 1 2]>>> from sklearn.model_selection import LeaveOneGroupOut\n\n>>> X = [1, 5, 10, 50, 60, 70, 80]\n>>> y = [0, 1, 1, 2, 2, 2, 2]\n>>> groups = [1, 1, 2, 2, 3, 3, 3]\n>>> logo = LeaveOneGroupOut()\n>>> for train, test in logo.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[2 3 4 5 6] [0 1]\n[0 1 4 5 6] [2 3]\n[0 1 2 3] [4 5 6]>>> from sklearn.model_selection import LeavePGroupsOut\n\n>>> X = np.arange(6)\n>>> y = [1, 1, 1, 2, 2, 2]\n>>> groups = [1, 1, 2, 2, 3, 3]\n>>> lpgo = LeavePGroupsOut(n_groups=2)\n>>> for train, test in lpgo.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[4 5] [0 1 2 3]\n[2 3] [0 1 4 5]\n[0 1] [2 3 4 5]>>> from sklearn.model_selection import GroupShuffleSplit\n\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n>>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\n>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\n>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n>>> for train, test in gss.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n...\n[0 1 2 3] [4 5 6 7]\n[2 3 6 7] [0 1 4 5]\n[2 3 4 5] [0 1 6 7]\n[4 5 6 7] [0 1 2 3]>>> from sklearn.model_selection import TimeSeriesSplit\n\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> tscv = TimeSeriesSplit(n_splits=3)\n>>> print(tscv)  \nTimeSeriesSplit(max_train_size=None, n_splits=3)\n>>> for train, test in tscv.split(X):\n...     print(\"%s %s\" % (train, test))\n[0 1 2] [3]\n[0 1 2 3] [4]\n[0 1 2 3 4] [5]", "Url": "https://scikit-learn.org/stable/modules/cross_validation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0061.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0071.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0091.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0081.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png"]}, "134": {"Title": "3.1.3. A note on shuffling", "Text": "LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for\nn\nsamples, we have\nn\ndifferent training sets and\nn\ndifferent tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:\nPotential users of LOO for model selection should weigh a few known caveats. When compared with\nk\n-fold cross validation, one builds\nn\nmodels from\nn\nsamples instead of\nk\nmodels, where\nn\n. Moreover, each is trained on\nn\nsamples rather than\n(\n. In both ways, assuming\nk\nis not too large and\nk\n, LOO is more computationally expensive than\nk\n-fold cross validation.\nIn terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since\nn\nof the\nn\nsamples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.\nHowever, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.\nAs a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.\nThe i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.\nSuch a grouping of data is domain specific. An example would be when there is medical data collected from multiple patients, with multiple samples taken from each patient. And such data is likely to be dependent on the individual group. In our example, the patient id for each sample will be its group identifier.\nIn this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.\nThe following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the groups parameter.\nLeavePGroupsOut is similar as LeaveOneGroupOut, but removes samples related to\nP\ngroups for each training/test set.\nExample of Leave-2-Group Out:\nIf the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.\nSome cross validation iterators, such as KFold, have an inbuilt option to shuffle the data indices before splitting them. Note that:\n", "Code_snippet": ">>> from sklearn.model_selection import LeaveOneOut\n\n>>> X = [1, 2, 3, 4]\n>>> loo = LeaveOneOut()\n>>> for train, test in loo.split(X):\n...     print(\"%s %s\" % (train, test))\n[1 2 3] [0]\n[0 2 3] [1]\n[0 1 3] [2]\n[0 1 2] [3]>>> from sklearn.model_selection import GroupKFold\n\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\n>>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\n>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\n>>> gkf = GroupKFold(n_splits=3)\n>>> for train, test in gkf.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[0 1 2 3 4 5] [6 7 8 9]\n[0 1 2 6 7 8 9] [3 4 5]\n[3 4 5 6 7 8 9] [0 1 2]>>> from sklearn.model_selection import LeaveOneGroupOut\n\n>>> X = [1, 5, 10, 50, 60, 70, 80]\n>>> y = [0, 1, 1, 2, 2, 2, 2]\n>>> groups = [1, 1, 2, 2, 3, 3, 3]\n>>> logo = LeaveOneGroupOut()\n>>> for train, test in logo.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[2 3 4 5 6] [0 1]\n[0 1 4 5 6] [2 3]\n[0 1 2 3] [4 5 6]>>> from sklearn.model_selection import LeavePGroupsOut\n\n>>> X = np.arange(6)\n>>> y = [1, 1, 1, 2, 2, 2]\n>>> groups = [1, 1, 2, 2, 3, 3]\n>>> lpgo = LeavePGroupsOut(n_groups=2)\n>>> for train, test in lpgo.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n[4 5] [0 1 2 3]\n[2 3] [0 1 4 5]\n[0 1] [2 3 4 5]>>> from sklearn.model_selection import GroupShuffleSplit\n\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n>>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\n>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\n>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n>>> for train, test in gss.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n...\n[0 1 2 3] [4 5 6 7]\n[2 3 6 7] [0 1 4 5]\n[2 3 4 5] [0 1 6 7]\n[4 5 6 7] [0 1 2 3]", "Url": "https://scikit-learn.org/stable/modules/cross_validation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0051.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0081.png"]}, "135": {"Title": "3.1.4. Cross validation and model selection", "Text": "LeavePOut is very similar to LeaveOneOut as it creates all the possible training/test sets by removing\np\nsamples from the complete set. For\nn\nsamples, this produces\n(\ntrain-test pairs. Unlike LeaveOneOut and KFold, the test sets will overlap for\np\n.\nExample of Leave-2-Out on a dataset with 4 samples:\nThe GroupShuffleSplit iterator behaves as a combination of ShuffleSplit and LeavePGroupsOut, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.\nHere is a usage example:\nHere is a visualization of the cross-validation behavior.\nThis class is useful when the behavior of LeavePGroupsOut is desired, but the number of groups is large enough that generating all possible partitions with\nP\ngroups withheld would be prohibitively expensive. In such a scenario, GroupShuffleSplit provides a random sample (with replacement) of the train / test splits generated by LeavePGroupsOut.\nFor some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using PredefinedSplit it is possible to use these folds e.g. when searching for hyperparameters.\nFor example, when using a validation set, set the test_fold to 0 for all samples that are part of the validation set, and to -1 for all other samples.\nCross validation iterators can also be used to directly perform model selection using Grid Search for the optimal hyperparameters of the model. This is the topic of the next section: Tuning the hyper-parameters of an estimator.\n", "Code_snippet": ">>> from sklearn.model_selection import LeavePOut\n\n>>> X = np.ones(4)\n>>> lpo = LeavePOut(p=2)\n>>> for train, test in lpo.split(X):\n...     print(\"%s %s\" % (train, test))\n[2 3] [0 1]\n[1 3] [0 2]\n[1 2] [0 3]\n[0 3] [1 2]\n[0 2] [1 3]\n[0 1] [2 3]>>> from sklearn.model_selection import GroupShuffleSplit\n\n>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n>>> y = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\n>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]\n>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\n>>> for train, test in gss.split(X, y, groups=groups):\n...     print(\"%s %s\" % (train, test))\n...\n[0 1 2 3] [4 5 6 7]\n[2 3 6 7] [0 1 4 5]\n[2 3 4 5] [0 1 6 7]\n[4 5 6 7] [0 1 2 3]", "Url": "https://scikit-learn.org/stable/modules/cross_validation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0081.png"]}, "136": {"Title": "3.2.1. Exhaustive Grid Search", "Text": "The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid:\nspecifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001].\nThe GridSearchCV instance implements the usual estimator API: when \u201cfitting\u201d it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.\nBy default, parameter search uses the score function of the estimator to evaluate a parameter setting. These are the sklearn.metrics.accuracy_score for classification and sklearn.metrics.r2_score for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the scoring parameter to GridSearchCV, RandomizedSearchCV and many of the specialized cross-validation tools described below. See The scoring parameter: defining model evaluation rules for more details.\nSome models can fit data for a range of values of some parameter almost as efficiently as fitting the estimator for a single value of the parameter. This feature can be leveraged to perform a more efficient cross-validation used for model selection of this parameter.\nThe most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the regularization path of the estimator.\nHere is the list of such models:\n", "Code_snippet": "param_grid = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]", "Url": "https://scikit-learn.org/stable/modules/grid_search.html", "Attachment_Url": []}, "137": {"Title": "3.2.2. Randomized Parameter Optimization", "Text": "While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\nSpecifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the n_iter parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified:\nThis example uses the scipy.stats module, which contains many useful distributions for sampling parameters, such as expon, gamma, uniform or randint. In principle, any function can be passed that provides a rvs (random variate sample) method to sample a value. A call to the rvs function should provide independent random samples from possible parameter values on consecutive calls.\nFor continuous parameters, such as C above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing n_iter will always lead to a finer search.\nGridSearchCV and RandomizedSearchCV allow specifying multiple metrics for the scoring parameter.\nMultimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See Using multiple metric evaluation for more details.\nWhen specifying multiple metrics, the refit parameter must be set to the metric (string) for which the best_params_ will be found and used to build the best_estimator_ on the whole dataset. If the search should not be refit, set refit=False. Leaving refit to the default value None will result in an error when using multiple metrics.\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV for an example usage.\nSome models can offer an information-theoretic closed-form formula of the optimal estimate of the regularization parameter by computing a single regularization path (instead of several when using cross-validation).\nHere is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:\n", "Code_snippet": "{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n  'kernel': ['rbf'], 'class_weight':['balanced', None]}", "Url": "https://scikit-learn.org/stable/modules/grid_search.html", "Attachment_Url": []}, "138": {"Title": "3.2.3. Tips for parameter search", "Text": "Pipeline: chaining estimators describes building composite estimators whose parameter space can be searched with these tools.\nWhen using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.\nThis left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes \u201cfor free\u201d as no additional data is needed and can be used for model selection.\nThis is currently implemented in the following classes:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/grid_search.html", "Attachment_Url": []}, "139": {"Title": "3.2.4. Alternatives to brute force parameter search", "Text": "Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to \u201ctrain\u201d the parameters of the grid.\nWhen evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a development set (to be fed to the GridSearchCV instance) and an evaluation set to compute performance metrics.\nThis can be done by using the train_test_split utility function.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/grid_search.html", "Attachment_Url": []}, "140": {"Title": "3.3.1. The scoring parameter: defining model evaluation rules", "Text": "Model selection and evaluation using tools, such as model_selection.GridSearchCV and model_selection.cross_val_score, take a scoring parameter that controls what metric they apply to the estimators evaluated.\nFor the most common use cases, you can designate a scorer object with the scoring parameter; the table below shows all possible values. All scorer objects follow the convention that higher return values are better than lower return values. Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated value of the metric.\nUsage examples:\nSome metrics are essentially defined for binary classification tasks (e.g. f1_score, roc_auc_score). In these cases, by default only the positive label is evaluated, assuming by default that the positive class is labelled 1 (though this may be configurable through the pos_label parameter).\nIn extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the average parameter.\nWhile multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell [i, j] has value 1 if sample i has label j and value 0 otherwise.\nIn a binary classification task, the terms \u2018\u2019positive\u2019\u2019 and \u2018\u2019negative\u2019\u2019 refer to the classifier\u2019s prediction, and the terms \u2018\u2019true\u2019\u2019 and \u2018\u2019false\u2019\u2019 refer to whether that prediction corresponds to the external judgment (sometimes known as the \u2018\u2019observation\u2019\u2018). Given these definitions, we can formulate the following table:\nIn this context, we can define the notions of precision, recall and F-measure:\nHere are some small examples in binary classification:\nThe coverage_error function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.\nFormally, given a binary indicator matrix of the ground truth labels y\u2208{0,1}nsamples\u00d7nlabels\nand the score associated with each label\n\u02c6\nf\n\u2208Rnsamples\u00d7nlabels\n, the coverage is defined as\nwith rankij=|{k:\n\u02c6\nf\nik\u2265\n\u02c6\nf\nij}|\n. Given the rank definition, ties in y_scores are broken by giving the maximal rank that would have been assigned to all tied values.\nHere is a small example of usage of this function:\nThe explained_variance_score computes the explained variance regression score.\nIf\n\u02c6\ny\nis the estimated target output, y\nthe corresponding (correct) target output, and Var\nis Variance, the square of the standard deviation, then the explained variance is estimated as follow:\nThe best possible score is 1.0, lower values are worse.\nHere is a small example of usage of the explained_variance_score function:\n", "Code_snippet": ">>> from sklearn import svm, datasets\n>>> from sklearn.model_selection import cross_val_score\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> clf = svm.SVC(gamma='scale', random_state=0)\n>>> cross_val_score(clf, X, y, scoring='recall_macro',\n...                 cv=5)  \narray([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])\n>>> model = svm.SVC()\n>>> cross_val_score(model, X, y, cv=5, scoring='wrong_choice')\nTraceback (most recent call last):\nValueError: 'wrong_choice' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.>>> from sklearn.metrics import fbeta_score, make_scorer\n>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.svm import LinearSVC\n>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n...                     scoring=ftwo_scorer, cv=5)>>> import numpy as np\n>>> def my_custom_loss_func(y_true, y_pred):\n...     diff = np.abs(y_true - y_pred).max()\n...     return np.log1p(diff)\n...\n>>> # score will negate the return value of my_custom_loss_func,\n>>> # which will be np.log(2), 0.693, given the values for X\n>>> # and y defined below.\n>>> score = make_scorer(my_custom_loss_func, greater_is_better=False)\n>>> X = [[1], [1]]\n>>> y = [0, 1]\n>>> from sklearn.dummy import DummyClassifier\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n>>> clf = clf.fit(X, y)\n>>> my_custom_loss_func(clf.predict(X), y) \n0.69...\n>>> score(clf, X, y) \n-0.69...>>> from custom_scorer_module import custom_scoring_function \n>>> cross_val_score(model,\n...  X_train,\n...  y_train,\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n...  cv=5,\n...  n_jobs=-1) >>> scoring = ['accuracy', 'precision']>>> from sklearn.metrics import accuracy_score\n>>> from sklearn.metrics import make_scorer\n>>> scoring = {'accuracy': make_scorer(accuracy_score),\n...            'prec': 'precision'}>>> from sklearn.model_selection import cross_validate\n>>> from sklearn.metrics import confusion_matrix\n>>> # A sample toy binary classification dataset\n>>> X, y = datasets.make_classification(n_classes=2, random_state=0)\n>>> svm = LinearSVC(random_state=0)\n>>> def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n>>> def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n>>> def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n>>> def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n>>> scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n...            'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n>>> cv_results = cross_validate(svm.fit(X, y), X, y,\n...                             scoring=scoring, cv=5)\n>>> # Getting the test set true positive scores\n>>> print(cv_results['test_tp'])  \n[10  9  8  7  8]\n>>> # Getting the test set false negative scores\n>>> print(cv_results['test_fn'])  \n[0 1 2 3 2]>>> from sklearn import metrics\n>>> y_pred = [0, 1, 0, 0]\n>>> y_true = [0, 1, 0, 1]\n>>> metrics.precision_score(y_true, y_pred)\n1.0\n>>> metrics.recall_score(y_true, y_pred)\n0.5\n>>> metrics.f1_score(y_true, y_pred)  \n0.66...\n>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)  \n0.83...\n>>> metrics.fbeta_score(y_true, y_pred, beta=1)  \n0.66...\n>>> metrics.fbeta_score(y_true, y_pred, beta=2) \n0.55...\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)  \n(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\n\n\n>>> import numpy as np\n>>> from sklearn.metrics import precision_recall_curve\n>>> from sklearn.metrics import average_precision_score\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n>>> precision  \narray([0.66..., 0.5       , 1.        , 1.        ])\n>>> recall\narray([1. , 0.5, 0.5, 0. ])\n>>> threshold\narray([0.35, 0.4 , 0.8 ])\n>>> average_precision_score(y_true, y_scores)  \n0.83...>>> import numpy as np\n>>> from sklearn.metrics import coverage_error\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> coverage_error(y_true, y_score)\n2.5>>> from sklearn.metrics import explained_variance_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> explained_variance_score(y_true, y_pred)  \n0.957...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')\n... \narray([0.967..., 1.        ])\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\n... \n0.990...", "Url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "Attachment_Url": []}, "141": {"Title": "3.3.2. Classification metrics", "Text": "The module sklearn.metrics also exposes a set of simple functions measuring a prediction error given ground truth and prediction:\nMetrics available for various machine learning tasks are detailed in sections below.\nMany metrics are not given names to be used as scoring values, sometimes because they require additional parameters, such as fbeta_score. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using make_scorer. That function converts metrics into callables that can be used for model evaluation.\nOne typical use case is to wrap an existing metric function from the library with non-default values for its parameters, such as the beta parameter for the fbeta_score function:\nThe second use case is to build a completely custom scorer object from a simple python function using make_scorer, which can take several parameters:\nHere is an example of building custom scorers, and of using the greater_is_better parameter:\nThe sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the sample_weight parameter.\nSome of these are restricted to the binary classification case:\nOthers also work in the multiclass case:\nSome also work in the multilabel case:\nAnd some work with binary and multilabel (but not multiclass) problems:\nIn the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.\nThe accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\nIn multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\nIf\ny\nis the predicted value of the\ni\n-th sample and\ny\nis the corresponding true value, then the fraction of correct predictions over\nn\nis defined as\nwhere\n1\nis the indicator function.\nIn the multilabel case with binary label indicators:\nIn multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the average argument to the average_precision_score (multilabel only), f1_score, fbeta_score, precision_recall_fscore_support, precision_score and recall_score functions, as described above. Note that if all labels are included, \u201cmicro\u201d-averaging in a multiclass setting will produce precision, recall and\nF\nthat are all identical to accuracy. Also note that \u201cweighted\u201d averaging may produce an F-score that is not between precision and recall.\nTo make this more explicit, consider the following notation:\nThen the metrics are defined as:\nFor multiclass classification with a \u201cnegative class\u201d, it is possible to exclude some labels:\nSimilarly, labels not present in the data sample may be accounted for in macro-averaging.\nThe label_ranking_average_precision_score function implements label ranking average precision (LRAP). This metric is linked to the average_precision_score function, but is based on the notion of label ranking instead of precision and recall.\nLabel ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank.\nFormally, given a binary indicator matrix of the ground truth labels\ny\nand the score associated with each label\nf\n, the average precision is defined as\nwhere\nL\n,\nrank\n,\n|\ncomputes the cardinality of the set (i.e., the number of elements in the set), and\n|\nis the\n\u2113\n\u201cnorm\u201d (which computes the number of nonzero elements in a vector).\nHere is a small example of usage of this function:\nThe max_error function computes the maximum residual error , a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, max_error would be 0 on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.\nIf\ny\nis the predicted value of the\ni\n-th sample, and\ny\nis the corresponding true value, then the max error is defined as\nHere is a small example of usage of the max_error function:\nThe max_error does not support multioutput.\n", "Code_snippet": ">>> from sklearn.metrics import fbeta_score, make_scorer\n>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.svm import LinearSVC\n>>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n...                     scoring=ftwo_scorer, cv=5)>>> import numpy as np\n>>> def my_custom_loss_func(y_true, y_pred):\n...     diff = np.abs(y_true - y_pred).max()\n...     return np.log1p(diff)\n...\n>>> # score will negate the return value of my_custom_loss_func,\n>>> # which will be np.log(2), 0.693, given the values for X\n>>> # and y defined below.\n>>> score = make_scorer(my_custom_loss_func, greater_is_better=False)\n>>> X = [[1], [1]]\n>>> y = [0, 1]\n>>> from sklearn.dummy import DummyClassifier\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n>>> clf = clf.fit(X, y)\n>>> my_custom_loss_func(clf.predict(X), y) \n0.69...\n>>> score(clf, X, y) \n-0.69...>>> import numpy as np\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred = [0, 2, 1, 3]\n>>> y_true = [0, 1, 2, 3]\n>>> accuracy_score(y_true, y_pred)\n0.5\n>>> accuracy_score(y_true, y_pred, normalize=False)\n2>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5>>> from sklearn.metrics import cohen_kappa_score\n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> cohen_kappa_score(y_true, y_pred)\n0.4285714285714286>>> from sklearn.metrics import confusion_matrix\n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> confusion_matrix(y_true, y_pred)\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n>>> tn, fp, fn, tp\n(2, 1, 2, 3)>>> from sklearn.metrics import classification_report\n>>> y_true = [0, 1, 2, 2, 0]\n>>> y_pred = [0, 0, 2, 1, 0]\n>>> target_names = ['class 0', 'class 1', 'class 2']\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\n              precision    recall  f1-score   support\n\n     class 0       0.67      1.00      0.80         2\n     class 1       0.00      0.00      0.00         1\n     class 2       1.00      0.50      0.67         2\n\n    accuracy                           0.60         5\n   macro avg       0.56      0.50      0.49         5\nweighted avg       0.67      0.60      0.59         5>>> from sklearn.metrics import hamming_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> hamming_loss(y_true, y_pred)\n0.25>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n0.75>>> from sklearn import metrics\n>>> y_pred = [0, 1, 0, 0]\n>>> y_true = [0, 1, 0, 1]\n>>> metrics.precision_score(y_true, y_pred)\n1.0\n>>> metrics.recall_score(y_true, y_pred)\n0.5\n>>> metrics.f1_score(y_true, y_pred)  \n0.66...\n>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)  \n0.83...\n>>> metrics.fbeta_score(y_true, y_pred, beta=1)  \n0.66...\n>>> metrics.fbeta_score(y_true, y_pred, beta=2) \n0.55...\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)  \n(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))\n\n\n>>> import numpy as np\n>>> from sklearn.metrics import precision_recall_curve\n>>> from sklearn.metrics import average_precision_score\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)\n>>> precision  \narray([0.66..., 0.5       , 1.        , 1.        ])\n>>> recall\narray([1. , 0.5, 0.5, 0. ])\n>>> threshold\narray([0.35, 0.4 , 0.8 ])\n>>> average_precision_score(y_true, y_scores)  \n0.83...>>> from sklearn import metrics\n>>> y_true = [0, 1, 2, 0, 1, 2]\n>>> y_pred = [0, 2, 1, 0, 0, 1]\n>>> metrics.precision_score(y_true, y_pred, average='macro')  \n0.22...\n>>> metrics.recall_score(y_true, y_pred, average='micro')\n... \n0.33...\n>>> metrics.f1_score(y_true, y_pred, average='weighted')  \n0.26...\n>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)  \n0.23...\n>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)\n... \n(array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')\n... # excluding 0, no labels were correctly recalled\n0.0>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')\n... \n0.166...>>> import numpy as np\n>>> from sklearn.metrics import jaccard_score\n>>> y_true = np.array([[0, 1, 1],\n...                    [1, 1, 0]])\n>>> y_pred = np.array([[1, 1, 1],\n...                    [1, 0, 0]])\n>>> jaccard_score(y_true[0], y_pred[0])  \n0.6666...>>> jaccard_score(y_true, y_pred, average='samples')  \n0.5833...\n>>> jaccard_score(y_true, y_pred, average='macro')  \n0.6666...\n>>> jaccard_score(y_true, y_pred, average=None)\narray([0.5, 0.5, 1. ])>>> y_pred = [0, 2, 1, 2]\n>>> y_true = [0, 1, 2, 2]\n>>> jaccard_score(y_true, y_pred, average=None)\n... \narray([1. , 0. , 0.33...])\n>>> jaccard_score(y_true, y_pred, average='macro')\n0.44...\n>>> jaccard_score(y_true, y_pred, average='micro')\n0.33...>>> from sklearn import svm\n>>> from sklearn.metrics import hinge_loss\n>>> X = [[0], [1]]\n>>> y = [-1, 1]\n>>> est = svm.LinearSVC(random_state=0)\n>>> est.fit(X, y)  \nLinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n     verbose=0)\n>>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n>>> pred_decision  \narray([-2.18...,  2.36...,  0.09...])\n>>> hinge_loss([-1, 1, 1], pred_decision)  \n0.3...>>> X = np.array([[0], [1], [2], [3]])\n>>> Y = np.array([0, 1, 2, 3])\n>>> labels = np.array([0, 1, 2, 3])\n>>> est = svm.LinearSVC()\n>>> est.fit(X, Y)  \nLinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0)\n>>> pred_decision = est.decision_function([[-1], [2], [3]])\n>>> y_true = [0, 2, 3]\n>>> hinge_loss(y_true, pred_decision, labels)  \n0.56...>>> from sklearn.metrics import log_loss\n>>> y_true = [0, 0, 1, 1]\n>>> y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n>>> log_loss(y_true, y_pred)    \n0.1738...>>> from sklearn.metrics import matthews_corrcoef\n>>> y_true = [+1, +1, +1, -1]\n>>> y_pred = [+1, -1, +1, +1]\n>>> matthews_corrcoef(y_true, y_pred)  \n-0.33...>>> import numpy as np\n>>> from sklearn.metrics import multilabel_confusion_matrix\n>>> y_true = np.array([[1, 0, 1],\n...                    [0, 1, 0]])\n>>> y_pred = np.array([[1, 0, 0],\n...                    [0, 1, 1]])\n>>> multilabel_confusion_matrix(y_true, y_pred)\narray([[[1, 0],\n        [0, 1]],\n\n       [[1, 0],\n        [0, 1]],\n\n       [[0, 1],\n        [1, 0]]])>>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\narray([[[1, 0],\n        [1, 1]],\n\n       [[1, 1],\n        [0, 1]]])>>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n>>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n>>> multilabel_confusion_matrix(y_true, y_pred,\n...                             labels=[\"ant\", \"bird\", \"cat\"])\narray([[[3, 1],\n        [0, 2]],\n\n       [[5, 0],\n        [1, 0]],\n\n       [[2, 1],\n        [1, 2]]])>>> y_true = np.array([[0, 0, 1],\n...                    [0, 1, 0],\n...                    [1, 1, 0]])\n>>> y_pred = np.array([[0, 1, 0],\n...                    [0, 0, 1],\n...                    [1, 1, 0]])\n>>> mcm = multilabel_confusion_matrix(y_true, y_pred)\n>>> tn = mcm[:, 0, 0]\n>>> tp = mcm[:, 1, 1]\n>>> fn = mcm[:, 1, 0]\n>>> fp = mcm[:, 0, 1]\n>>> tp / (tp + fn)\narray([1. , 0.5, 0. ])>>> tn / (tn + fp)\narray([1. , 0. , 0.5])>>> fp / (fp + tn)\narray([0. , 1. , 0.5])>>> fn / (fn + tp)\narray([0. , 0.5, 1. ])>>> import numpy as np\n>>> from sklearn.metrics import roc_curve\n>>> y = np.array([1, 1, 2, 2])\n>>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)\n>>> fpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>> tpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>> thresholds\narray([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])>>> import numpy as np\n>>> from sklearn.metrics import roc_auc_score\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> roc_auc_score(y_true, y_scores)\n0.75>>> from sklearn.metrics import zero_one_loss\n>>> y_pred = [1, 2, 3, 4]\n>>> y_true = [2, 2, 3, 4]\n>>> zero_one_loss(y_true, y_pred)\n0.25\n>>> zero_one_loss(y_true, y_pred, normalize=False)\n1>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n0.5\n\n>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)\n1>>> import numpy as np\n>>> from sklearn.metrics import brier_score_loss\n>>> y_true = np.array([0, 1, 1, 0])\n>>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])\n>>> y_pred = np.array([0, 1, 1, 0])\n>>> brier_score_loss(y_true, y_prob)\n0.055\n>>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)\n0.055\n>>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n0.055\n>>> brier_score_loss(y_true, y_prob > 0.5)\n0.0>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_average_precision_score\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_average_precision_score(y_true, y_score) \n0.416...>>> from sklearn.metrics import max_error\n>>> y_true = [3, 2, 7, 1]\n>>> y_pred = [9, 2, 7, 1]\n>>> max_error(y_true, y_pred)\n6", "Url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_roc_0021.png"]}, "142": {"Title": "3.3.3. Multilabel ranking metrics", "Text": "You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the make_scorer factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules:\nThe balanced_accuracy_score function computes the balanced accuracy, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.\nIn the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC curve with binary predictions rather than scores.\nIf the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).\nIn contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to\n1\n.\nThe score ranges from 0 to 1, or when adjusted=True is used, it rescaled to the range\n1\nto 1, inclusive, with performance at random scoring 0.\nIf\ny\nis the true value of the\ni\n-th sample, and\nw\nis the corresponding sample weight, then we adjust the sample weight to:\nwhere\n1\nis the indicator function. Given predicted\ny\nfor sample\ni\n, balanced accuracy is defined as:\nWith adjusted=True, balanced accuracy reports the relative increase from\nbalanced-accuracy\n. In the binary case, this is also known as *Youden\u2019s J statistic*, or informedness.\nIn multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.\nThe label_ranking_loss function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.\nFormally, given a binary indicator matrix of the ground truth labels\ny\nand the score associated with each label\nf\n, the ranking loss is defined as\nwhere\n|\ncomputes the cardinality of the set (i.e., the number of elements in the set) and\n|\nis the\n\u2113\n\u201cnorm\u201d (which computes the number of nonzero elements in a vector).\nHere is a small example of usage of this function:\nThe mean_absolute_error function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or\nl\n-norm loss.\nIf\ny\nis the predicted value of the\ni\n-th sample, and\ny\nis the corresponding true value, then the mean absolute error (MAE) estimated over\nn\nis defined as\nHere is a small example of usage of the mean_absolute_error function:\n", "Code_snippet": ">>> from custom_scorer_module import custom_scoring_function \n>>> cross_val_score(model,\n...  X_train,\n...  y_train,\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\n...  cv=5,\n...  n_jobs=-1) >>> import numpy as np\n>>> from sklearn.metrics import coverage_error\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> coverage_error(y_true, y_score)\n2.5>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_average_precision_score\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_average_precision_score(y_true, y_score) \n0.416...>>> import numpy as np\n>>> from sklearn.metrics import label_ranking_loss\n>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n>>> label_ranking_loss(y_true, y_score) \n0.75...\n>>> # With the following prediction, we have perfect and minimal loss\n>>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\n>>> label_ranking_loss(y_true, y_score)\n0.0>>> from sklearn.metrics import mean_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_absolute_error(y_true, y_pred)\n0.5\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_absolute_error(y_true, y_pred)\n0.75\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n... \n0.85...", "Url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "Attachment_Url": []}, "143": {"Title": "3.3.4. Regression metrics", "Text": "Scikit-learn also permits evaluation of multiple metrics in GridSearchCV, RandomizedSearchCV and cross_validate.\nThere are two ways to specify multiple scoring metrics for the scoring parameter:\nNote that the dict values can either be scorer functions or one of the predefined metric strings.\nCurrently only those scorer functions that return a single score can be passed inside the dict. Scorer functions that return multiple values are not permitted and will require a wrapper to return a single metric:\nThe function cohen_kappa_score computes Cohen\u2019s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nThe sklearn.metrics module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: mean_squared_error, mean_absolute_error, explained_variance_score and r2_score.\nThese functions have an multioutput keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is 'uniform_average', which specifies a uniformly weighted mean over outputs. If an ndarray of shape (n_outputs,) is passed, then its entries are interpreted as weights and an according weighted average is returned. If multioutput is 'raw_values' is specified, then all unaltered individual scores or losses will be returned in an array of shape (n_outputs,).\nThe r2_score and explained_variance_score accept an additional value 'variance_weighted' for the multioutput parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. multioutput='variance_weighted' is the default value for r2_score for backward compatibility. This will be changed to uniform_average in the future.\nThe mean_squared_error function computes mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.\nIf\ny\nis the predicted value of the\ni\n-th sample, and\ny\nis the corresponding true value, then the mean squared error (MSE) estimated over\nn\nis defined as\nHere is a small example of usage of the mean_squared_error function:\n", "Code_snippet": ">>> scoring = ['accuracy', 'precision']>>> from sklearn.metrics import accuracy_score\n>>> from sklearn.metrics import make_scorer\n>>> scoring = {'accuracy': make_scorer(accuracy_score),\n...            'prec': 'precision'}>>> from sklearn.model_selection import cross_validate\n>>> from sklearn.metrics import confusion_matrix\n>>> # A sample toy binary classification dataset\n>>> X, y = datasets.make_classification(n_classes=2, random_state=0)\n>>> svm = LinearSVC(random_state=0)\n>>> def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n>>> def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n>>> def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n>>> def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n>>> scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n...            'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n>>> cv_results = cross_validate(svm.fit(X, y), X, y,\n...                             scoring=scoring, cv=5)\n>>> # Getting the test set true positive scores\n>>> print(cv_results['test_tp'])  \n[10  9  8  7  8]\n>>> # Getting the test set false negative scores\n>>> print(cv_results['test_fn'])  \n[0 1 2 3 2]>>> from sklearn.metrics import cohen_kappa_score\n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> cohen_kappa_score(y_true, y_pred)\n0.4285714285714286>>> from sklearn.metrics import explained_variance_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> explained_variance_score(y_true, y_pred)  \n0.957...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')\n... \narray([0.967..., 1.        ])\n>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\n... \n0.990...>>> from sklearn.metrics import max_error\n>>> y_true = [3, 2, 7, 1]\n>>> y_pred = [9, 2, 7, 1]\n>>> max_error(y_true, y_pred)\n6>>> from sklearn.metrics import mean_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_absolute_error(y_true, y_pred)\n0.5\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_absolute_error(y_true, y_pred)\n0.75\n>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\narray([0.5, 1. ])\n>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n... \n0.85...>>> from sklearn.metrics import mean_squared_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> mean_squared_error(y_true, y_pred)\n0.375\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> mean_squared_error(y_true, y_pred)  \n0.7083...>>> from sklearn.metrics import mean_squared_log_error\n>>> y_true = [3, 5, 2.5, 7]\n>>> y_pred = [2.5, 5, 4, 8]\n>>> mean_squared_log_error(y_true, y_pred)  \n0.039...\n>>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n>>> mean_squared_log_error(y_true, y_pred)  \n0.044...>>> from sklearn.metrics import median_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> median_absolute_error(y_true, y_pred)\n0.5>>> from sklearn.metrics import r2_score\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> r2_score(y_true, y_pred)  \n0.948...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> r2_score(y_true, y_pred, multioutput='variance_weighted')\n... \n0.938...\n>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n>>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n>>> r2_score(y_true, y_pred, multioutput='uniform_average')\n... \n0.936...\n>>> r2_score(y_true, y_pred, multioutput='raw_values')\n... \narray([0.965..., 0.908...])\n>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])\n... \n0.925...", "Url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "Attachment_Url": []}, "144": {"Title": "3.3.5. Clustering metrics", "Text": "The confusion_matrix function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class <https://en.wikipedia.org/wiki/Confusion_matrix>`_. (Wikipedia and other references may use different convention for axes.)\nBy definition, entry\ni\nin a confusion matrix is the number of observations actually in group\ni\n, but predicted to be in group\nj\n. Here is an example:\nHere is a visual representation of such a confusion matrix (this figure comes from the Confusion matrix example):\nFor binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:\nThe mean_squared_log_error function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.\nIf\ny\nis the predicted value of the\ni\n-th sample, and\ny\nis the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over\nn\nis defined as\nWhere\nlog\nmeans the natural logarithm of\nx\n. This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.\nHere is a small example of usage of the mean_squared_log_error function:\nThe sklearn.metrics module implements several loss, score, and utility functions. For more information see the Clustering performance evaluation section for instance clustering, and Biclustering evaluation for biclustering.\n", "Code_snippet": ">>> from sklearn.metrics import confusion_matrix\n>>> y_true = [2, 0, 2, 2, 0, 1]\n>>> y_pred = [0, 0, 2, 2, 0, 2]\n>>> confusion_matrix(y_true, y_pred)\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n>>> tn, fp, fn, tp\n(2, 1, 2, 3)>>> from sklearn.metrics import mean_squared_log_error\n>>> y_true = [3, 5, 2.5, 7]\n>>> y_pred = [2.5, 5, 4, 8]\n>>> mean_squared_log_error(y_true, y_pred)  \n0.039...\n>>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n>>> mean_squared_log_error(y_true, y_pred)  \n0.044...", "Url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_0011.png"]}, "145": {"Title": "3.3.6. Dummy estimators", "Text": "The classification_report function builds a text report showing the main classification metrics. Here is a small example with custom target_names and inferred labels:\nThe median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.\nIf\ny\nis the predicted value of the\ni\n-th sample and\ny\nis the corresponding true value, then the median absolute error (MedAE) estimated over\nn\nis defined as\nThe median_absolute_error does not support multioutput.\nHere is a small example of usage of the median_absolute_error function:\nWhen doing supervised learning, a simple sanity check consists of comparing one\u2019s estimator against simple rules of thumb. DummyClassifier implements several such simple strategies for classification:\nNote that with all these strategies, the predict method completely ignores the input data!\nTo illustrate DummyClassifier, first let\u2019s create an imbalanced dataset:\nNext, let\u2019s compare the accuracy of SVC and most_frequent:\nWe see that SVC doesn\u2019t do much better than a dummy classifier. Now, let\u2019s change the kernel:\nWe see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details.\nMore generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc\u2026\nDummyRegressor also implements four simple rules of thumb for regression:\nIn all these strategies, the predict method completely ignores the input data.\n", "Code_snippet": ">>> from sklearn.metrics import classification_report\n>>> y_true = [0, 1, 2, 2, 0]\n>>> y_pred = [0, 0, 2, 1, 0]\n>>> target_names = ['class 0', 'class 1', 'class 2']\n>>> print(classification_report(y_true, y_pred, target_names=target_names))\n              precision    recall  f1-score   support\n\n     class 0       0.67      1.00      0.80         2\n     class 1       0.00      0.00      0.00         1\n     class 2       1.00      0.50      0.67         2\n\n    accuracy                           0.60         5\n   macro avg       0.56      0.50      0.49         5\nweighted avg       0.67      0.60      0.59         5>>> from sklearn.metrics import median_absolute_error\n>>> y_true = [3, -0.5, 2, 7]\n>>> y_pred = [2.5, 0.0, 2, 8]\n>>> median_absolute_error(y_true, y_pred)\n0.5>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> y[y != 1] = -1\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)>>> from sklearn.dummy import DummyClassifier\n>>> from sklearn.svm import SVC\n>>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)\n>>> clf.score(X_test, y_test) \n0.63...\n>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)\n>>> clf.fit(X_train, y_train)\nDummyClassifier(constant=None, random_state=0, strategy='most_frequent')\n>>> clf.score(X_test, y_test)  \n0.57...>>> clf = SVC(gamma='scale', kernel='rbf', C=1).fit(X_train, y_train)\n>>> clf.score(X_test, y_test)  \n0.94...", "Url": "https://scikit-learn.org/stable/modules/model_evaluation.html", "Attachment_Url": []}, "146": {"Title": "3.4.1. Persistence example", "Text": "It is possible to save a model in scikit-learn by using Python\u2019s built-in persistence model, namely pickle:\nIn the specific case of scikit-learn, it may be better to use joblib\u2019s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:\nLater you can load back the pickled model (possibly in another Python process) with:\n", "Code_snippet": ">>> from sklearn import svm\n>>> from sklearn import datasets\n>>> clf = svm.SVC(gamma='scale')\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data, iris.target\n>>> clf.fit(X, y)  \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\n\n>>> import pickle\n>>> s = pickle.dumps(clf)\n>>> clf2 = pickle.loads(s)\n>>> clf2.predict(X[0:1])\narray([0])\n>>> y[0]\n0>>> from joblib import dump, load\n>>> dump(clf, 'filename.joblib') >>> clf = load('filename.joblib') ", "Url": "https://scikit-learn.org/stable/modules/model_persistence.html", "Attachment_Url": []}, "147": {"Title": "3.4.2. Security & maintainability limitations", "Text": "pickle (and joblib by extension), has some issues regarding maintainability and security. Because of this,\nIn order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\nThis should make it possible to check that the cross-validation score is in the same range as before.\nSince a model internal representation may be different on two different architectures, dumping a model on one architecture and loading it on another architecture is not supported.\nIf you want to know more about these issues and explore other possible serialization methods, please refer to this talk by Alex Gaynor.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/model_persistence.html", "Attachment_Url": []}, "148": {"Title": "3.5.1. Validation curve", "Text": "To validate a model we need a scoring function (see Model evaluation: quantifying the quality of predictions), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.\nHowever, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.\nThe function validation_curve can help in this case:\nIf the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter\n\u03b3\nof an SVM on the digits dataset.\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn.model_selection import validation_curve\n>>> from sklearn.datasets import load_iris\n>>> from sklearn.linear_model import Ridge\n\n>>> np.random.seed(0)\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> indices = np.arange(y.shape[0])\n>>> np.random.shuffle(indices)\n>>> X, y = X[indices], y[indices]\n\n>>> train_scores, valid_scores = validation_curve(Ridge(), X, y, \"alpha\",\n...                                               np.logspace(-7, 3, 3),\n...                                               cv=5)\n>>> train_scores            \narray([[0.93..., 0.94..., 0.92..., 0.91..., 0.92...],\n       [0.93..., 0.94..., 0.92..., 0.91..., 0.92...],\n       [0.51..., 0.52..., 0.49..., 0.47..., 0.49...]])\n>>> valid_scores           \narray([[0.90..., 0.84..., 0.94..., 0.96..., 0.93...],\n       [0.90..., 0.84..., 0.94..., 0.96..., 0.93...],\n       [0.46..., 0.25..., 0.50..., 0.49..., 0.52...]])", "Url": "https://scikit-learn.org/stable/modules/learning_curve.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_validation_curve_0011.png"]}, "149": {"Title": "3.5.2. Learning curve", "Text": "A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.\nWe will probably have to use an estimator or a parametrization of the current estimator that can learn more complex concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples, adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.\nWe can use the function learning_curve to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):\n", "Code_snippet": ">>> from sklearn.model_selection import learning_curve\n>>> from sklearn.svm import SVC\n\n>>> train_sizes, train_scores, valid_scores = learning_curve(\n...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)\n>>> train_sizes            \narray([ 50, 80, 110])\n>>> train_scores           \narray([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],\n       [0.98..., 1.   , 0.98..., 0.98..., 0.98...],\n       [0.98..., 1.   , 0.98..., 0.98..., 0.99...]])\n>>> valid_scores           \narray([[1. ,  0.93...,  1. ,  1. ,  0.96...],\n       [1. ,  0.96...,  1. ,  1. ,  0.96...],\n       [1. ,  0.96...,  1. ,  1. ,  0.96...]])", "Url": "https://scikit-learn.org/stable/modules/learning_curve.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_0021.png"]}, "150": {"Title": "5.1.1. Pipeline: chaining estimators", "Text": "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\nAll estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.).\nThe Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object:\nThe utility function make_pipeline is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:\nA FeatureUnion is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object:\nLike pipelines, feature unions have a shorthand constructor called make_union that does not require explicit naming of the components.\nLike Pipeline, individual steps may be replaced using set_params, and ignored by setting to 'drop':\n", "Code_snippet": ">>> from sklearn.pipeline import Pipeline\n>>> from sklearn.svm import SVC\n>>> from sklearn.decomposition import PCA\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n>>> pipe = Pipeline(estimators)\n>>> pipe \nPipeline(memory=None,\n         steps=[('reduce_dim', PCA(copy=True,...)),\n                ('clf', SVC(C=1.0,...))], verbose=False)>>> from sklearn.pipeline import make_pipeline\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn.preprocessing import Binarizer\n>>> make_pipeline(Binarizer(), MultinomialNB()) \nPipeline(memory=None,\n         steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),\n                ('multinomialnb', MultinomialNB(alpha=1.0,\n                                                class_prior=None,\n                                                fit_prior=True))],\n         verbose=False)>>> pipe.steps[0]  \n('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None,\n                   random_state=None, svd_solver='auto', tol=0.0,\n                   whiten=False))\n>>> pipe[0]  \nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False)\n>>> pipe['reduce_dim']  \nPCA(copy=True, ...)>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\nTrue>>> pipe[:1] \nPipeline(memory=None, steps=[('reduce_dim', PCA(copy=True, ...))],...)\n>>> pipe[-1:] \nPipeline(memory=None, steps=[('clf', SVC(C=1.0, ...))],...)>>> pipe.set_params(clf__C=10) \nPipeline(memory=None,\n         steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),\n                ('clf', SVC(C=10, cache_size=200, class_weight=None,...))],\n         verbose=False)>>> from sklearn.model_selection import GridSearchCV\n>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\n...                   clf__C=[0.1, 10, 100])\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)>>> from sklearn.linear_model import LogisticRegression\n>>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n...                   clf=[SVC(), LogisticRegression()],\n...                   clf__C=[0.1, 10, 100])\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)>>> pipe[0]  \nPCA(copy=True, ...)>>> pipe['reduce_dim']  \nPCA(copy=True, ...)>>> from tempfile import mkdtemp\n>>> from shutil import rmtree\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.svm import SVC\n>>> from sklearn.pipeline import Pipeline\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n>>> cachedir = mkdtemp()\n>>> pipe = Pipeline(estimators, memory=cachedir)\n>>> pipe \nPipeline(...,\n         steps=[('reduce_dim', PCA(copy=True,...)),\n                ('clf', SVC(C=1.0,...))], verbose=False)\n>>> # Clear the cache directory when you don't need it anymore\n>>> rmtree(cachedir)>>> from sklearn.datasets import load_digits\n>>> digits = load_digits()\n>>> pca1 = PCA()\n>>> svm1 = SVC(gamma='scale')\n>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\n>>> pipe.fit(digits.data, digits.target)\n... \nPipeline(memory=None,\n         steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))],\n         verbose=False)\n>>> # The pca instance can be inspected directly\n>>> print(pca1.components_) \n    [[-1.77484909e-19  ... 4.07058917e-18]]>>> cachedir = mkdtemp()\n>>> pca2 = PCA()\n>>> svm2 = SVC(gamma='scale')\n>>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],\n...                        memory=cachedir)\n>>> cached_pipe.fit(digits.data, digits.target)\n... \n Pipeline(memory=...,\n          steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))],\n          verbose=False)\n>>> print(cached_pipe.named_steps['reduce_dim'].components_)\n... \n    [[-1.77484909e-19  ... 4.07058917e-18]]\n>>> # Remove the cache directory\n>>> rmtree(cachedir)>>> from sklearn.pipeline import FeatureUnion\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.decomposition import KernelPCA\n>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n>>> combined = FeatureUnion(estimators)\n>>> combined \nFeatureUnion(n_jobs=None,\n             transformer_list=[('linear_pca', PCA(copy=True,...)),\n                               ('kernel_pca', KernelPCA(alpha=1.0,...))],\n             transformer_weights=None, verbose=False)>>> combined.set_params(kernel_pca='drop')\n... \nFeatureUnion(n_jobs=None,\n             transformer_list=[('linear_pca', PCA(copy=True,...)),\n                               ('kernel_pca', 'drop')],\n             transformer_weights=None, verbose=False)", "Url": "https://scikit-learn.org/stable/modules/compose.html", "Attachment_Url": []}, "151": {"Title": "5.1.2. Transforming target in regression", "Text": "The estimators of a pipeline are stored as a list in the steps attribute, but can be accessed by index or name by indexing (with [idx]) the Pipeline:\nPipeline\u2019s named_steps attribute allows accessing steps by name with tab completion in interactive environments:\nA sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse):\nCalling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.\nTransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable:\nFor simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:\nSubsequently, the object is created as:\nBy default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting check_inverse to False:\n", "Code_snippet": ">>> pipe.steps[0]  \n('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None,\n                   random_state=None, svd_solver='auto', tol=0.0,\n                   whiten=False))\n>>> pipe[0]  \nPCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False)\n>>> pipe['reduce_dim']  \nPCA(copy=True, ...)>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']\nTrue>>> pipe[:1] \nPipeline(memory=None, steps=[('reduce_dim', PCA(copy=True, ...))],...)\n>>> pipe[-1:] \nPipeline(memory=None, steps=[('clf', SVC(C=1.0, ...))],...)>>> import numpy as np\n>>> from sklearn.datasets import load_boston\n>>> from sklearn.compose import TransformedTargetRegressor\n>>> from sklearn.preprocessing import QuantileTransformer\n>>> from sklearn.linear_model import LinearRegression\n>>> from sklearn.model_selection import train_test_split\n>>> boston = load_boston()\n>>> X = boston.data\n>>> y = boston.target\n>>> transformer = QuantileTransformer(output_distribution='normal')\n>>> regressor = LinearRegression()\n>>> regr = TransformedTargetRegressor(regressor=regressor,\n...                                   transformer=transformer)\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n>>> regr.fit(X_train, y_train) \nTransformedTargetRegressor(...)\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\nR2 score: 0.67\n>>> raw_target_regr = LinearRegression().fit(X_train, y_train)\n>>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))\nR2 score: 0.64>>> def func(x):\n...     return np.log(x)\n>>> def inverse_func(x):\n...     return np.exp(x)>>> regr = TransformedTargetRegressor(regressor=regressor,\n...                                   func=func,\n...                                   inverse_func=inverse_func)\n>>> regr.fit(X_train, y_train) \nTransformedTargetRegressor(...)\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\nR2 score: 0.65>>> def inverse_func(x):\n...     return x\n>>> regr = TransformedTargetRegressor(regressor=regressor,\n...                                   func=func,\n...                                   inverse_func=inverse_func,\n...                                   check_inverse=False)\n>>> regr.fit(X_train, y_train) \nTransformedTargetRegressor(...)\n>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\nR2 score: -4.50", "Url": "https://scikit-learn.org/stable/modules/compose.html", "Attachment_Url": []}, "152": {"Title": "5.1.3. FeatureUnion: composite feature spaces", "Text": "Parameters of the estimators in the pipeline can be accessed using the <estimator>__<parameter> syntax:\nThis is particularly important for doing grid searches:\nIndividual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to 'passthrough':\nThe estimators of the pipeline can be retrieved by index:\nor by name:\nFitting transformers may be computationally expensive. With its memory parameter set, Pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.\nThe parameter memory is needed in order to cache the transformers. memory can be either a string containing the directory where to cache the transformers or a joblib.Memory object:\nFeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.\nWhen you want to apply different transformations to each field of the data, see the related class sklearn.compose.ColumnTransformer (see user guide).\nFeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.\nFeatureUnion and Pipeline can be combined to create complex models.\n(A FeatureUnion has no way of checking whether two transformers might produce identical features. It only produces a union when the feature sets are disjoint, and making sure they are the caller\u2019s responsibility.)\n", "Code_snippet": ">>> pipe.set_params(clf__C=10) \nPipeline(memory=None,\n         steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),\n                ('clf', SVC(C=10, cache_size=200, class_weight=None,...))],\n         verbose=False)>>> from sklearn.model_selection import GridSearchCV\n>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],\n...                   clf__C=[0.1, 10, 100])\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)>>> from sklearn.linear_model import LogisticRegression\n>>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],\n...                   clf=[SVC(), LogisticRegression()],\n...                   clf__C=[0.1, 10, 100])\n>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)>>> pipe[0]  \nPCA(copy=True, ...)>>> pipe['reduce_dim']  \nPCA(copy=True, ...)>>> from tempfile import mkdtemp\n>>> from shutil import rmtree\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.svm import SVC\n>>> from sklearn.pipeline import Pipeline\n>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]\n>>> cachedir = mkdtemp()\n>>> pipe = Pipeline(estimators, memory=cachedir)\n>>> pipe \nPipeline(...,\n         steps=[('reduce_dim', PCA(copy=True,...)),\n                ('clf', SVC(C=1.0,...))], verbose=False)\n>>> # Clear the cache directory when you don't need it anymore\n>>> rmtree(cachedir)>>> from sklearn.datasets import load_digits\n>>> digits = load_digits()\n>>> pca1 = PCA()\n>>> svm1 = SVC(gamma='scale')\n>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])\n>>> pipe.fit(digits.data, digits.target)\n... \nPipeline(memory=None,\n         steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))],\n         verbose=False)\n>>> # The pca instance can be inspected directly\n>>> print(pca1.components_) \n    [[-1.77484909e-19  ... 4.07058917e-18]]>>> cachedir = mkdtemp()\n>>> pca2 = PCA()\n>>> svm2 = SVC(gamma='scale')\n>>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],\n...                        memory=cachedir)\n>>> cached_pipe.fit(digits.data, digits.target)\n... \n Pipeline(memory=...,\n          steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))],\n          verbose=False)\n>>> print(cached_pipe.named_steps['reduce_dim'].components_)\n... \n    [[-1.77484909e-19  ... 4.07058917e-18]]\n>>> # Remove the cache directory\n>>> rmtree(cachedir)>>> from sklearn.pipeline import FeatureUnion\n>>> from sklearn.decomposition import PCA\n>>> from sklearn.decomposition import KernelPCA\n>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\n>>> combined = FeatureUnion(estimators)\n>>> combined \nFeatureUnion(n_jobs=None,\n             transformer_list=[('linear_pca', PCA(copy=True,...)),\n                               ('kernel_pca', KernelPCA(alpha=1.0,...))],\n             transformer_weights=None, verbose=False)>>> combined.set_params(kernel_pca='drop')\n... \nFeatureUnion(n_jobs=None,\n             transformer_list=[('linear_pca', PCA(copy=True,...)),\n                               ('kernel_pca', 'drop')],\n             transformer_weights=None, verbose=False)", "Url": "https://scikit-learn.org/stable/modules/compose.html", "Attachment_Url": []}, "153": {"Title": "5.1.4. ColumnTransformer for heterogeneous data", "Text": "Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using pandas. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:\nThe ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames.\nTo each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:\nFor this data, we might want to encode the 'city' column as a categorical variable using preprocessing.OneHotEncoder but apply a feature_extraction.text.CountVectorizer to the 'title' column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say 'city_category' and 'title_bow'. By default, the remaining rating columns are ignored (remainder='drop'):\nIn the above example, the CountVectorizer expects a 1D array as input and therefore the columns were specified as a string ('title'). However, preprocessing.OneHotEncoder as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (['city']).\nApart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, or a boolean mask. Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.\nWe can keep the remaining rating columns by setting remainder='passthrough'. The values are appended to the end of the transformation:\nThe remainder parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:\nThe make_column_transformer function is available to more easily create a ColumnTransformer object. Specifically, the names will be given automatically. The equivalent for the above example would be:\n", "Code_snippet": ">>> import pandas as pd\n>>> X = pd.DataFrame(\n...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],\n...      'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n...                \"A Moveable Feast\", \"The Grapes of Wrath\"],\n...      'expert_rating': [5, 3, 4, 5],\n...      'user_rating': [4, 5, 4, 3]})>>> from sklearn.compose import ColumnTransformer\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> column_trans = ColumnTransformer(\n...     [('city_category', OneHotEncoder(dtype='int'),['city']),\n...      ('title_bow', CountVectorizer(), 'title')],\n...     remainder='drop')\n\n>>> column_trans.fit(X) \nColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n    transformer_weights=None,\n    transformers=...)\n\n>>> column_trans.get_feature_names()\n... \n['city_category__x0_London', 'city_category__x0_Paris', 'city_category__x0_Sallisaw',\n'title_bow__bow', 'title_bow__feast', 'title_bow__grapes', 'title_bow__his',\n'title_bow__how', 'title_bow__last', 'title_bow__learned', 'title_bow__moveable',\n'title_bow__of', 'title_bow__the', 'title_bow__trick', 'title_bow__watson',\n'title_bow__wrath']\n\n>>> column_trans.transform(X).toarray()\n... \narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)>>> column_trans = ColumnTransformer(\n...     [('city_category', OneHotEncoder(dtype='int'),['city']),\n...      ('title_bow', CountVectorizer(), 'title')],\n...     remainder='passthrough')\n\n>>> column_trans.fit_transform(X)\n... \narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\n       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\n       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)>>> from sklearn.preprocessing import MinMaxScaler\n>>> column_trans = ColumnTransformer(\n...     [('city_category', OneHotEncoder(), ['city']),\n...      ('title_bow', CountVectorizer(), 'title')],\n...     remainder=MinMaxScaler())\n\n>>> column_trans.fit_transform(X)[:, -2:]\n... \narray([[1. , 0.5],\n       [0. , 1. ],\n       [0.5, 0.5],\n       [1. , 0. ]])>>> from sklearn.compose import make_column_transformer\n>>> column_trans = make_column_transformer(\n...     (OneHotEncoder(), ['city']),\n...     (CountVectorizer(), 'title'),\n...     remainder=MinMaxScaler())\n>>> column_trans \nColumnTransformer(n_jobs=None, remainder=MinMaxScaler(copy=True, ...),\n         sparse_threshold=0.3,\n         transformer_weights=None,\n         transformers=[('onehotencoder', ...)", "Url": "https://scikit-learn.org/stable/modules/compose.html", "Attachment_Url": []}, "154": {"Title": "5.2.1. Loading features from dicts", "Text": "The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators.\nWhile not particularly fast to process, Python\u2019s dict has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values.\nDictVectorizer implements what is called one-of-K or \u201cone-hot\u201d coding for categorical (aka nominal, discrete) features. Categorical features are \u201cattribute-value\u201d pairs where the value is restricted to a list of discrete of possibilities without ordering (e.g. topic identifiers, types of objects, tags, names\u2026).\nIn the following, \u201ccity\u201d is a categorical attribute while \u201ctemperature\u201d is a traditional numerical feature:\nDictVectorizer is also a useful representation transformation for training sequence classifiers in Natural Language Processing models that typically work by extracting feature windows around a particular word of interest.\nFor example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word \u2018sat\u2019 in the sentence \u2018The cat sat on the mat.\u2019:\nThis description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a text.TfidfTransformer for normalization):\nAs you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the DictVectorizer class uses a scipy.sparse matrix by default instead of a numpy.ndarray.\nFeatureHasher uses the signed 32-bit variant of MurmurHash3. As a result (and because of limitations in scipy.sparse), the maximum number of features supported is currently .\nThe original formulation of the hashing trick by Weinberger et al. used two separate hash functions and to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.\nSince a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the n_features parameter; otherwise the features will not be mapped evenly to the columns.\nText Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\nIn order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\nIn this scheme, features and samples are defined as follows:\nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\nStop words are words like \u201cand\u201d, \u201cthe\u201d, \u201chim\u201d, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.\nThere are several known issues in our provided \u2018english\u2019 stop word list. See [NQY18].\nPlease take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer.\nYou should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word we\u2019ve is split into we and ve by CountVectorizer\u2019s default tokenizer, so if we\u2019ve is in stop_words, but ve is not, ve will be retained from we\u2019ve in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.\nThe extract_patches_2d function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use reconstruct_from_patches_2d. For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):\nLet us now try to reconstruct the original image from the patches by averaging on overlapping areas:\nThe PatchExtractor class works in the same way as extract_patches_2d, only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See:\n", "Code_snippet": ">>> measurements = [\n...     {'city': 'Dubai', 'temperature': 33.},\n...     {'city': 'London', 'temperature': 12.},\n...     {'city': 'San Francisco', 'temperature': 18.},\n... ]\n\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> vec = DictVectorizer()\n\n>>> vec.fit_transform(measurements).toarray()\narray([[ 1.,  0.,  0., 33.],\n       [ 0.,  1.,  0., 12.],\n       [ 0.,  0.,  1., 18.]])\n\n>>> vec.get_feature_names()\n['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']>>> pos_window = [\n...     {\n...         'word-2': 'the',\n...         'pos-2': 'DT',\n...         'word-1': 'cat',\n...         'pos-1': 'NN',\n...         'word+1': 'on',\n...         'pos+1': 'PP',\n...     },\n...     # in a real application one would extract many such dictionaries\n... ]>>> vec = DictVectorizer()\n>>> pos_vectorized = vec.fit_transform(pos_window)\n>>> pos_vectorized                \n<1x6 sparse matrix of type '<... 'numpy.float64'>'\n    with 6 stored elements in Compressed Sparse ... format>\n>>> pos_vectorized.toarray()\narray([[1., 1., 1., 1., 1., 1.]])\n>>> vec.get_feature_names()\n['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']>>> import numpy as np\n>>> from sklearn.feature_extraction import image\n\n>>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n>>> one_image[:, :, 0]  # R channel of a fake RGB picture\narray([[ 0,  3,  6,  9],\n       [12, 15, 18, 21],\n       [24, 27, 30, 33],\n       [36, 39, 42, 45]])\n\n>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\n...     random_state=0)\n>>> patches.shape\n(2, 2, 2, 3)\n>>> patches[:, :, :, 0]\narray([[[ 0,  3],\n        [12, 15]],\n\n       [[15, 18],\n        [27, 30]]])\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\n>>> patches.shape\n(9, 2, 2, 3)\n>>> patches[4, :, :, 0]\narray([[15, 18],\n       [27, 30]])>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\n>>> np.testing.assert_array_equal(one_image, reconstructed)>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n>>> patches = image.PatchExtractor((2, 2)).transform(five_images)\n>>> patches.shape\n(45, 2, 2, 3)", "Url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "Attachment_Url": []}, "155": {"Title": "5.2.2. Feature hashing", "Text": "The class FeatureHasher is a high-speed, low-memory vectorizer that uses a technique known as feature hashing, or the \u201chashing trick\u201d. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of FeatureHasher apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no inverse_transform method.\nSince the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature\u2019s value is zero. This mechanism is enabled by default with alternate_sign=True and is particularly useful for small hash table sizes (n_features < 10000). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like sklearn.naive_bayes.MultinomialNB or sklearn.feature_selection.chi2 feature selectors that expect non-negative inputs.\nFeatureHasher accepts either mappings (like Python\u2019s dict and its variants in the collections module), (feature, value) pairs, or strings, depending on the constructor parameter input_type. Mapping are treated as lists of (feature, value) pairs, while single strings have an implicit value of 1, so ['feat1', 'feat2', 'feat3'] is interpreted as [('feat1', 1), ('feat2', 1), ('feat3', 1)]. If a single feature occurs multiple times in a sample, the associated values will be summed (so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)). The output from FeatureHasher is always a scipy.sparse matrix in the CSR format.\nFeature hashing can be employed in document classification, but unlike text.CountVectorizer, FeatureHasher does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.\nAs an example, consider a word-level natural language processing task that needs features extracted from (token, part_of_speech) pairs. One could use a Python generator function to extract features:\nThen, the raw_X to be fed to FeatureHasher.transform can be constructed using:\nand fed to a hasher with:\nto get a scipy.sparse matrix X.\nNote the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only processed on demand from the hasher.\nAs most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\nFor instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\nIn order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the scipy.sparse package.\nSeveral estimators in the scikit-learn can use connectivity information between features or samples. For instance Ward clustering (Hierarchical clustering) can cluster together only neighboring pixels of an image, thus forming contiguous patches:\nFor this purpose, the estimators use a \u2018connectivity\u2019 matrix, giving which samples are connected.\nThe function img_to_graph returns such a matrix from a 2D or 3D image. Similarly, grid_to_graph build a connectivity matrix for images given the shape of these image.\nThese matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (Hierarchical clustering), but also to build precomputed kernels, or similarity matrices.\n", "Code_snippet": "def token_features(token, part_of_speech):\n    if token.isdigit():\n        yield \"numeric\"\n    else:\n        yield \"token={}\".format(token.lower())\n        yield \"token,pos={},{}\".format(token, part_of_speech)\n    if token[0].isupper():\n        yield \"uppercase_initial\"\n    if token.isupper():\n        yield \"all_uppercase\"\n    yield \"pos={}\".format(part_of_speech)raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)hasher = FeatureHasher(input_type='string')\nX = hasher.transform(raw_X)", "Url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_ward_segmentation_0011.png"]}, "156": {"Title": "5.2.3. Text feature extraction", "Text": "CountVectorizer implements both tokenization and occurrence counting in a single class:\nThis model has many parameters, however the default values are quite reasonable (please see the reference documentation for the details):\nLet\u2019s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:\nThe default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:\nEach term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows:\nThe converse mapping from feature name to column index is stored in the vocabulary_ attribute of the vectorizer:\nHence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:\nNote that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. In particular we lose the information that the last document is an interrogative form. To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words):\nThe vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:\nIn particular the interrogative form \u201cIs this\u201d is only present in the last document:\n", "Code_snippet": ">>> from sklearn.feature_extraction.text import CountVectorizer>>> vectorizer = CountVectorizer()\n>>> vectorizer                     \nCountVectorizer(analyzer=...'word', binary=False, decode_error=...'strict',\n        dtype=<... 'numpy.int64'>, encoding=...'utf-8', input=...'content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip_accents=None, token_pattern=...'(?u)\\\\b\\\\w\\\\w+\\\\b',\n        tokenizer=None, vocabulary=None)>>> corpus = [\n...     'This is the first document.',\n...     'This is the second second document.',\n...     'And the third one.',\n...     'Is this the first document?',\n... ]\n>>> X = vectorizer.fit_transform(corpus)\n>>> X                              \n<4x9 sparse matrix of type '<... 'numpy.int64'>'\n    with 19 stored elements in Compressed Sparse ... format>>>> analyze = vectorizer.build_analyzer()\n>>> analyze(\"This is a text document to analyze.\") == (\n...     ['this', 'is', 'text', 'document', 'to', 'analyze'])\nTrue>>> vectorizer.get_feature_names() == (\n...     ['and', 'document', 'first', 'is', 'one',\n...      'second', 'the', 'third', 'this'])\nTrue\n\n>>> X.toarray()           \narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)>>> vectorizer.vocabulary_.get('document')\n1>>> vectorizer.transform(['Something completely new.']).toarray()\n...                           \narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n...                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n>>> analyze = bigram_vectorizer.build_analyzer()\n>>> analyze('Bi-grams are cool!') == (\n...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\nTrue>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n>>> X_2\n...                           \narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')\n>>> X_2[:, feature_index]     \narray([0, 0, 0, 1]...)>>> from sklearn.feature_extraction.text import TfidfTransformer\n>>> transformer = TfidfTransformer(smooth_idf=False)\n>>> transformer   \nTfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,\n                 use_idf=True)>>> counts = [[3, 0, 1],\n...           [2, 0, 0],\n...           [3, 0, 0],\n...           [4, 0, 0],\n...           [3, 2, 0],\n...           [3, 0, 2]]\n...\n>>> tfidf = transformer.fit_transform(counts)\n>>> tfidf                         \n<6x3 sparse matrix of type '<... 'numpy.float64'>'\n    with 9 stored elements in Compressed Sparse ... format>\n\n>>> tfidf.toarray()                        \narray([[0.81940995, 0.        , 0.57320793],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.47330339, 0.88089948, 0.        ],\n       [0.58149261, 0.        , 0.81355169]])>>> transformer = TfidfTransformer()\n>>> transformer.fit_transform(counts).toarray()\narray([[0.85151335, 0.        , 0.52433293],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.55422893, 0.83236428, 0.        ],\n       [0.63035731, 0.        , 0.77630514]])>>> transformer.idf_                       \narray([1. ..., 2.25..., 1.84...])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer()\n>>> vectorizer.fit_transform(corpus)\n...                                \n<4x9 sparse matrix of type '<... 'numpy.float64'>'\n    with 19 stored elements in Compressed Sparse ... format>>>> import chardet    \n>>> text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n>>> text2 = b\"holdselig sind deine Ger\\xfcche\"\n>>> text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n>>> decoded = [x.decode(chardet.detect(x)['encoding'])\n...            for x in (text1, text2, text3)]        \n>>> v = CountVectorizer().fit(decoded).vocabulary_    \n>>> for term in v: print(v)                           >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n>>> ngram_vectorizer.get_feature_names() == (\n...     [' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])\nTrue\n>>> counts.toarray().astype(int)\narray([[1, 1, 1, 0, 1, 1, 1, 0],\n       [1, 1, 0, 1, 1, 1, 0, 1]])>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\n...                                \n<1x4 sparse matrix of type '<... 'numpy.int64'>'\n   with 4 stored elements in Compressed Sparse ... format>\n>>> ngram_vectorizer.get_feature_names() == (\n...     [' fox ', ' jump', 'jumpy', 'umpy '])\nTrue\n\n>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\n>>> ngram_vectorizer.fit_transform(['jumpy fox'])\n...                                \n<1x5 sparse matrix of type '<... 'numpy.int64'>'\n    with 5 stored elements in Compressed Sparse ... format>\n>>> ngram_vectorizer.get_feature_names() == (\n...     ['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'])\nTrue>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> hv = HashingVectorizer(n_features=10)\n>>> hv.transform(corpus)\n...                                \n<4x10 sparse matrix of type '<... 'numpy.float64'>'\n    with 16 stored elements in Compressed Sparse ... format>>>> hv = HashingVectorizer()\n>>> hv.transform(corpus)\n...                               \n<4x1048576 sparse matrix of type '<... 'numpy.float64'>'\n    with 19 stored elements in Compressed Sparse ... format>>>> def my_tokenizer(s):\n...     return s.split()\n...\n>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n>>> vectorizer.build_analyzer()(u\"Some... punctuation!\") == (\n...     ['some...', 'punctuation!'])\nTrue>>> from nltk import word_tokenize          \n>>> from nltk.stem import WordNetLemmatizer \n>>> class LemmaTokenizer(object):\n...     def __init__(self):\n...         self.wnl = WordNetLemmatizer()\n...     def __call__(self, doc):\n...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n...\n>>> vect = CountVectorizer(tokenizer=LemmaTokenizer())  >>> import re\n>>> def to_british(tokens):\n...     for t in tokens:\n...         t = re.sub(r\"(...)our$\", r\"\\1or\", t)\n...         t = re.sub(r\"([bt])re$\", r\"\\1er\", t)\n...         t = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\1z\\2\", t)\n...         t = re.sub(r\"ogue$\", \"og\", t)\n...         yield t\n...\n>>> class CustomVectorizer(CountVectorizer):\n...     def build_tokenizer(self):\n...         tokenize = super().build_tokenizer()\n...         return lambda doc: list(to_british(tokenize(doc)))\n...\n>>> print(CustomVectorizer().build_analyzer()(u\"color colour\")) \n[...'color', ...'color']", "Url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "Attachment_Url": []}, "157": {"Title": "5.2.4. Image feature extraction", "Text": "In a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\nIn order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf\u2013idf transform.\nTf means term-frequency while tf\u2013idf means term-frequency times inverse document-frequency:\ntf-idf(t,d)\n.\nUsing the TfidfTransformer\u2019s default settings, TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\nidf\n,\nwhere\nn\nis the total number of documents in the document set, and\ndf\nis the number of documents in the document set that contain term\nt\n. The resulting tf-idf vectors are then normalized by the Euclidean norm:\n.\nThis was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.\nThe following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn\u2019s TfidfTransformer and TfidfVectorizer differ slightly from the standard textbook notation that defines the idf as\nidf\nIn the TfidfTransformer and TfidfVectorizer with smooth_idf=False, the \u201c1\u201d count is added to the idf instead of the idf\u2019s denominator:\nidf\nThis normalization is implemented by the TfidfTransformer class:\nAgain please see the reference documentation for the details on all the parameters.\nLet\u2019s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:\nEach row is normalized to have unit Euclidean norm:\n\nFor example, we can compute the tf-idf of the first term in the first document in the counts array as follows:\nn\ndf\nidf\ntf-idf\nNow, if we repeat this computation for the remaining 2 terms in the document, we get\ntf-idf\ntf-idf\nand the vector of raw tf-idfs:\ntf-idf\nThen, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:\n[\nFurthermore, the default parameter smooth_idf=True adds \u201c1\u201d to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:\nidf\nUsing this modification, the tf-idf of the third term in document 1 changes to 1.8473:\ntf-idf\nAnd the L2-normalized tf-idf changes to\n[\n:\nThe weights of each feature computed by the fit method call are stored in a model attribute:\nAs tf\u2013idf is very often used for text features, there is also another class called TfidfVectorizer that combines all the options of CountVectorizer and TfidfTransformer in a single model:\nWhile the tf\u2013idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the binary parameter of CountVectorizer. In particular, some estimators such as Bernoulli Naive Bayes explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf\u2013idf values while the binary occurrence info is more stable.\nAs usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier:\n", "Code_snippet": ">>> from sklearn.feature_extraction.text import TfidfTransformer\n>>> transformer = TfidfTransformer(smooth_idf=False)\n>>> transformer   \nTfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,\n                 use_idf=True)>>> counts = [[3, 0, 1],\n...           [2, 0, 0],\n...           [3, 0, 0],\n...           [4, 0, 0],\n...           [3, 2, 0],\n...           [3, 0, 2]]\n...\n>>> tfidf = transformer.fit_transform(counts)\n>>> tfidf                         \n<6x3 sparse matrix of type '<... 'numpy.float64'>'\n    with 9 stored elements in Compressed Sparse ... format>\n\n>>> tfidf.toarray()                        \narray([[0.81940995, 0.        , 0.57320793],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.47330339, 0.88089948, 0.        ],\n       [0.58149261, 0.        , 0.81355169]])>>> transformer = TfidfTransformer()\n>>> transformer.fit_transform(counts).toarray()\narray([[0.85151335, 0.        , 0.52433293],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.55422893, 0.83236428, 0.        ],\n       [0.63035731, 0.        , 0.77630514]])>>> transformer.idf_                       \narray([1. ..., 2.25..., 1.84...])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer()\n>>> vectorizer.fit_transform(corpus)\n...                                \n<4x9 sparse matrix of type '<... 'numpy.float64'>'\n    with 19 stored elements in Compressed Sparse ... format>>>> import numpy as np\n>>> from sklearn.feature_extraction import image\n\n>>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n>>> one_image[:, :, 0]  # R channel of a fake RGB picture\narray([[ 0,  3,  6,  9],\n       [12, 15, 18, 21],\n       [24, 27, 30, 33],\n       [36, 39, 42, 45]])\n\n>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\n...     random_state=0)\n>>> patches.shape\n(2, 2, 2, 3)\n>>> patches[:, :, :, 0]\narray([[[ 0,  3],\n        [12, 15]],\n\n       [[15, 18],\n        [27, 30]]])\n>>> patches = image.extract_patches_2d(one_image, (2, 2))\n>>> patches.shape\n(9, 2, 2, 3)\n>>> patches[4, :, :, 0]\narray([[15, 18],\n       [27, 30]])>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\n>>> np.testing.assert_array_equal(one_image, reconstructed)>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n>>> patches = image.PatchExtractor((2, 2)).transform(five_images)\n>>> patches.shape\n(45, 2, 2, 3)", "Url": "https://scikit-learn.org/stable/modules/feature_extraction.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_coin_ward_segmentation_0011.png"]}, "158": {"Title": "5.3.1. Standardization, or mean removal and variance scaling", "Text": "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\nIn practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\nFor instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\nThe function scale provides a quick and easy way to perform this operation on a single array-like dataset:\nScaled data has zero mean and unit variance:\nThe preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:\nThe scaler instance can then be used on new data to transform it the same way it did on the training set:\nIt is possible to disable either centering or scaling by either passing with_mean=False or with_std=False to the constructor of StandardScaler.\nAn alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\nThe motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\nHere is an example to scale a toy data matrix to the [0, 1] range:\nThe same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:\nIt is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:\nIf MinMaxScaler is given an explicit feature_range=(min, max) the full formula is:\nMaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within the range [-1, 1] by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data.\nHere is how to use the toy data from the previous example with this scaler:\nAs with scale, the module further provides convenience functions minmax_scale and maxabs_scale if you don\u2019t want to create an object.\nQuantileTransformer and quantile_transform provide a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1:\nThis feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:\nThis can be confirmed on a independent testing set with similar remarks:\nKBinsDiscretizer discretizes features into k bins:\nBy default the output is one-hot encoded into a sparse matrix (See Encoding categorical features) and this can be configured with the encode parameter. For each feature, the bin edges are computed during fit and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as:\nBased on these bin intervals, X is transformed as follows:\nThe resulting dataset contains ordinal attributes which can be further used in a sklearn.pipeline.Pipeline.\nDiscretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins.\nKBinsDiscretizer implements different binning strategies, which can be selected with the strategy parameter. The \u2018uniform\u2019 strategy uses constant-width bins. The \u2018quantile\u2019 strategy uses the quantiles values to have equally populated bins in each feature. The \u2018kmeans\u2019 strategy defines bins based on a k-means clustering procedure performed on each feature independently.\n", "Code_snippet": ">>> from sklearn import preprocessing\n>>> import numpy as np\n>>> X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n>>> X_scaled = preprocessing.scale(X_train)\n\n>>> X_scaled                                          \narray([[ 0.  ..., -1.22...,  1.33...],\n       [ 1.22...,  0.  ..., -0.26...],\n       [-1.22...,  1.22..., -1.06...]])>>> X_scaled.mean(axis=0)\narray([0., 0., 0.])\n\n>>> X_scaled.std(axis=0)\narray([1., 1., 1.])>>> scaler = preprocessing.StandardScaler().fit(X_train)\n>>> scaler\nStandardScaler(copy=True, with_mean=True, with_std=True)\n\n>>> scaler.mean_                                      \narray([1. ..., 0. ..., 0.33...])\n\n>>> scaler.scale_                                       \narray([0.81..., 0.81..., 1.24...])\n\n>>> scaler.transform(X_train)                           \narray([[ 0.  ..., -1.22...,  1.33...],\n       [ 1.22...,  0.  ..., -0.26...],\n       [-1.22...,  1.22..., -1.06...]])>>> X_test = [[-1., 1., 0.]]\n>>> scaler.transform(X_test)                \narray([[-2.44...,  1.22..., -0.26...]])>>> X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n...\n>>> min_max_scaler = preprocessing.MinMaxScaler()\n>>> X_train_minmax = min_max_scaler.fit_transform(X_train)\n>>> X_train_minmax\narray([[0.5       , 0.        , 1.        ],\n       [1.        , 0.5       , 0.33333333],\n       [0.        , 1.        , 0.        ]])>>> X_test = np.array([[-3., -1.,  4.]])\n>>> X_test_minmax = min_max_scaler.transform(X_test)\n>>> X_test_minmax\narray([[-1.5       ,  0.        ,  1.66666667]])>>> min_max_scaler.scale_                             \narray([0.5       , 0.5       , 0.33...])\n\n>>> min_max_scaler.min_                               \narray([0.        , 0.5       , 0.33...])X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n\nX_scaled = X_std * (max - min) + min>>> X_train = np.array([[ 1., -1.,  2.],\n...                     [ 2.,  0.,  0.],\n...                     [ 0.,  1., -1.]])\n...\n>>> max_abs_scaler = preprocessing.MaxAbsScaler()\n>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n>>> X_train_maxabs                # doctest +NORMALIZE_WHITESPACE^\narray([[ 0.5, -1. ,  1. ],\n       [ 1. ,  0. ,  0. ],\n       [ 0. ,  1. , -0.5]])\n>>> X_test = np.array([[ -3., -1.,  4.]])\n>>> X_test_maxabs = max_abs_scaler.transform(X_test)\n>>> X_test_maxabs                 \narray([[-1.5, -1. ,  2. ]])\n>>> max_abs_scaler.scale_         \narray([2.,  1.,  2.])>>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n>>> X_train_trans = quantile_transformer.fit_transform(X_train)\n>>> X_test_trans = quantile_transformer.transform(X_test)\n>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) \narray([ 4.3,  5.1,  5.8,  6.5,  7.9])>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\n... \narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])>>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n... \narray([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\n>>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\n... \narray([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])>>> X = np.array([[ -3., 5., 15 ],\n...               [  0., 6., 14 ],\n...               [  6., 3., 11 ]])\n>>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)>>> est.transform(X)                      \narray([[ 0., 1., 1.],\n       [ 1., 1., 1.],\n       [ 2., 0., 0.]])", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "159": {"Title": "5.3.2. Non-linear transformation", "Text": "Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.\nMaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go about this. However, scale and StandardScaler can accept scipy.sparse matrices as input, as long as with_mean=False is explicitly passed to the constructor. Otherwise a ValueError will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. RobustScaler cannot be fitted to sparse inputs, but you can use the transform method on sparse inputs.\nNote that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see scipy.sparse.csr_matrix and scipy.sparse.csc_matrix). Any other sparse input will be converted to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream.\nFinally, if the centered data is expected to be small enough, explicitly converting the input to an array using the toarray method of sparse matrices is another option.\nTwo types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature.\nQuantile transforms put all features into the same desired distribution based on the formula\nG\nwhere\nF\nis the cumulative distribution function of the feature and\nG\nthe quantile function of the desired output distribution\nG\n. This formula is using the two following facts: (i) if\nX\nis a random variable with a continuous cumulative distribution function\nF\nthen\nF\nis uniformly distributed on\n[\n; (ii) if\nU\nis a random variable with uniform distribution on\n[\nthen\nG\nhas distribution\nG\n. By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.\nPower transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution.\nIn many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.\nPowerTransformer currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform.\nThe Yeo-Johnson transform is given by:\nwhile the Box-Cox transform is given by:\nBox-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by\n\u03bb\n, which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution:\nWhile the above example sets the standardize option to False, PowerTransformer will apply zero-mean, unit-variance normalization to the transformed output by default.\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation.\nIt is also possible to map data to a normal distribution using QuantileTransformer by setting output_distribution='normal'. Using the earlier example with the iris dataset:\nThus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input\u2019s minimum and maximum \u2014 corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively \u2014 do not become infinite under the transformation.\nFeature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the sklearn.neural_network.BernoulliRBM.\nIt is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.\nAs for the Normalizer, the utility class Binarizer is meant to be used in the early stages of sklearn.pipeline.Pipeline. The fit method does nothing as each sample is treated independently of others:\nIt is possible to adjust the threshold of the binarizer:\nAs for the StandardScaler and Normalizer classes, the preprocessing module provides a companion function binarize to be used when the transformer API is not necessary.\nNote that the Binarizer is similar to the KBinsDiscretizer when k = 2, and when the bin edge is at the value threshold.\n", "Code_snippet": ">>> from sklearn.datasets import load_iris\n>>> from sklearn.model_selection import train_test_split\n>>> iris = load_iris()\n>>> X, y = iris.data, iris.target\n>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n>>> X_train_trans = quantile_transformer.fit_transform(X_train)\n>>> X_test_trans = quantile_transformer.transform(X_test)\n>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) \narray([ 4.3,  5.1,  5.8,  6.5,  7.9])>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\n... \narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])>>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\n... \narray([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\n>>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\n... \narray([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)\n>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n>>> X_lognormal                                         \narray([[1.28..., 1.18..., 0.84...],\n       [0.94..., 1.60..., 0.38...],\n       [1.35..., 0.21..., 1.09...]])\n>>> pt.fit_transform(X_lognormal)                   \narray([[ 0.49...,  0.17..., -0.15...],\n       [-0.05...,  0.58..., -0.57...],\n       [ 0.69..., -0.84...,  0.10...]])>>> quantile_transformer = preprocessing.QuantileTransformer(\n...     output_distribution='normal', random_state=0)\n>>> X_trans = quantile_transformer.fit_transform(X)\n>>> quantile_transformer.quantiles_ \narray([[4.3, 2. , 1. , 0.1],\n       [4.4, 2.2, 1.1, 0.1],\n       [4.4, 2.2, 1.2, 0.1],\n       ...,\n       [7.7, 4.1, 6.7, 2.5],\n       [7.7, 4.2, 6.7, 2.5],\n       [7.9, 4.4, 6.9, 2.5]])>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n>>> binarizer\nBinarizer(copy=True, threshold=0.0)\n\n>>> binarizer.transform(X)\narray([[1., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])>>> binarizer = preprocessing.Binarizer(threshold=1.1)\n>>> binarizer.transform(X)\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 0.]])", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_map_data_to_normal_0011.png"]}, "160": {"Title": "5.3.3. Normalization", "Text": "If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use robust_scale and RobustScaler as drop-in replacements instead. They use more robust estimates for the center and range of your data.\nNormalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\nThis assumption is the base of the Vector Space Model often used in text classification and clustering contexts.\nThe function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1 or l2 norms:\nThe preprocessing module further provides a utility class Normalizer that implements the same operation using the Transformer API (even though the fit method is useless in this case: the class is stateless as this operation treats samples independently).\nThis class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:\nThe normalizer instance can then be used on sample vectors as any transformer:\n", "Code_snippet": ">>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n>>> X_normalized = preprocessing.normalize(X, norm='l2')\n\n>>> X_normalized                                      \narray([[ 0.40..., -0.40...,  0.81...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 0.  ...,  0.70..., -0.70...]])>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\n>>> normalizer\nNormalizer(copy=True, norm='l2')>>> normalizer.transform(X)                            \narray([[ 0.40..., -0.40...,  0.81...],\n       [ 1.  ...,  0.  ...,  0.  ...],\n       [ 0.  ...,  0.70..., -0.70...]])\n\n>>> normalizer.transform([[-1.,  1., 0.]])             \narray([[-0.70...,  0.70...,  0.  ...]])", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "161": {"Title": "5.3.4. Encoding categorical features", "Text": "If you have a kernel matrix of a kernel\nK\nthat computes a dot product in a feature space defined by function\np\n, a KernelCenterer can transform the kernel matrix so that it contains inner products in the feature space defined by\np\nfollowed by removal of the mean in that space.\nOften features are not given as continuous values but categorical. For example a person could have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [0, 1, 3] while [\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\nTo convert categorical features to such integer codes, we can use the OrdinalEncoder. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1):\nSuch integer representation can, however, not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired (i.e. the set of browsers was ordered arbitrarily).\nAnother possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.\nContinuing the example above:\nBy default, the values each feature can take is inferred automatically from the dataset and can be found in the categories_ attribute:\nIt is possible to specify this explicitly using the parameter categories. There are two genders, four possible continents and four web browsers in our dataset:\nIf there is a possibility that the training data might have missing categorical features, it can often be better to specify handle_unknown='ignore' instead of setting the categories manually as above. When handle_unknown='ignore' is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (handle_unknown='ignore' is only supported for one-hot encoding):\nIt is also possible to encode each column into n_categories - 1 columns instead of n_categories columns by using the drop parameter. This parameter allows the user to specify a category for each feature to be dropped. This is useful to avoid co-linearity in the input matrix in some classifiers. Such functionality is useful, for example, when using non-regularized regression (LinearRegression), since co-linearity would cause the covariance matrix to be non-invertible. When this paramenter is not None, handle_unknown must be set to error:\nSee Loading features from dicts for categorical features that are represented as a dict, not as scalars.\n", "Code_snippet": ">>> enc = preprocessing.OrdinalEncoder()\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X)  \nOrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)\n>>> enc.transform([['female', 'from US', 'uses Safari']])\narray([[0., 1., 1.]])>>> enc = preprocessing.OneHotEncoder()\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X)  \nOneHotEncoder(categorical_features=None, categories=None, drop=None,\n       dtype=<... 'numpy.float64'>, handle_unknown='error',\n       n_values=None, sparse=True)\n>>> enc.transform([['female', 'from US', 'uses Safari'],\n...                ['male', 'from Europe', 'uses Safari']]).toarray()\narray([[1., 0., 0., 1., 0., 1.],\n       [0., 1., 1., 0., 0., 1.]])>>> enc.categories_\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]>>> genders = ['female', 'male']\n>>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']\n>>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']\n>>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\n>>> # Note that for there are missing categorical values for the 2nd and 3rd\n>>> # feature\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X) \nOneHotEncoder(categorical_features=None,\n       categories=[...], drop=None,\n       dtype=<... 'numpy.float64'>, handle_unknown='error',\n       n_values=None, sparse=True)\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\narray([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])>>> enc = preprocessing.OneHotEncoder(handle_unknown='ignore')\n>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> enc.fit(X) \nOneHotEncoder(categorical_features=None, categories=None, drop=None,\n       dtype=<... 'numpy.float64'>, handle_unknown='ignore',\n       n_values=None, sparse=True)\n>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()\narray([[1., 0., 0., 0., 0., 0.]])>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n>>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)\n>>> drop_enc.categories_\n[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]\n>>> drop_enc.transform(X).toarray()\narray([[1., 1., 1.],\n       [0., 0., 0.]])", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "162": {"Title": "5.3.5. Discretization", "Text": "Discretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes.\nOne-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models.\n", "Code_snippet": ">>> X = np.array([[ -3., 5., 15 ],\n...               [  0., 6., 14 ],\n...               [  6., 3., 11 ]])\n>>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)>>> est.transform(X)                      \narray([[ 0., 1., 1.],\n       [ 1., 1., 1.],\n       [ 2., 0., 0.]])>>> X = [[ 1., -1.,  2.],\n...      [ 2.,  0.,  0.],\n...      [ 0.,  1., -1.]]\n\n>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n>>> binarizer\nBinarizer(copy=True, threshold=0.0)\n\n>>> binarizer.transform(X)\narray([[1., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])>>> binarizer = preprocessing.Binarizer(threshold=1.1)\n>>> binarizer.transform(X)\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 0.]])", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "163": {"Title": "5.3.6. Imputation of missing values", "Text": "Tools for imputing missing values are discussed at Imputation of missing values.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "164": {"Title": "5.3.7. Generating polynomial features", "Text": "Often it\u2019s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features\u2019 high-order and interaction terms. It is implemented in PolynomialFeatures:\nThe features of X have been transformed from\n(\nto\n(\n.\nIn some cases, only interaction terms among features are required, and it can be gotten with the setting interaction_only=True:\nThe features of X have been transformed from\n(\nto\n(\n.\nNote that polynomial features are used implicitly in kernel methods (e.g., sklearn.svm.SVC, sklearn.decomposition.KernelPCA) when using polynomial Kernel functions.\nSee Polynomial interpolation for Ridge regression using created polynomial features.\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> X = np.arange(6).reshape(3, 2)\n>>> X                                                 \narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n>>> poly = PolynomialFeatures(2)\n>>> poly.fit_transform(X)                             \narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\n       [ 1.,  2.,  3.,  4.,  6.,  9.],\n       [ 1.,  4.,  5., 16., 20., 25.]])>>> X = np.arange(9).reshape(3, 3)\n>>> X                                                 \narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> poly = PolynomialFeatures(degree=3, interaction_only=True)\n>>> poly.fit_transform(X)                             \narray([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\n       [  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\n       [  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "165": {"Title": "5.3.8. Custom transformers", "Text": "Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with FunctionTransformer. For example, to build a transformer that applies a log transformation in a pipeline, do:\nYou can ensure that func and inverse_func are the inverse of each other by setting check_inverse=True and calling fit before transform. Please note that a warning is raised and can be turned into an error with a filterwarnings:\nFor a full code example that demonstrates using a FunctionTransformer to do custom feature selection, see Using FunctionTransformer to select columns\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn.preprocessing import FunctionTransformer\n>>> transformer = FunctionTransformer(np.log1p, validate=True)\n>>> X = np.array([[0, 1], [2, 3]])\n>>> transformer.transform(X)\narray([[0.        , 0.69314718],\n       [1.09861229, 1.38629436]])>>> import warnings\n>>> warnings.filterwarnings(\"error\", message=\".*check_inverse*.\",\n...                         category=UserWarning, append=False)", "Url": "https://scikit-learn.org/stable/modules/preprocessing.html", "Attachment_Url": []}, "166": {"Title": "5.4.1. Univariate vs. Multivariate Imputation", "Text": "One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer). By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer).\nThere are many well-established imputation packages in the R data science ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns out to be a particular instance of different sequential imputation algorithms that can all be implemented with IterativeImputer by passing in different regressors to be used for predicting missing feature values. In the case of missForest, this regressor is a Random Forest. See sphx_glr_auto_examples_plot_iterative_imputer_variants_comparison.py.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/impute.html", "Attachment_Url": []}, "167": {"Title": "5.4.2. Univariate feature imputation", "Text": "The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\nThe following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values:\nThe SimpleImputer class also supports sparse matrices:\nNote that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input.\nThe SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy:\nIn the statistics community, it is common practice to perform multiple imputations, generating, for example, m separate imputations for a single feature matrix. Each of these m imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The m final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. The above practice is called multiple imputation.\nOur implementation of IterativeImputer was inspired by the R MICE package (Multivariate Imputation by Chained Equations) [1], but differs from it by returning a single imputation instead of multiple imputations. However, IterativeImputer can also be used for multiple imputations by applying it repeatedly to the same dataset with different random seeds when sample_posterior=True. See [2], chapter 4 for more discussion on multiple vs. single imputations.\nIt is still an open problem as to how useful single vs. multiple imputation is in the context of prediction and classification when the user is not interested in measuring uncertainty due to missing values.\nNote that a call to the transform method of IterativeImputer is not allowed to change the number of samples. Therefore multiple imputations cannot be achieved by a single call to transform.\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn.impute import SimpleImputer\n>>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])  \nSimpleImputer(add_indicator=False, copy=True, fill_value=None,\n              missing_values=nan, strategy='mean', verbose=0)\n>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]\n>>> print(imp.transform(X))      \n[[4.          2.        ]\n [6.          3.666...]\n [7.          6.        ]]>>> import scipy.sparse as sp\n>>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\n>>> imp = SimpleImputer(missing_values=-1, strategy='mean')\n>>> imp.fit(X)                  \nSimpleImputer(add_indicator=False, copy=True, fill_value=None,\n              missing_values=-1, strategy='mean', verbose=0)\n>>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\n>>> print(imp.transform(X_test).toarray())  \n[[3. 2.]\n [6. 3.]\n [7. 6.]]>>> import pandas as pd\n>>> df = pd.DataFrame([[\"a\", \"x\"],\n...                    [np.nan, \"y\"],\n...                    [\"a\", np.nan],\n...                    [\"b\", \"y\"]], dtype=\"category\")\n...\n>>> imp = SimpleImputer(strategy=\"most_frequent\")\n>>> print(imp.fit_transform(df))      \n[['a' 'x']\n ['a' 'y']\n ['a' 'y']\n ['b' 'y']]", "Url": "https://scikit-learn.org/stable/modules/impute.html", "Attachment_Url": []}, "168": {"Title": "5.4.3. Multivariate feature imputation", "Text": "A more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\nBoth SimpleImputer and IterativeImputer can be used in a Pipeline as a way to build a composite estimator that supports imputation. See Imputing missing values before building an estimator.\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn.experimental import enable_iterative_imputer\n>>> from sklearn.impute import IterativeImputer\n>>> imp = IterativeImputer(max_iter=10, random_state=0)\n>>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])  \nIterativeImputer(add_indicator=False, estimator=None,\n                 imputation_order='ascending', initial_strategy='mean',\n                 max_iter=10, max_value=None, min_value=None,\n                 missing_values=nan, n_nearest_features=None,\n                 random_state=0, sample_posterior=False, tol=0.001,\n                 verbose=0)\n>>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n>>> # the model learns that the second feature is double the first\n>>> print(np.round(imp.transform(X_test)))\n[[ 1.  2.]\n [ 6. 12.]\n [ 3.  6.]]", "Url": "https://scikit-learn.org/stable/modules/impute.html", "Attachment_Url": []}, "169": {"Title": "5.4.4. References", "Text": "", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/impute.html", "Attachment_Url": []}, "170": {"Title": "5.4.5. Marking imputed values", "Text": "The MissingIndicator transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative.\nNaN is usually used as the placeholder for missing values. However, it enforces the data type to be float. The parameter missing_values allows to specify other placeholder such as integer. In the following example, we will use -1 as missing values:\nThe features parameter is used to choose the features for which the mask is constructed. By default, it is 'missing-only' which returns the imputer mask of the features containing missing values at fit time:\nThe features parameter can be set to 'all' to returned all features whether or not they contain missing values:\nWhen using the MissingIndicator in a Pipeline, be sure to use the FeatureUnion or ColumnTransformer to add the indicator features to the regular features. First we obtain the iris dataset, and add some missing values to it.\nNow we create a FeatureUnion. All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. Additionally, it adds the the indicator variables from MissingIndicator.\nOf course, we cannot use the transformer to make any predictions. We should wrap this in a Pipeline with a classifier (e.g., a DecisionTreeClassifier) to be able to make predictions.\n", "Code_snippet": ">>> from sklearn.impute import MissingIndicator\n>>> X = np.array([[-1, -1, 1, 3],\n...               [4, -1, 0, -1],\n...               [8, -1, 1, 0]])\n>>> indicator = MissingIndicator(missing_values=-1)\n>>> mask_missing_values_only = indicator.fit_transform(X)\n>>> mask_missing_values_only\narray([[ True,  True, False],\n       [False,  True,  True],\n       [False,  True, False]])>>> indicator.features_\narray([0, 1, 3])>>> indicator = MissingIndicator(missing_values=-1, features=\"all\")\n>>> mask_all = indicator.fit_transform(X)\n>>> mask_all\narray([[ True,  True, False, False],\n       [False,  True, False,  True],\n       [False,  True, False, False]])\n>>> indicator.features_\narray([0, 1, 2, 3])>>> from sklearn.datasets import load_iris\n>>> from sklearn.impute import SimpleImputer, MissingIndicator\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.pipeline import FeatureUnion, make_pipeline\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> X, y = load_iris(return_X_y=True)\n>>> mask = np.random.randint(0, 2, size=X.shape).astype(np.bool)\n>>> X[mask] = np.nan\n>>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\n...                                                random_state=0)>>> transformer = FeatureUnion(\n...     transformer_list=[\n...         ('features', SimpleImputer(strategy='mean')),\n...         ('indicators', MissingIndicator())])\n>>> transformer = transformer.fit(X_train, y_train)\n>>> results = transformer.transform(X_test)\n>>> results.shape\n(100, 8)>>> clf = make_pipeline(transformer, DecisionTreeClassifier())\n>>> clf = clf.fit(X_train, y_train)\n>>> results = clf.predict(X_test)\n>>> results.shape\n(100,)", "Url": "https://scikit-learn.org/stable/modules/impute.html", "Attachment_Url": []}, "171": {"Title": "5.5.1. PCA: principal component analysis", "Text": "decomposition.PCA looks for a combination of features that capture well the variance of the original features. See Decomposing signals in components (matrix factorization problems).\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "Attachment_Url": []}, "172": {"Title": "5.5.2. Random projections", "Text": "The module: random_projection provides several tools for data reduction by random projections. See the relevant section of the documentation: Random Projection.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "Attachment_Url": []}, "173": {"Title": "5.5.3. Feature agglomeration", "Text": "cluster.FeatureAgglomeration applies Hierarchical clustering to group together features that behave similarly.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html", "Attachment_Url": []}, "174": {"Title": "5.6.1. The Johnson-Lindenstrauss lemma", "Text": "The main theoretical result behind the efficiency of random projection is the Johnson-Lindenstrauss lemma (quoting Wikipedia):\nKnowing only the number of samples, the sklearn.random_projection.johnson_lindenstrauss_min_dim estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:\n", "Code_snippet": ">>> from sklearn.random_projection import johnson_lindenstrauss_min_dim\n>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\n663\n>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\narray([    663,   11841, 1112658])\n>>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\narray([ 7894,  9868, 11841])", "Url": "https://scikit-learn.org/stable/modules/random_projection.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_0021.png"]}, "175": {"Title": "5.6.2. Gaussian random projection", "Text": "The sklearn.random_projection.GaussianRandomProjection reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution .\nHere a small excerpt which illustrates how to use the Gaussian random projection transformer:\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn import random_projection\n>>> X = np.random.rand(100, 10000)\n>>> transformer = random_projection.GaussianRandomProjection()\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)", "Url": "https://scikit-learn.org/stable/modules/random_projection.html", "Attachment_Url": []}, "176": {"Title": "5.6.3. Sparse random projection", "Text": "The sklearn.random_projection.SparseRandomProjection reduces the dimensionality by projecting the original input space using a sparse random matrix.\nSparse random matrices are an alternative to dense Gaussian random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.\nIf we define s = 1 / density, the elements of the random matrix are drawn from\nwhere\nn\nis the size of the projected subspace. By default the density of non zero elements is set to the minimum density as recommended by Ping Li et al.:\n1\n.\nHere a small excerpt which illustrates how to use the sparse random projection transformer:\n", "Code_snippet": ">>> import numpy as np\n>>> from sklearn import random_projection\n>>> X = np.random.rand(100, 10000)\n>>> transformer = random_projection.SparseRandomProjection()\n>>> X_new = transformer.fit_transform(X)\n>>> X_new.shape\n(100, 3947)", "Url": "https://scikit-learn.org/stable/modules/random_projection.html", "Attachment_Url": []}, "177": {"Title": "5.7.1. Nystroem Method for Kernel Approximation", "Text": "The Nystroem method, as implemented in Nystroem is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default Nystroem uses the rbf kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter n_components.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "Attachment_Url": []}, "178": {"Title": "5.7.2. Radial Basis Function Kernel", "Text": "The RBFSampler constructs an approximate mapping for the radial basis function kernel, also known as Random Kitchen Sinks [RR2007]. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:\nThe mapping relies on a Monte Carlo approximation to the kernel values. The fit function performs the Monte Carlo sampling, whereas the transform method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the fit function.\nThe fit function takes two arguments: n_components, which is the target dimensionality of the feature transform, and gamma, the parameter of the RBF-kernel. A higher n_components will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that \u201cfitting\u201d the feature function does not actually depend on the data given to the fit function. Only the dimensionality of the data is used. Details on the method can be found in [RR2007].\nFor a given value of n_components RBFSampler is often less accurate as Nystroem. RBFSampler is cheaper to compute, though, making use of larger feature spaces more efficient.\n", "Code_snippet": ">>> from sklearn.kernel_approximation import RBFSampler\n>>> from sklearn.linear_model import SGDClassifier\n>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n>>> y = [0, 0, 1, 1]\n>>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n>>> X_features = rbf_feature.fit_transform(X)\n>>> clf = SGDClassifier(max_iter=5)\n>>> clf.fit(X_features, y)   \nSGDClassifier(alpha=0.0001, average=False, class_weight=None,\n       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n       n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n       random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,\n       verbose=0, warm_start=False)\n>>> clf.score(X_features, y)\n1.0", "Url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_approximation_0021.png"]}, "179": {"Title": "5.7.3. Additive Chi Squared Kernel", "Text": "The additive chi squared kernel is a kernel on histograms, often used in computer vision.\nThe additive chi squared kernel as used here is given by\nThis is not exactly the same as sklearn.metrics.additive_chi2_kernel. The authors of [VZ2010] prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components\nx\nseparately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.\nThe class AdditiveChi2Sampler implements this component wise deterministic sampling. Each component is sampled\nn\ntimes, yielding\n2\ndimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature,\nn\nis usually chosen to be 1 or 2, transforming the dataset to size n_samples * 5 * n_features (in the case of\nn\n).\nThe approximate feature map provided by AdditiveChi2Sampler can be combined with the approximate feature map provided by RBFSampler to yield an approximate feature map for the exponentiated chi squared kernel. See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "Attachment_Url": []}, "180": {"Title": "5.7.4. Skewed Chi Squared Kernel", "Text": "The skewed chi squared kernel is given by:\nIt has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.\nThe usage of the SkewedChi2Sampler is the same as the usage described above for the RBFSampler. The only difference is in the free parameter, that is called\nc\n. For a motivation for this mapping and the mathematical details see [LS2010].\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "Attachment_Url": []}, "181": {"Title": "5.7.5. Mathematical Details", "Text": "Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function\nk\n(a so called Mercer kernel), it is guaranteed that there exists a mapping\n\u03d5\ninto a Hilbert space\nH\n, such that\nWhere\n\u27e8\ndenotes the inner product in the Hilbert space.\nIf an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points\nx\n, one may use the value of\nk\n, which corresponds to applying the algorithm to the mapped data points\n\u03d5\n. The advantage of using\nk\nis that the mapping\n\u03d5\nnever has to be calculated explicitly, allowing for arbitrary large features (even infinite).\nOne drawback of kernel methods is, that it might be necessary to store many kernel values\nk\nduring optimization. If a kernelized classifier is applied to new data\ny\n,\nk\nneeds to be computed to make predictions, possibly for many different\nx\nin the training set.\nThe classes in this submodule allow to approximate the embedding\n\u03d5\n, thereby working explicitly with the representations\n\u03d5\n, which obviates the need to apply the kernel or store training examples.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/kernel_approximation.html", "Attachment_Url": []}, "182": {"Title": "5.8.1. Cosine similarity", "Text": "cosine_similarity computes the L2-normalized dot product of vectors. That is, if x\nand y\nare row vectors, their cosine similarity k\nis defined as:\nThis is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.\nThis kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. cosine_similarity accepts scipy.sparse matrices. (Note that the tf-idf functionality in sklearn.feature_extraction.text can produce normalized vectors, in which case cosine_similarity is equivalent to linear_kernel, only slower.)\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "183": {"Title": "5.8.2. Linear kernel", "Text": "The function linear_kernel computes the linear kernel, that is, a special case of polynomial_kernel with degree=1 and coef0=0 (homogeneous). If x and y are column vectors, their linear kernel is:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "184": {"Title": "5.8.3. Polynomial kernel", "Text": "The function polynomial_kernel computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.\nThe polynomial kernel is defined as:\nwhere:\nIf\nc\nthe kernel is said to be homogeneous.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "185": {"Title": "5.8.4. Sigmoid kernel", "Text": "The function sigmoid_kernel computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:\nwhere:\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "186": {"Title": "5.8.5. RBF kernel", "Text": "The function rbf_kernel computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:\nwhere x and y are the input vectors. If\n\u03b3\nthe kernel is known as the Gaussian kernel of variance\n\u03c3\n.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "187": {"Title": "5.8.6. Laplacian kernel", "Text": "The function laplacian_kernel is a variant on the radial basis function kernel defined as:\nwhere x and y are the input vectors and\n\u2016\nis the Manhattan distance between the input vectors.\nIt has proven useful in ML applied to noiseless data. See e.g. Machine learning for quantum mechanics in a nutshell.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "188": {"Title": "5.8.7. Chi-squared kernel", "Text": "The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using chi2_kernel and then passed to an sklearn.svm.SVC with kernel=\"precomputed\":\nIt can also be directly used as the kernel argument:\nThe chi squared kernel is given by\nThe data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.\nThe chi squared kernel is most commonly used on histograms (bags) of visual words.\n", "Code_snippet": ">>> from sklearn.svm import SVC\n>>> from sklearn.metrics.pairwise import chi2_kernel\n>>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\n>>> y = [0, 1, 0, 1]\n>>> K = chi2_kernel(X, gamma=.5)\n>>> K                        \narray([[1.        , 0.36787944, 0.89483932, 0.58364548],\n       [0.36787944, 1.        , 0.51341712, 0.83822343],\n       [0.89483932, 0.51341712, 1.        , 0.7768366 ],\n       [0.58364548, 0.83822343, 0.7768366 , 1.        ]])\n\n>>> svm = SVC(kernel='precomputed').fit(K, y)\n>>> svm.predict(K)\narray([0, 1, 0, 1])>>> svm = SVC(kernel=chi2_kernel).fit(X, y)\n>>> svm.predict(X)\narray([0, 1, 0, 1])", "Url": "https://scikit-learn.org/stable/modules/metrics.html", "Attachment_Url": []}, "189": {"Title": "5.9.1. Label binarization", "Text": "LabelBinarizer is a utility class to help create a label indicator matrix from a list of multi-class labels:\nFor multiple labels per instance, use MultiLabelBinarizer:\n", "Code_snippet": ">>> from sklearn import preprocessing\n>>> lb = preprocessing.LabelBinarizer()\n>>> lb.fit([1, 2, 6, 4, 2])\nLabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n>>> lb.classes_\narray([1, 2, 4, 6])\n>>> lb.transform([1, 6])\narray([[1, 0, 0, 0],\n       [0, 0, 0, 1]])>>> lb = preprocessing.MultiLabelBinarizer()\n>>> lb.fit_transform([(1, 2), (3,)])\narray([[1, 1, 0],\n       [0, 0, 1]])\n>>> lb.classes_\narray([1, 2, 3])", "Url": "https://scikit-learn.org/stable/modules/preprocessing_targets.html", "Attachment_Url": []}, "190": {"Title": "5.9.2. Label encoding", "Text": "LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. This is sometimes useful for writing efficient Cython routines. LabelEncoder can be used as follows:\nIt can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:\n", "Code_snippet": ">>> from sklearn import preprocessing\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([1, 2, 2, 6])\nLabelEncoder()\n>>> le.classes_\narray([1, 2, 6])\n>>> le.transform([1, 1, 2, 6])\narray([0, 0, 1, 2])\n>>> le.inverse_transform([0, 0, 1, 2])\narray([1, 1, 2, 6])>>> le = preprocessing.LabelEncoder()\n>>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nLabelEncoder()\n>>> list(le.classes_)\n['amsterdam', 'paris', 'tokyo']\n>>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\narray([2, 2, 1])\n>>> list(le.inverse_transform([2, 2, 1]))\n['tokyo', 'tokyo', 'paris']", "Url": "https://scikit-learn.org/stable/modules/preprocessing_targets.html", "Attachment_Url": []}, "191": {"Title": "6.1. General dataset API", "Text": "There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\nThe dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section.\nThe dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section.\nBoth loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape n_samples * n_features with key data (except for 20newsgroups) and a numpy array of length n_samples, containing the target values, with key target.\nIt\u2019s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the return_X_y parameter to True.\nThe datasets also contain a full description in their DESCR attribute and some contain feature_names and target_names. See the dataset descriptions below for details.\nThe dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section.\nThese functions return a tuple (X, y) consisting of a n_samples * n_features numpy array X and an array of length n_samples containing the targets y.\nIn addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section.\nData Set Characteristics:\nThis is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u2018Hedonic prices and the demand for clean air\u2019, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, \u2018Regression diagnostics \u2026\u2019, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\nThe Boston house-price data has been used in many machine learning papers that address regression problems.\nThis dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T.\nAs described on the original website:\nData Set Characteristics:\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.\nThe \u201ctarget\u201d for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.\nThe original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\nThe sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them:\nThe real data lies in the filenames and target attributes. The target attribute is the integer index of the category:\nIt is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function:\nscikit-learn provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.\nThe first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):\nThe default slice is a rectangular shape around the face, removing most of the background:\nEach of the 1140 faces is assigned to a single person id in the target array:\nThe second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:\nBoth for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing color=True, in that case the shape will be (2200, 2, 62, 47, 3).\nThe sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a 10-folds cross validation scheme.\nThese generators produce a matrix of features and corresponding discrete targets.\nBoth make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\nmake_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\nmake_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles.\nScikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data.\nA dataset is uniquely specified by its data_id, but not necessarily by its name. Several different \u201cversions\u201d of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that fetch_openml(name=\"miceprotein\") can yield different results at different times if earlier versions become inactive. You can see that the dataset with data_id 40966 that we fetched above is the version 1 of the \u201cmiceprotein\u201d dataset:\nIn fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:\nSpecifying the dataset by the name \u201ciris\u201d yields the lowest version, version 1, with the data_id 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset data_id. The other dataset, with data_id 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:\nYou can also specify both the name and the version, which also uniquely identifies the dataset:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#general-dataset-api", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "192": {"Title": "6.2. Toy datasets", "Text": "scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\nThey can be loaded using the following functions:\nThese datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.\nData Set Characteristics:\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher\u2019s paper. Note that it\u2019s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\nThis module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\nData Set Characteristics:\nIn order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news:\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):\nsklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names.\nFaces recognition example using eigenfaces and SVMs\nmake_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\nmake_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance).\nOther regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target.\nscikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.\nYou may load a dataset like as follows:\nYou may also load two (or more) datasets at once:\nIn this case, X_train and X_test are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         ", "Url": "https://scikit-learn.org/stable/datasets/index.html#general-dataset-api", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "193": {"Title": "6.3. Real world datasets", "Text": "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\nData Set Characteristics:\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).\nSource URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\nFor more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \u201cLeast Angle Regression,\u201d Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nscikit-learn provides tools to load larger datasets, downloading them if necessary.\nThey can be loaded using the following functions:\nIt is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren\u2019t from this window of time.\nFor example, let\u2019s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:\n(The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of what\u2019s going on inside this classifier?)\nLet\u2019s take a look at what the most informative features are:\nYou can now see many things that these features have overfit to:\nWith such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.\nFor this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively.\nThis classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:\nSome other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the --filter option to compare the results.\nThis dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:\nEach picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\nAn alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\nBoth Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.\nData Set Characteristics:\nopenml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.\nThe sklearn.datasets package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml.\nFor example, to download a dataset of gene expressions in mice brains:\nTo fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes:\nYou can get more information on the dataset by looking at the DESCR and details attributes:\nThe DESCR contains a free-text description of the data, while details contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The data_id of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:\nThe data_id also uniquely identifies a dataset from OpenML:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#general-dataset-api", "Attachment_Url": []}, "194": {"Title": "6.4. Generated datasets", "Text": "Data Set Characteristics:\nThis is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\nThe data set contains images of hand-written digits: 10 classes where each class refers to a digit.\nPreprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.\nThe samples in this dataset correspond to 30\u00d730m patches of forest in the US, collected for the task of predicting each patch\u2019s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the dataset\u2019s homepage. Some of the features are boolean indicators, while others are discrete or continuous measurements.\nData Set Characteristics:\nsklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the data member and the target values in target. The dataset will be downloaded from the web if necessary.\nIn addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\nscikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.\nHere are some recommended ways to load standard columnar data into a format usable by scikit-learn:\nFor some miscellaneous data such as images, videos, and audio, you may wish to refer to:\nCategorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data.\nNote: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/datasets/index.html#general-dataset-api", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "195": {"Title": "6.5. Loading other datasets", "Text": "Data Set Characteristics:\nThe Linnerud dataset constains two small dataset:\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1].\nData Set Characteristics:\nsklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels:\nIt returns a dictionary-like object, with the following attributes:\ndata: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in [1]: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values:\ntarget: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:\nsample_id: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:\ntarget_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for \u2018GMIL\u2019, to 381327 for \u2018CCAT\u2019:\nThe dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.\n", "Code_snippet": ">>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         >>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#general-dataset-api", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "196": {"Title": "6.1. General dataset API", "Text": "There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\nThe dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section.\nThe dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section.\nBoth loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape n_samples * n_features with key data (except for 20newsgroups) and a numpy array of length n_samples, containing the target values, with key target.\nIt\u2019s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the return_X_y parameter to True.\nThe datasets also contain a full description in their DESCR attribute and some contain feature_names and target_names. See the dataset descriptions below for details.\nThe dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section.\nThese functions return a tuple (X, y) consisting of a n_samples * n_features numpy array X and an array of length n_samples containing the targets y.\nIn addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section.\nData Set Characteristics:\nThis is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u2018Hedonic prices and the demand for clean air\u2019, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, \u2018Regression diagnostics \u2026\u2019, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\nThe Boston house-price data has been used in many machine learning papers that address regression problems.\nThis dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T.\nAs described on the original website:\nData Set Characteristics:\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.\nThe \u201ctarget\u201d for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.\nThe original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\nThe sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them:\nThe real data lies in the filenames and target attributes. The target attribute is the integer index of the category:\nIt is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function:\nscikit-learn provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.\nThe first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):\nThe default slice is a rectangular shape around the face, removing most of the background:\nEach of the 1140 faces is assigned to a single person id in the target array:\nThe second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:\nBoth for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing color=True, in that case the shape will be (2200, 2, 62, 47, 3).\nThe sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a 10-folds cross validation scheme.\nThese generators produce a matrix of features and corresponding discrete targets.\nBoth make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\nmake_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\nmake_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles.\nScikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data.\nA dataset is uniquely specified by its data_id, but not necessarily by its name. Several different \u201cversions\u201d of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that fetch_openml(name=\"miceprotein\") can yield different results at different times if earlier versions become inactive. You can see that the dataset with data_id 40966 that we fetched above is the version 1 of the \u201cmiceprotein\u201d dataset:\nIn fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:\nSpecifying the dataset by the name \u201ciris\u201d yields the lowest version, version 1, with the data_id 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset data_id. The other dataset, with data_id 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:\nYou can also specify both the name and the version, which also uniquely identifies the dataset:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#toy-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "197": {"Title": "6.2. Toy datasets", "Text": "scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\nThey can be loaded using the following functions:\nThese datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.\nData Set Characteristics:\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher\u2019s paper. Note that it\u2019s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\nThis module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\nData Set Characteristics:\nIn order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news:\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):\nsklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names.\nFaces recognition example using eigenfaces and SVMs\nmake_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\nmake_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance).\nOther regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target.\nscikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.\nYou may load a dataset like as follows:\nYou may also load two (or more) datasets at once:\nIn this case, X_train and X_test are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         ", "Url": "https://scikit-learn.org/stable/datasets/index.html#toy-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "198": {"Title": "6.3. Real world datasets", "Text": "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\nData Set Characteristics:\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).\nSource URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\nFor more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \u201cLeast Angle Regression,\u201d Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nscikit-learn provides tools to load larger datasets, downloading them if necessary.\nThey can be loaded using the following functions:\nIt is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren\u2019t from this window of time.\nFor example, let\u2019s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:\n(The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of what\u2019s going on inside this classifier?)\nLet\u2019s take a look at what the most informative features are:\nYou can now see many things that these features have overfit to:\nWith such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.\nFor this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively.\nThis classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:\nSome other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the --filter option to compare the results.\nThis dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:\nEach picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\nAn alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\nBoth Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.\nData Set Characteristics:\nopenml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.\nThe sklearn.datasets package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml.\nFor example, to download a dataset of gene expressions in mice brains:\nTo fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes:\nYou can get more information on the dataset by looking at the DESCR and details attributes:\nThe DESCR contains a free-text description of the data, while details contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The data_id of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:\nThe data_id also uniquely identifies a dataset from OpenML:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#toy-datasets", "Attachment_Url": []}, "199": {"Title": "6.4. Generated datasets", "Text": "Data Set Characteristics:\nThis is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\nThe data set contains images of hand-written digits: 10 classes where each class refers to a digit.\nPreprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.\nThe samples in this dataset correspond to 30\u00d730m patches of forest in the US, collected for the task of predicting each patch\u2019s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the dataset\u2019s homepage. Some of the features are boolean indicators, while others are discrete or continuous measurements.\nData Set Characteristics:\nsklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the data member and the target values in target. The dataset will be downloaded from the web if necessary.\nIn addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\nscikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.\nHere are some recommended ways to load standard columnar data into a format usable by scikit-learn:\nFor some miscellaneous data such as images, videos, and audio, you may wish to refer to:\nCategorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data.\nNote: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/datasets/index.html#toy-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "200": {"Title": "6.5. Loading other datasets", "Text": "Data Set Characteristics:\nThe Linnerud dataset constains two small dataset:\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1].\nData Set Characteristics:\nsklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels:\nIt returns a dictionary-like object, with the following attributes:\ndata: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in [1]: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values:\ntarget: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:\nsample_id: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:\ntarget_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for \u2018GMIL\u2019, to 381327 for \u2018CCAT\u2019:\nThe dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.\n", "Code_snippet": ">>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         >>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#toy-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "201": {"Title": "6.1. General dataset API", "Text": "There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\nThe dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section.\nThe dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section.\nBoth loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape n_samples * n_features with key data (except for 20newsgroups) and a numpy array of length n_samples, containing the target values, with key target.\nIt\u2019s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the return_X_y parameter to True.\nThe datasets also contain a full description in their DESCR attribute and some contain feature_names and target_names. See the dataset descriptions below for details.\nThe dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section.\nThese functions return a tuple (X, y) consisting of a n_samples * n_features numpy array X and an array of length n_samples containing the targets y.\nIn addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section.\nData Set Characteristics:\nThis is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u2018Hedonic prices and the demand for clean air\u2019, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, \u2018Regression diagnostics \u2026\u2019, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\nThe Boston house-price data has been used in many machine learning papers that address regression problems.\nThis dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T.\nAs described on the original website:\nData Set Characteristics:\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.\nThe \u201ctarget\u201d for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.\nThe original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\nThe sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them:\nThe real data lies in the filenames and target attributes. The target attribute is the integer index of the category:\nIt is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function:\nscikit-learn provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.\nThe first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):\nThe default slice is a rectangular shape around the face, removing most of the background:\nEach of the 1140 faces is assigned to a single person id in the target array:\nThe second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:\nBoth for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing color=True, in that case the shape will be (2200, 2, 62, 47, 3).\nThe sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a 10-folds cross validation scheme.\nThese generators produce a matrix of features and corresponding discrete targets.\nBoth make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\nmake_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\nmake_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles.\nScikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data.\nA dataset is uniquely specified by its data_id, but not necessarily by its name. Several different \u201cversions\u201d of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that fetch_openml(name=\"miceprotein\") can yield different results at different times if earlier versions become inactive. You can see that the dataset with data_id 40966 that we fetched above is the version 1 of the \u201cmiceprotein\u201d dataset:\nIn fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:\nSpecifying the dataset by the name \u201ciris\u201d yields the lowest version, version 1, with the data_id 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset data_id. The other dataset, with data_id 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:\nYou can also specify both the name and the version, which also uniquely identifies the dataset:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#real-world-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "202": {"Title": "6.2. Toy datasets", "Text": "scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\nThey can be loaded using the following functions:\nThese datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.\nData Set Characteristics:\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher\u2019s paper. Note that it\u2019s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\nThis module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\nData Set Characteristics:\nIn order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news:\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):\nsklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names.\nFaces recognition example using eigenfaces and SVMs\nmake_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\nmake_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance).\nOther regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target.\nscikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.\nYou may load a dataset like as follows:\nYou may also load two (or more) datasets at once:\nIn this case, X_train and X_test are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         ", "Url": "https://scikit-learn.org/stable/datasets/index.html#real-world-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "203": {"Title": "6.3. Real world datasets", "Text": "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\nData Set Characteristics:\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).\nSource URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\nFor more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \u201cLeast Angle Regression,\u201d Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nscikit-learn provides tools to load larger datasets, downloading them if necessary.\nThey can be loaded using the following functions:\nIt is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren\u2019t from this window of time.\nFor example, let\u2019s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:\n(The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of what\u2019s going on inside this classifier?)\nLet\u2019s take a look at what the most informative features are:\nYou can now see many things that these features have overfit to:\nWith such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.\nFor this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively.\nThis classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:\nSome other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the --filter option to compare the results.\nThis dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:\nEach picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\nAn alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\nBoth Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.\nData Set Characteristics:\nopenml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.\nThe sklearn.datasets package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml.\nFor example, to download a dataset of gene expressions in mice brains:\nTo fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes:\nYou can get more information on the dataset by looking at the DESCR and details attributes:\nThe DESCR contains a free-text description of the data, while details contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The data_id of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:\nThe data_id also uniquely identifies a dataset from OpenML:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#real-world-datasets", "Attachment_Url": []}, "204": {"Title": "6.4. Generated datasets", "Text": "Data Set Characteristics:\nThis is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\nThe data set contains images of hand-written digits: 10 classes where each class refers to a digit.\nPreprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.\nThe samples in this dataset correspond to 30\u00d730m patches of forest in the US, collected for the task of predicting each patch\u2019s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the dataset\u2019s homepage. Some of the features are boolean indicators, while others are discrete or continuous measurements.\nData Set Characteristics:\nsklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the data member and the target values in target. The dataset will be downloaded from the web if necessary.\nIn addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\nscikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.\nHere are some recommended ways to load standard columnar data into a format usable by scikit-learn:\nFor some miscellaneous data such as images, videos, and audio, you may wish to refer to:\nCategorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data.\nNote: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/datasets/index.html#real-world-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "205": {"Title": "6.5. Loading other datasets", "Text": "Data Set Characteristics:\nThe Linnerud dataset constains two small dataset:\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1].\nData Set Characteristics:\nsklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels:\nIt returns a dictionary-like object, with the following attributes:\ndata: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in [1]: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values:\ntarget: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:\nsample_id: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:\ntarget_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for \u2018GMIL\u2019, to 381327 for \u2018CCAT\u2019:\nThe dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.\n", "Code_snippet": ">>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         >>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#real-world-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "206": {"Title": "6.1. General dataset API", "Text": "There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\nThe dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section.\nThe dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section.\nBoth loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape n_samples * n_features with key data (except for 20newsgroups) and a numpy array of length n_samples, containing the target values, with key target.\nIt\u2019s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the return_X_y parameter to True.\nThe datasets also contain a full description in their DESCR attribute and some contain feature_names and target_names. See the dataset descriptions below for details.\nThe dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section.\nThese functions return a tuple (X, y) consisting of a n_samples * n_features numpy array X and an array of length n_samples containing the targets y.\nIn addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section.\nData Set Characteristics:\nThis is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u2018Hedonic prices and the demand for clean air\u2019, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, \u2018Regression diagnostics \u2026\u2019, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\nThe Boston house-price data has been used in many machine learning papers that address regression problems.\nThis dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T.\nAs described on the original website:\nData Set Characteristics:\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.\nThe \u201ctarget\u201d for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.\nThe original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\nThe sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them:\nThe real data lies in the filenames and target attributes. The target attribute is the integer index of the category:\nIt is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function:\nscikit-learn provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.\nThe first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):\nThe default slice is a rectangular shape around the face, removing most of the background:\nEach of the 1140 faces is assigned to a single person id in the target array:\nThe second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:\nBoth for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing color=True, in that case the shape will be (2200, 2, 62, 47, 3).\nThe sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a 10-folds cross validation scheme.\nThese generators produce a matrix of features and corresponding discrete targets.\nBoth make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\nmake_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\nmake_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles.\nScikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data.\nA dataset is uniquely specified by its data_id, but not necessarily by its name. Several different \u201cversions\u201d of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that fetch_openml(name=\"miceprotein\") can yield different results at different times if earlier versions become inactive. You can see that the dataset with data_id 40966 that we fetched above is the version 1 of the \u201cmiceprotein\u201d dataset:\nIn fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:\nSpecifying the dataset by the name \u201ciris\u201d yields the lowest version, version 1, with the data_id 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset data_id. The other dataset, with data_id 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:\nYou can also specify both the name and the version, which also uniquely identifies the dataset:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#generated-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "207": {"Title": "6.2. Toy datasets", "Text": "scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\nThey can be loaded using the following functions:\nThese datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.\nData Set Characteristics:\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher\u2019s paper. Note that it\u2019s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\nThis module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\nData Set Characteristics:\nIn order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news:\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):\nsklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names.\nFaces recognition example using eigenfaces and SVMs\nmake_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\nmake_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance).\nOther regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target.\nscikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.\nYou may load a dataset like as follows:\nYou may also load two (or more) datasets at once:\nIn this case, X_train and X_test are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         ", "Url": "https://scikit-learn.org/stable/datasets/index.html#generated-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "208": {"Title": "6.3. Real world datasets", "Text": "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\nData Set Characteristics:\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).\nSource URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\nFor more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \u201cLeast Angle Regression,\u201d Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nscikit-learn provides tools to load larger datasets, downloading them if necessary.\nThey can be loaded using the following functions:\nIt is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren\u2019t from this window of time.\nFor example, let\u2019s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:\n(The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of what\u2019s going on inside this classifier?)\nLet\u2019s take a look at what the most informative features are:\nYou can now see many things that these features have overfit to:\nWith such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.\nFor this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively.\nThis classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:\nSome other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the --filter option to compare the results.\nThis dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:\nEach picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\nAn alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\nBoth Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.\nData Set Characteristics:\nopenml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.\nThe sklearn.datasets package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml.\nFor example, to download a dataset of gene expressions in mice brains:\nTo fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes:\nYou can get more information on the dataset by looking at the DESCR and details attributes:\nThe DESCR contains a free-text description of the data, while details contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The data_id of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:\nThe data_id also uniquely identifies a dataset from OpenML:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#generated-datasets", "Attachment_Url": []}, "209": {"Title": "6.4. Generated datasets", "Text": "Data Set Characteristics:\nThis is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\nThe data set contains images of hand-written digits: 10 classes where each class refers to a digit.\nPreprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.\nThe samples in this dataset correspond to 30\u00d730m patches of forest in the US, collected for the task of predicting each patch\u2019s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the dataset\u2019s homepage. Some of the features are boolean indicators, while others are discrete or continuous measurements.\nData Set Characteristics:\nsklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the data member and the target values in target. The dataset will be downloaded from the web if necessary.\nIn addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\nscikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.\nHere are some recommended ways to load standard columnar data into a format usable by scikit-learn:\nFor some miscellaneous data such as images, videos, and audio, you may wish to refer to:\nCategorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data.\nNote: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/datasets/index.html#generated-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "210": {"Title": "6.5. Loading other datasets", "Text": "Data Set Characteristics:\nThe Linnerud dataset constains two small dataset:\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1].\nData Set Characteristics:\nsklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels:\nIt returns a dictionary-like object, with the following attributes:\ndata: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in [1]: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values:\ntarget: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:\nsample_id: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:\ntarget_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for \u2018GMIL\u2019, to 381327 for \u2018CCAT\u2019:\nThe dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.\n", "Code_snippet": ">>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         >>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#generated-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "211": {"Title": "6.1. General dataset API", "Text": "There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.\nThe dataset loaders. They can be used to load small standard datasets, described in the Toy datasets section.\nThe dataset fetchers. They can be used to download and load larger datasets, described in the Real world datasets section.\nBoth loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape n_samples * n_features with key data (except for 20newsgroups) and a numpy array of length n_samples, containing the target values, with key target.\nIt\u2019s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the return_X_y parameter to True.\nThe datasets also contain a full description in their DESCR attribute and some contain feature_names and target_names. See the dataset descriptions below for details.\nThe dataset generation functions. They can be used to generate controlled synthetic datasets, described in the Generated datasets section.\nThese functions return a tuple (X, y) consisting of a n_samples * n_features numpy array X and an array of length n_samples containing the targets y.\nIn addition, there are also miscellaneous tools to load datasets of other formats or from other locations, described in the Loading other datasets section.\nData Set Characteristics:\nThis is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u2018Hedonic prices and the demand for clean air\u2019, J. Environ. Economics & Management, vol.5, 81-102, 1978. Used in Belsley, Kuh & Welsch, \u2018Regression diagnostics \u2026\u2019, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.\nThe Boston house-price data has been used in many machine learning papers that address regression problems.\nThis dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The sklearn.datasets.fetch_olivetti_faces function is the data fetching / caching function that downloads the data archive from AT&T.\nAs described on the original website:\nData Set Characteristics:\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.\nThe \u201ctarget\u201d for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.\nThe original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\nThe sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them:\nThe real data lies in the filenames and target attributes. The target attribute is the integer index of the category:\nIt is possible to load only a sub-selection of the categories by passing the list of the categories to load to the sklearn.datasets.fetch_20newsgroups function:\nscikit-learn provides two loaders that will automatically download, cache, parse the metadata files, decode the jpeg and convert the interesting slices into memmapped numpy arrays. This dataset size is more than 200 MB. The first load typically takes more than a couple of minutes to fully decode the relevant part of the JPEG files into numpy arrays. If the dataset has been loaded once, the following times the loading times less than 200ms by using a memmapped version memoized on the disk in the ~/scikit_learn_data/lfw_home/ folder using joblib.\nThe first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):\nThe default slice is a rectangular shape around the face, removing most of the background:\nEach of the 1140 faces is assigned to a single person id in the target array:\nThe second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:\nBoth for the sklearn.datasets.fetch_lfw_people and sklearn.datasets.fetch_lfw_pairs function it is possible to get an additional dimension with the RGB color channels by passing color=True, in that case the shape will be (2200, 2, 62, 47, 3).\nThe sklearn.datasets.fetch_lfw_pairs datasets is subdivided into 3 subsets: the development train set, the development test set and an evaluation 10_folds set meant to compute performance metrics using a 10-folds cross validation scheme.\nThese generators produce a matrix of features and corresponding discrete targets.\nBoth make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. make_blobs provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. make_classification specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.\nmake_gaussian_quantiles divides a single Gaussian cluster into near-equal-size classes separated by concentric hyperspheres. make_hastie_10_2 generates a similar binary, 10-dimensional problem.\nmake_circles and make_moons generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. They are useful for visualisation. make_circles produces Gaussian data with a spherical decision boundary for binary classification, while make_moons produces two interleaving half circles.\nScikit-learn also embed a couple of sample JPEG images published under Creative Commons license by their authors. Those images can be useful to test algorithms and pipeline on 2D data.\nA dataset is uniquely specified by its data_id, but not necessarily by its name. Several different \u201cversions\u201d of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that fetch_openml(name=\"miceprotein\") can yield different results at different times if earlier versions become inactive. You can see that the dataset with data_id 40966 that we fetched above is the version 1 of the \u201cmiceprotein\u201d dataset:\nIn fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:\nSpecifying the dataset by the name \u201ciris\u201d yields the lowest version, version 1, with the data_id 61. To make sure you always get this exact dataset, it is safest to specify it by the dataset data_id. The other dataset, with data_id 969, is version 3 (version 2 has become inactive), and contains a binarized version of the data:\nYou can also specify both the name and the version, which also uniquely identifies the dataset:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#loading-other-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "212": {"Title": "6.2. Toy datasets", "Text": "scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\nThey can be loaded using the following functions:\nThese datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.\nData Set Characteristics:\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher\u2019s paper. Note that it\u2019s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\nThis module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\nData Set Characteristics:\nIn order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the sklearn.feature_extraction.text as demonstrated in the following example that extract TF-IDF vectors of unigram tokens from a subset of 20news:\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):\nsklearn.datasets.fetch_20newsgroups_vectorized is a function which returns ready-to-use token counts features instead of file names.\nFaces recognition example using eigenfaces and SVMs\nmake_multilabel_classification generates random samples with multiple labels, reflecting a bag of words drawn from a mixture of topics. The number of topics for each document is drawn from a Poisson distribution, and the topics themselves are drawn from a fixed random distribution. Similarly, the number of words is drawn from Poisson, with words drawn from a multinomial, where each topic defines a probability distribution over words. Simplifications with respect to true bag-of-words mixtures include:\nmake_regression produces regression targets as an optionally-sparse random linear combination of random features, with noise. Its informative features may be uncorrelated, or low rank (few features account for most of the variance).\nOther regression generators generate functions deterministically from randomized features. make_sparse_uncorrelated produces a target as a linear combination of four features with fixed coefficients. Others encode explicitly non-linear relations: make_friedman1 is related by polynomial and sine transforms; make_friedman2 includes feature multiplication and reciprocation; and make_friedman3 is similar with an arctan transformation on the target.\nscikit-learn includes utility functions for loading datasets in the svmlight / libsvm format. In this format, each line takes the form <label> <feature-id>:<feature-value> <feature-id>:<feature-value> .... This format is especially suitable for sparse datasets. In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.\nYou may load a dataset like as follows:\nYou may also load two (or more) datasets at once:\nIn this case, X_train and X_test are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         ", "Url": "https://scikit-learn.org/stable/datasets/index.html#loading-other-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "213": {"Title": "6.3. Real world datasets", "Text": "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\nData Set Characteristics:\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times n_samples (i.e. the sum of squares of each column totals 1).\nSource URL: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\nFor more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \u201cLeast Angle Regression,\u201d Annals of Statistics (with discussion), 407-499. (https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nscikit-learn provides tools to load larger datasets, downloading them if necessary.\nThey can be loaded using the following functions:\nIt is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren\u2019t from this window of time.\nFor example, let\u2019s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:\n(The example Classification of text documents using sparse features shuffles the training and test data, instead of segmenting by time, and in that case multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious yet of what\u2019s going on inside this classifier?)\nLet\u2019s take a look at what the most informative features are:\nYou can now see many things that these features have overfit to:\nWith such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.\nFor this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. remove should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively.\nThis classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:\nSome other classifiers cope better with this harder version of the task. Try running Sample pipeline for text feature extraction and evaluation with and without the --filter option to compare the results.\nThis dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:\nEach picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\nAn alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\nBoth Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.\nData Set Characteristics:\nopenml.org is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.\nThe sklearn.datasets package is able to download datasets from the repository using the function sklearn.datasets.fetch_openml.\nFor example, to download a dataset of gene expressions in mice brains:\nTo fully specify a dataset, you need to provide a name and a version, though the version is optional, see Dataset Versions below. The dataset contains a total of 1080 examples belonging to 8 different classes:\nYou can get more information on the dataset by looking at the DESCR and details attributes:\nThe DESCR contains a free-text description of the data, while details contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the OpenML documentation The data_id of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:\nThe data_id also uniquely identifies a dataset from OpenML:\n", "Code_snippet": ">>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n>>> from pprint import pprint\n>>> pprint(list(newsgroups_train.target_names))\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']>>> newsgroups_train.filenames.shape\n(11314,)\n>>> newsgroups_train.target.shape\n(11314,)\n>>> newsgroups_train.target[:10]\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])>>> cats = ['alt.atheism', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n>>> list(newsgroups_train.target_names)\n['alt.atheism', 'sci.space']\n>>> newsgroups_train.filenames.shape\n(1073,)\n>>> newsgroups_train.target.shape\n(1073,)\n>>> newsgroups_train.target[:10]\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> categories = ['alt.atheism', 'talk.religion.misc',\n...               'comp.graphics', 'sci.space']\n>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       categories=categories)\n>>> vectorizer = TfidfVectorizer()\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> vectors.shape\n(2034, 34118)>>> vectors.nnz / float(vectors.shape[0])       \n159.01327...>>> from sklearn.naive_bayes import MultinomialNB\n>>> from sklearn import metrics\n>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.88213...>>> import numpy as np\n>>> def show_top10(classifier, vectorizer, categories):\n...     feature_names = np.asarray(vectorizer.get_feature_names())\n...     for i, category in enumerate(categories):\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n...\n>>> show_top10(clf, vectorizer, newsgroups_train.target_names)\nalt.atheism: edu it and in you that is of to the\ncomp.graphics: edu in graphics it is for and of to the\nsci.space: edu it that is in and space to of the\ntalk.religion.misc: not it you in is that and to of the>>> newsgroups_test = fetch_20newsgroups(subset='test',\n...                                      remove=('headers', 'footers', 'quotes'),\n...                                      categories=categories)\n>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  \n0.77310...>>> newsgroups_train = fetch_20newsgroups(subset='train',\n...                                       remove=('headers', 'footers', 'quotes'),\n...                                       categories=categories)\n>>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n>>> clf = MultinomialNB(alpha=.01)\n>>> clf.fit(vectors, newsgroups_train.target)\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)>>> vectors_test = vectorizer.transform(newsgroups_test.data)\n>>> pred = clf.predict(vectors_test)\n>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  \n0.76995...>>> from sklearn.datasets import fetch_lfw_people\n>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n>>> for name in lfw_people.target_names:\n...     print(name)\n...\nAriel Sharon\nColin Powell\nDonald Rumsfeld\nGeorge W Bush\nGerhard Schroeder\nHugo Chavez\nTony Blair>>> lfw_people.data.dtype\ndtype('float32')\n\n>>> lfw_people.data.shape\n(1288, 1850)\n\n>>> lfw_people.images.shape\n(1288, 50, 37)>>> lfw_people.target.shape\n(1288,)\n\n>>> list(lfw_people.target[:10])\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]>>> from sklearn.datasets import fetch_lfw_pairs\n>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\n\n>>> list(lfw_pairs_train.target_names)\n['Different persons', 'Same person']\n\n>>> lfw_pairs_train.pairs.shape\n(2200, 2, 62, 47)\n\n>>> lfw_pairs_train.data.shape\n(2200, 5828)\n\n>>> lfw_pairs_train.target.shape\n(2200,)>>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#loading-other-datasets", "Attachment_Url": []}, "214": {"Title": "6.4. Generated datasets", "Text": "Data Set Characteristics:\nThis is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\nThe data set contains images of hand-written digits: 10 classes where each class refers to a digit.\nPreprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.\nThe samples in this dataset correspond to 30\u00d730m patches of forest in the US, collected for the task of predicting each patch\u2019s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the dataset\u2019s homepage. Some of the features are boolean indicators, while others are discrete or continuous measurements.\nData Set Characteristics:\nsklearn.datasets.fetch_covtype will load the covertype dataset; it returns a dictionary-like object with the feature matrix in the data member and the target values in target. The dataset will be downloaded from the web if necessary.\nIn addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.\nscikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.\nHere are some recommended ways to load standard columnar data into a format usable by scikit-learn:\nFor some miscellaneous data such as images, videos, and audio, you may wish to refer to:\nCategorical (or nominal) features stored as strings (common in pandas DataFrames) will need converting to numerical features using sklearn.preprocessing.OneHotEncoder or sklearn.preprocessing.OrdinalEncoder or similar. See Preprocessing data.\nNote: if you manage your own numerical data it is recommended to use an optimized file format such as HDF5 to reduce data load times. Various libraries such as H5Py, PyTables and pandas provides a Python interface for reading and writing data in that format.\n", "Code_snippet": "", "Url": "https://scikit-learn.org/stable/datasets/index.html#loading-other-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_random_dataset_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_random_multilabel_dataset_0011.png"]}, "215": {"Title": "6.5. Loading other datasets", "Text": "Data Set Characteristics:\nThe Linnerud dataset constains two small dataset:\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes. The dataset is extensively described in [1].\nData Set Characteristics:\nsklearn.datasets.fetch_rcv1 will load the following version: RCV1-v2, vectors, full sets, topics multilabels:\nIt returns a dictionary-like object, with the following attributes:\ndata: The feature matrix is a scipy CSR sparse matrix, with 804414 samples and 47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors. A nearly chronological split is proposed in [1]: The first 23149 samples are the training set. The last 781265 samples are the testing set. This follows the official LYRL2004 chronological split. The array has 0.16% of non zero values:\ntarget: The target values are stored in a scipy CSR sparse matrix, with 804414 samples and 103 categories. Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values:\nsample_id: Each sample can be identified by its ID, ranging (with gaps) from 2286 to 810596:\ntarget_names: The target values are the topics of each sample. Each sample belongs to at least one topic, and to up to 17 topics. There are 103 topics, each represented by a string. Their corpus frequencies span five orders of magnitude, from 5 occurrences for \u2018GMIL\u2019, to 381327 for \u2018CCAT\u2019:\nThe dataset will be downloaded from the rcv1 homepage if necessary. The compressed size is about 656 MB.\n", "Code_snippet": ">>> from sklearn.datasets import fetch_rcv1\n>>> rcv1 = fetch_rcv1()>>> rcv1.data.shape\n(804414, 47236)>>> rcv1.target.shape\n(804414, 103)>>> rcv1.sample_id[:3]\narray([2286, 2287, 2288], dtype=uint32)>>> rcv1.target_names[:3].tolist()  \n['E11', 'ECAT', 'M11']>>> from sklearn.datasets import load_svmlight_file\n>>> X_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\n...                                                         >>> X_train, y_train, X_test, y_test = load_svmlight_files(\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\n...                                                         >>> X_test, y_test = load_svmlight_file(\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\n...                                                         >>> from sklearn.datasets import fetch_openml\n>>> mice = fetch_openml(name='miceprotein', version=4)>>> mice.data.shape\n(1080, 77)\n>>> mice.target.shape\n(1080,)\n>>> np.unique(mice.target) \narray(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)>>> print(mice.DESCR) \n**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015\n**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\nSyndrome. PLoS ONE 10(6): e0129126...\n\n>>> mice.details \n{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',\n'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',\n'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',\n'file_id': '17928620', 'default_target_attribute': 'class',\n'row_id_attribute': 'MouseID',\n'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],\n'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],\n'visibility': 'public', 'status': 'active',\n'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}>>> mice.url\n'https://www.openml.org/d/40966'>>> mice = fetch_openml(data_id=40966)\n>>> mice.details \n{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',\n'creator': ...,\n'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':\n'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':\n'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\ne0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',\n'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':\n'3c479a6885bfa0438971388283a1ce32'}>>> mice.details['version']  \n'1'>>> iris = fetch_openml(name=\"iris\")\n>>> iris.details['version']  \n'1'\n>>> iris.details['id']  \n'61'\n\n>>> iris_61 = fetch_openml(data_id=61)\n>>> iris_61.details['version']\n'1'\n>>> iris_61.details['id']\n'61'\n\n>>> iris_969 = fetch_openml(data_id=969)\n>>> iris_969.details['version']\n'3'\n>>> iris_969.details['id']\n'969'>>> np.unique(iris_969.target)\narray(['N', 'P'], dtype=object)>>> iris_version_3 = fetch_openml(name=\"iris\", version=3)\n>>> iris_version_3.details['version']\n'3'\n>>> iris_version_3.details['id']\n'969'", "Url": "https://scikit-learn.org/stable/datasets/index.html#loading-other-datasets", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_color_quantization_0011.png"]}, "216": {"Title": "7.1. Strategies to scale computationally: bigger data", "Text": "For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.\nOut-of-core (or \u201cexternal memory\u201d) learning is a technique used to learn from data that cannot fit in a computer\u2019s main memory (RAM).\nHere is a sketch of a system designed to achieve this goal:\nBasically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.\nOne of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment.\nA last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.\nIn general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:\n\n\nTo benchmark different estimators for your case you can simply change the n_features parameter in this example: Prediction Latency. This should give you an estimate of the order of magnitude of the prediction latency.\nAs scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library.\nNot all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR, NuSVC, NuSVR). On the other hand a linear model implemented with a BLAS DGEMM call (via numpy.dot) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS.\nYou can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands:\nMore information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.\nScikit-learn uses the joblib library to enable parallel computing inside its estimators. See the joblib documentation for the switches to control parallel computing.\nNote that, by default, scikit-learn uses its embedded (vendored) version of joblib. A configuration switch (documented below) controls this behavior.\nsklearn.set_config controls the following behaviors:\n", "Code_snippet": ">>> import sklearn\n>>> with sklearn.config_context(assume_finite=True):\n...     pass  # do learning/prediction here with reduced validationdef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))", "Url": "https://scikit-learn.org/stable/modules/computing.html#strategies-to-scale-computationally-bigger-data", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0041.png"]}, "217": {"Title": "7.2. Computational Performance", "Text": "2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents.\nFor some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).\nWe will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.\nPrediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile).\nPrediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second).\nAn important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.\nScikit-learn does some validation on data that increases the overhead per call to predict and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable SKLEARN_ASSUME_FINITE to a non-empty string before importing scikit-learn, or configure it in Python with sklearn.set_config. For more control than these global settings, a config_context allows you to set this configuration within a specified context:\nNote that this will affect all uses of sklearn.utils.assert_all_finite within the context.\nAnother important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this quantity for a number of estimators on synthetic data:\n\nThese throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the GIL) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.\nSome calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.set_config or config_context. The following suggests to limit temporary working memory to 128 MiB:\nAn example of a chunked operation adhering to this setting is metric.pairwise_distances_chunked, which facilitates computing row-wise reductions of a pairwise distance matrix.\nThese environment variables should be set before importing scikit-learn.\n", "Code_snippet": ">>> import sklearn\n>>> with sklearn.config_context(assume_finite=True):\n...     pass  # do learning/prediction here with reduced validationdef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work hereclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)", "Url": "https://scikit-learn.org/stable/modules/computing.html#strategies-to-scale-computationally-bigger-data", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0041.png"]}, "218": {"Title": "7.3. Parallelism, resource management, and configuration", "Text": "Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the partial_fit API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called \u201conline learning\u201d) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning [1].\nHere is a list of incremental estimators for different tasks:\nFor classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first partial_fit call using the classes= parameter.\nAnother aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the Perceptron is still sensitive to badly labeled examples even after many examples whereas the SGD* and PassiveAggressive* families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.\nObviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of\nM\ninstances with\nN\nfeatures, the space complexity is in\nO\n. From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features:\n\nOverall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator).\nModel compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.\nHere is sample code that illustrates the use of the sparsify() method:\nIn this example we prefer the elasticnet penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the l1_ratio parameter (in combination with the regularization strength alpha) to control this tradeoff.\nA typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.\n", "Code_snippet": "from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work hereclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)", "Url": "https://scikit-learn.org/stable/modules/computing.html#strategies-to-scale-computationally-bigger-data", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png"]}, "219": {"Title": "7.1. Strategies to scale computationally: bigger data", "Text": "For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.\nOut-of-core (or \u201cexternal memory\u201d) learning is a technique used to learn from data that cannot fit in a computer\u2019s main memory (RAM).\nHere is a sketch of a system designed to achieve this goal:\nBasically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.\nOne of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment.\nA last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.\nIn general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:\n\n\nTo benchmark different estimators for your case you can simply change the n_features parameter in this example: Prediction Latency. This should give you an estimate of the order of magnitude of the prediction latency.\nAs scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library.\nNot all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR, NuSVC, NuSVR). On the other hand a linear model implemented with a BLAS DGEMM call (via numpy.dot) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS.\nYou can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands:\nMore information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.\nScikit-learn uses the joblib library to enable parallel computing inside its estimators. See the joblib documentation for the switches to control parallel computing.\nNote that, by default, scikit-learn uses its embedded (vendored) version of joblib. A configuration switch (documented below) controls this behavior.\nsklearn.set_config controls the following behaviors:\n", "Code_snippet": ">>> import sklearn\n>>> with sklearn.config_context(assume_finite=True):\n...     pass  # do learning/prediction here with reduced validationdef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))", "Url": "https://scikit-learn.org/stable/modules/computing.html#computational-performance", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0041.png"]}, "220": {"Title": "7.2. Computational Performance", "Text": "2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents.\nFor some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).\nWe will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.\nPrediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile).\nPrediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second).\nAn important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.\nScikit-learn does some validation on data that increases the overhead per call to predict and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable SKLEARN_ASSUME_FINITE to a non-empty string before importing scikit-learn, or configure it in Python with sklearn.set_config. For more control than these global settings, a config_context allows you to set this configuration within a specified context:\nNote that this will affect all uses of sklearn.utils.assert_all_finite within the context.\nAnother important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this quantity for a number of estimators on synthetic data:\n\nThese throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the GIL) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.\nSome calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.set_config or config_context. The following suggests to limit temporary working memory to 128 MiB:\nAn example of a chunked operation adhering to this setting is metric.pairwise_distances_chunked, which facilitates computing row-wise reductions of a pairwise distance matrix.\nThese environment variables should be set before importing scikit-learn.\n", "Code_snippet": ">>> import sklearn\n>>> with sklearn.config_context(assume_finite=True):\n...     pass  # do learning/prediction here with reduced validationdef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work hereclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)", "Url": "https://scikit-learn.org/stable/modules/computing.html#computational-performance", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0041.png"]}, "221": {"Title": "7.3. Parallelism, resource management, and configuration", "Text": "Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the partial_fit API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called \u201conline learning\u201d) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning [1].\nHere is a list of incremental estimators for different tasks:\nFor classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first partial_fit call using the classes= parameter.\nAnother aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the Perceptron is still sensitive to badly labeled examples even after many examples whereas the SGD* and PassiveAggressive* families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.\nObviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of\nM\ninstances with\nN\nfeatures, the space complexity is in\nO\n. From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features:\n\nOverall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator).\nModel compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.\nHere is sample code that illustrates the use of the sparsify() method:\nIn this example we prefer the elasticnet penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the l1_ratio parameter (in combination with the regularization strength alpha) to control this tradeoff.\nA typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.\n", "Code_snippet": "from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work hereclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)", "Url": "https://scikit-learn.org/stable/modules/computing.html#computational-performance", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png"]}, "222": {"Title": "7.1. Strategies to scale computationally: bigger data", "Text": "For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.\nOut-of-core (or \u201cexternal memory\u201d) learning is a technique used to learn from data that cannot fit in a computer\u2019s main memory (RAM).\nHere is a sketch of a system designed to achieve this goal:\nBasically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.\nOne of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment.\nA last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.\nIn general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:\n\n\nTo benchmark different estimators for your case you can simply change the n_features parameter in this example: Prediction Latency. This should give you an estimate of the order of magnitude of the prediction latency.\nAs scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized BLAS / LAPACK library.\nNot all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR, NuSVC, NuSVR). On the other hand a linear model implemented with a BLAS DGEMM call (via numpy.dot) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS.\nYou can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands:\nMore information can be found on the Scipy install page and in this blog post from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.\nScikit-learn uses the joblib library to enable parallel computing inside its estimators. See the joblib documentation for the switches to control parallel computing.\nNote that, by default, scikit-learn uses its embedded (vendored) version of joblib. A configuration switch (documented below) controls this behavior.\nsklearn.set_config controls the following behaviors:\n", "Code_snippet": ">>> import sklearn\n>>> with sklearn.config_context(assume_finite=True):\n...     pass  # do learning/prediction here with reduced validationdef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))", "Url": "https://scikit-learn.org/stable/modules/computing.html#parallelism-resource-management-and-configuration", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0041.png"]}, "223": {"Title": "7.2. Computational Performance", "Text": "2. could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.feature_extraction.FeatureHasher for datasets with categorical variables represented as list of Python dicts or sklearn.feature_extraction.text.HashingVectorizer for text documents.\nFor some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).\nWe will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.\nPrediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile).\nPrediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second).\nAn important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.\nScikit-learn does some validation on data that increases the overhead per call to predict and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable SKLEARN_ASSUME_FINITE to a non-empty string before importing scikit-learn, or configure it in Python with sklearn.set_config. For more control than these global settings, a config_context allows you to set this configuration within a specified context:\nNote that this will affect all uses of sklearn.utils.assert_all_finite within the context.\nAnother important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the Prediction Latency example that measures this quantity for a number of estimators on synthetic data:\n\nThese throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the GIL) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.\nSome calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory. This may potentially exhaust system memory. Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using sklearn.set_config or config_context. The following suggests to limit temporary working memory to 128 MiB:\nAn example of a chunked operation adhering to this setting is metric.pairwise_distances_chunked, which facilitates computing row-wise reductions of a pairwise distance matrix.\nThese environment variables should be set before importing scikit-learn.\n", "Code_snippet": ">>> import sklearn\n>>> with sklearn.config_context(assume_finite=True):\n...     pass  # do learning/prediction here with reduced validationdef sparsity_ratio(X):\n    return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\nprint(\"input sparsity ratio:\", sparsity_ratio(X))from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work hereclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)", "Url": "https://scikit-learn.org/stable/modules/computing.html#parallelism-resource-management-and-configuration", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0011.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0021.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_model_complexity_influence_0031.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0041.png", "https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0041.png"]}, "224": {"Title": "7.3. Parallelism, resource management, and configuration", "Text": "Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the partial_fit API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called \u201conline learning\u201d) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning [1].\nHere is a list of incremental estimators for different tasks:\nFor classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first partial_fit call using the classes= parameter.\nAnother aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the Perceptron is still sensitive to badly labeled examples even after many examples whereas the SGD* and PassiveAggressive* families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.\nObviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of\nM\ninstances with\nN\nfeatures, the space complexity is in\nO\n. From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features:\n\nOverall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator).\nModel compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.\nHere is sample code that illustrates the use of the sparsify() method:\nIn this example we prefer the elasticnet penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the l1_ratio parameter (in combination with the regularization strength alpha) to control this tradeoff.\nA typical benchmark on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.\n", "Code_snippet": "from numpy.distutils.system_info import get_info\nprint(get_info('blas_opt'))\nprint(get_info('lapack_opt'))>>> import sklearn\n>>> with sklearn.config_context(working_memory=128):\n...     pass  # do chunked work hereclf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nclf.fit(X_train, y_train).sparsify()\nclf.predict(X_test)", "Url": "https://scikit-learn.org/stable/modules/computing.html#parallelism-resource-management-and-configuration", "Attachment_Url": ["https://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_0031.png"]}, "225": {"Title": "Object Creation", "Text": "See the Data Structure Intro section.Creating a Series by passing a list of values, letting pandas create a default integer index:", "Code_snippet": "In [3]: s = pd.Series([1, 3, 5, np.nan, 6, 8])\n\nIn [4]: s\nOut[4]: \n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "226": {"Title": "Object Creation", "Text": "Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:", "Code_snippet": "In [5]: dates = pd.date_range('20130101', periods=6)\n\nIn [6]: dates\nOut[6]: \nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [7]: df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nIn [8]: df\nOut[8]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "227": {"Title": "Object Creation", "Text": "Creating a DataFrame by passing a dict of objects that can be converted to series-like.", "Code_snippet": "In [9]: df2 = pd.DataFrame({'A': 1.,\n   ...:                     'B': pd.Timestamp('20130102'),\n   ...:                     'C': pd.Series(1, index=list(range(4)), dtype='float32'),\n   ...:                     'D': np.array([3] * 4, dtype='int32'),\n   ...:                     'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n   ...:                     'F': 'foo'})\n   ...: \n\nIn [10]: df2\nOut[10]: \n     A          B    C  D      E    F\n0  1.0 2013-01-02  1.0  3   test  foo\n1  1.0 2013-01-02  1.0  3  train  foo\n2  1.0 2013-01-02  1.0  3   test  foo\n3  1.0 2013-01-02  1.0  3  train  foo", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "228": {"Title": "Object Creation", "Text": "The columns of the resulting DataFrame have different dtypes.", "Code_snippet": "In [11]: df2.dtypes\nOut[11]: \nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "229": {"Title": "Object Creation", "Text": "If you\u2019re using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here\u2019s a subset of the attributes that will be completed:", "Code_snippet": "In [12]: df2.<TAB>  # noqa: E225, E999\ndf2.A                  df2.bool\ndf2.abs                df2.boxplot\ndf2.add                df2.C\ndf2.add_prefix         df2.clip\ndf2.add_suffix         df2.clip_lower\ndf2.align              df2.clip_upper\ndf2.all                df2.columns\ndf2.any                df2.combine\ndf2.append             df2.combine_first\ndf2.apply              df2.compound\ndf2.applymap           df2.consolidate\ndf2.D", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "230": {"Title": "Viewing Data", "Text": "See the Basics section.Here is how to view the top and bottom rows of the frame:", "Code_snippet": "In [13]: df.head()\nOut[13]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n\nIn [14]: df.tail(3)\nOut[14]: \n                   A         B         C         D\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "231": {"Title": "Viewing Data", "Text": "Display the index, columns:", "Code_snippet": "In [15]: df.index\nOut[15]: \nDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n               '2013-01-05', '2013-01-06'],\n              dtype='datetime64[ns]', freq='D')\n\nIn [16]: df.columns\nOut[16]: Index(['A', 'B', 'C', 'D'], dtype='object')", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "232": {"Title": "Viewing Data", "Text": "DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that his can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental difference between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object.For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn\u2019t require copying data.", "Code_snippet": "In [17]: df.to_numpy()\nOut[17]: \narray([[ 0.4691, -0.2829, -1.5091, -1.1356],\n       [ 1.2121, -0.1732,  0.1192, -1.0442],\n       [-0.8618, -2.1046, -0.4949,  1.0718],\n       [ 0.7216, -0.7068, -1.0396,  0.2719],\n       [-0.425 ,  0.567 ,  0.2762, -1.0874],\n       [-0.6737,  0.1136, -1.4784,  0.525 ]])", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "233": {"Title": "Viewing Data", "Text": "For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive.", "Code_snippet": "In [18]: df2.to_numpy()\nOut[18]: \narray([[1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo'],\n       [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],\n       [1.0, Timestamp('2013-01-02 00:00:00'), 1.0, 3, 'train', 'foo']], dtype=object)", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "234": {"Title": "Viewing Data", "Text": "NoteDataFrame.to_numpy() does not include the index or column labels in the output.describe() shows a quick statistic summary of your data:", "Code_snippet": "In [19]: df.describe()\nOut[19]: \n              A         B         C         D\ncount  6.000000  6.000000  6.000000  6.000000\nmean   0.073711 -0.431125 -0.687758 -0.233103\nstd    0.843157  0.922818  0.779887  0.973118\nmin   -0.861849 -2.104569 -1.509059 -1.135632\n25%   -0.611510 -0.600794 -1.368714 -1.076610\n50%    0.022070 -0.228039 -0.767252 -0.386188\n75%    0.658444  0.041933 -0.034326  0.461706\nmax    1.212112  0.567020  0.276232  1.071804", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "235": {"Title": "Viewing Data", "Text": "Transposing your data:", "Code_snippet": "In [20]: df.T\nOut[20]: \n   2013-01-01  2013-01-02  2013-01-03  2013-01-04  2013-01-05  2013-01-06\nA    0.469112    1.212112   -0.861849    0.721555   -0.424972   -0.673690\nB   -0.282863   -0.173215   -2.104569   -0.706771    0.567020    0.113648\nC   -1.509059    0.119209   -0.494929   -1.039575    0.276232   -1.478427\nD   -1.135632   -1.044236    1.071804    0.271860   -1.087401    0.524988", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "236": {"Title": "Viewing Data", "Text": "Sorting by an axis:", "Code_snippet": "In [21]: df.sort_index(axis=1, ascending=False)\nOut[21]: \n                   D         C         B         A\n2013-01-01 -1.135632 -1.509059 -0.282863  0.469112\n2013-01-02 -1.044236  0.119209 -0.173215  1.212112\n2013-01-03  1.071804 -0.494929 -2.104569 -0.861849\n2013-01-04  0.271860 -1.039575 -0.706771  0.721555\n2013-01-05 -1.087401  0.276232  0.567020 -0.424972\n2013-01-06  0.524988 -1.478427  0.113648 -0.673690", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "237": {"Title": "Viewing Data", "Text": "Sorting by values:", "Code_snippet": "In [22]: df.sort_values(by='B')\nOut[22]: \n                   A         B         C         D\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "238": {"Title": "Selection", "Text": "NoteWhile standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.See the indexing documentation Indexing and Selecting Data and MultiIndex / Advanced Indexing.Selecting a single column, which yields a Series, equivalent to df.A:", "Code_snippet": "In [23]: df['A']\nOut[23]: \n2013-01-01    0.469112\n2013-01-02    1.212112\n2013-01-03   -0.861849\n2013-01-04    0.721555\n2013-01-05   -0.424972\n2013-01-06   -0.673690\nFreq: D, Name: A, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "239": {"Title": "Selection", "Text": "Selecting via [], which slices the rows.", "Code_snippet": "In [24]: df[0:3]\nOut[24]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n\nIn [25]: df['20130102':'20130104']\nOut[25]: \n                   A         B         C         D\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "240": {"Title": "Selection", "Text": "See more in Selection by Label.For getting a cross section using a label:", "Code_snippet": "In [26]: df.loc[dates[0]]\nOut[26]: \nA    0.469112\nB   -0.282863\nC   -1.509059\nD   -1.135632\nName: 2013-01-01 00:00:00, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "241": {"Title": "Selection", "Text": "Selecting on a multi-axis by label:", "Code_snippet": "In [27]: df.loc[:, ['A', 'B']]\nOut[27]: \n                   A         B\n2013-01-01  0.469112 -0.282863\n2013-01-02  1.212112 -0.173215\n2013-01-03 -0.861849 -2.104569\n2013-01-04  0.721555 -0.706771\n2013-01-05 -0.424972  0.567020\n2013-01-06 -0.673690  0.113648", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "242": {"Title": "Selection", "Text": "Showing label slicing, both endpoints are included:", "Code_snippet": "In [28]: df.loc['20130102':'20130104', ['A', 'B']]\nOut[28]: \n                   A         B\n2013-01-02  1.212112 -0.173215\n2013-01-03 -0.861849 -2.104569\n2013-01-04  0.721555 -0.706771", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "243": {"Title": "Selection", "Text": "Reduction in the dimensions of the returned object:", "Code_snippet": "In [29]: df.loc['20130102', ['A', 'B']]\nOut[29]: \nA    1.212112\nB   -0.173215\nName: 2013-01-02 00:00:00, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "244": {"Title": "Selection", "Text": "For getting a scalar value:", "Code_snippet": "In [30]: df.loc[dates[0], 'A']\nOut[30]: 0.46911229990718628", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "245": {"Title": "Selection", "Text": "For getting fast access to a scalar (equivalent to the prior method):", "Code_snippet": "In [31]: df.at[dates[0], 'A']\nOut[31]: 0.46911229990718628", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "246": {"Title": "Selection", "Text": "See more in Selection by Position.Select via the position of the passed integers:", "Code_snippet": "In [32]: df.iloc[3]\nOut[32]: \nA    0.721555\nB   -0.706771\nC   -1.039575\nD    0.271860\nName: 2013-01-04 00:00:00, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "247": {"Title": "Selection", "Text": "By integer slices, acting similar to numpy/python:", "Code_snippet": "In [33]: df.iloc[3:5, 0:2]\nOut[33]: \n                   A         B\n2013-01-04  0.721555 -0.706771\n2013-01-05 -0.424972  0.567020", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "248": {"Title": "Selection", "Text": "By lists of integer position locations, similar to the numpy/python style:", "Code_snippet": "In [34]: df.iloc[[1, 2, 4], [0, 2]]\nOut[34]: \n                   A         C\n2013-01-02  1.212112  0.119209\n2013-01-03 -0.861849 -0.494929\n2013-01-05 -0.424972  0.276232", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "249": {"Title": "Selection", "Text": "For slicing rows explicitly:", "Code_snippet": "In [35]: df.iloc[1:3, :]\nOut[35]: \n                   A         B         C         D\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "250": {"Title": "Selection", "Text": "For slicing columns explicitly:", "Code_snippet": "In [36]: df.iloc[:, 1:3]\nOut[36]: \n                   B         C\n2013-01-01 -0.282863 -1.509059\n2013-01-02 -0.173215  0.119209\n2013-01-03 -2.104569 -0.494929\n2013-01-04 -0.706771 -1.039575\n2013-01-05  0.567020  0.276232\n2013-01-06  0.113648 -1.478427", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "251": {"Title": "Selection", "Text": "For getting a value explicitly:", "Code_snippet": "In [37]: df.iloc[1, 1]\nOut[37]: -0.17321464905330858", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "252": {"Title": "Selection", "Text": "For getting fast access to a scalar (equivalent to the prior method):", "Code_snippet": "In [38]: df.iat[1, 1]\nOut[38]: -0.17321464905330858", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "253": {"Title": "Selection", "Text": "Using a single column\u2019s values to select data.", "Code_snippet": "In [39]: df[df.A > 0]\nOut[39]: \n                   A         B         C         D\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "254": {"Title": "Selection", "Text": "Selecting values from a DataFrame where a boolean condition is met.", "Code_snippet": "In [40]: df[df > 0]\nOut[40]: \n                   A         B         C         D\n2013-01-01  0.469112       NaN       NaN       NaN\n2013-01-02  1.212112       NaN  0.119209       NaN\n2013-01-03       NaN       NaN       NaN  1.071804\n2013-01-04  0.721555       NaN       NaN  0.271860\n2013-01-05       NaN  0.567020  0.276232       NaN\n2013-01-06       NaN  0.113648       NaN  0.524988", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "255": {"Title": "Selection", "Text": "Using the isin() method for filtering:", "Code_snippet": "In [41]: df2 = df.copy()\n\nIn [42]: df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']\n\nIn [43]: df2\nOut[43]: \n                   A         B         C         D      E\n2013-01-01  0.469112 -0.282863 -1.509059 -1.135632    one\n2013-01-02  1.212112 -0.173215  0.119209 -1.044236    one\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804    two\n2013-01-04  0.721555 -0.706771 -1.039575  0.271860  three\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401   four\n2013-01-06 -0.673690  0.113648 -1.478427  0.524988  three\n\nIn [44]: df2[df2['E'].isin(['two', 'four'])]\nOut[44]: \n                   A         B         C         D     E\n2013-01-03 -0.861849 -2.104569 -0.494929  1.071804   two\n2013-01-05 -0.424972  0.567020  0.276232 -1.087401  four", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "256": {"Title": "Selection", "Text": "Setting a new column automatically aligns the data by the indexes.", "Code_snippet": "In [45]: s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range('20130102', periods=6))\n\nIn [46]: s1\nOut[46]: \n2013-01-02    1\n2013-01-03    2\n2013-01-04    3\n2013-01-05    4\n2013-01-06    5\n2013-01-07    6\nFreq: D, dtype: int64\n\nIn [47]: df['F'] = s1", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "257": {"Title": "Selection", "Text": "Setting values by label:", "Code_snippet": "In [48]: df.at[dates[0], 'A'] = 0", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "258": {"Title": "Selection", "Text": "Setting values by position:", "Code_snippet": "In [49]: df.iat[0, 1] = 0", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "259": {"Title": "Selection", "Text": "Setting by assigning with a NumPy array:", "Code_snippet": "In [50]: df.loc[:, 'D'] = np.array([5] * len(df))", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "260": {"Title": "Selection", "Text": "The result of the prior setting operations.", "Code_snippet": "In [51]: df\nOut[51]: \n                   A         B         C  D    F\n2013-01-01  0.000000  0.000000 -1.509059  5  NaN\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0\n2013-01-03 -0.861849 -2.104569 -0.494929  5  2.0\n2013-01-04  0.721555 -0.706771 -1.039575  5  3.0\n2013-01-05 -0.424972  0.567020  0.276232  5  4.0\n2013-01-06 -0.673690  0.113648 -1.478427  5  5.0", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "261": {"Title": "Selection", "Text": "A where operation with setting.", "Code_snippet": "In [52]: df2 = df.copy()\n\nIn [53]: df2[df2 > 0] = -df2\n\nIn [54]: df2\nOut[54]: \n                   A         B         C  D    F\n2013-01-01  0.000000  0.000000 -1.509059 -5  NaN\n2013-01-02 -1.212112 -0.173215 -0.119209 -5 -1.0\n2013-01-03 -0.861849 -2.104569 -0.494929 -5 -2.0\n2013-01-04 -0.721555 -0.706771 -1.039575 -5 -3.0\n2013-01-05 -0.424972 -0.567020 -0.276232 -5 -4.0\n2013-01-06 -0.673690 -0.113648 -1.478427 -5 -5.0", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "262": {"Title": "Missing Data", "Text": "pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section.Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.", "Code_snippet": "In [55]: df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])\n\nIn [56]: df1.loc[dates[0]:dates[1], 'E'] = 1\n\nIn [57]: df1\nOut[57]: \n                   A         B         C  D    F    E\n2013-01-01  0.000000  0.000000 -1.509059  5  NaN  1.0\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0  1.0\n2013-01-03 -0.861849 -2.104569 -0.494929  5  2.0  NaN\n2013-01-04  0.721555 -0.706771 -1.039575  5  3.0  NaN", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "263": {"Title": "Missing Data", "Text": "To drop any rows that have missing data.", "Code_snippet": "In [58]: df1.dropna(how='any')\nOut[58]: \n                   A         B         C  D    F    E\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0  1.0", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "264": {"Title": "Missing Data", "Text": "Filling missing data.", "Code_snippet": "In [59]: df1.fillna(value=5)\nOut[59]: \n                   A         B         C  D    F    E\n2013-01-01  0.000000  0.000000 -1.509059  5  5.0  1.0\n2013-01-02  1.212112 -0.173215  0.119209  5  1.0  1.0\n2013-01-03 -0.861849 -2.104569 -0.494929  5  2.0  5.0\n2013-01-04  0.721555 -0.706771 -1.039575  5  3.0  5.0", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "265": {"Title": "Missing Data", "Text": "To get the boolean mask where values are nan.", "Code_snippet": "In [60]: pd.isna(df1)\nOut[60]: \n                A      B      C      D      F      E\n2013-01-01  False  False  False  False   True  False\n2013-01-02  False  False  False  False  False  False\n2013-01-03  False  False  False  False  False   True\n2013-01-04  False  False  False  False  False   True", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "266": {"Title": "Operations", "Text": "See the Basic section on Binary Ops.Operations in general exclude missing data.Performing a descriptive statistic:", "Code_snippet": "In [61]: df.mean()\nOut[61]: \nA   -0.004474\nB   -0.383981\nC   -0.687758\nD    5.000000\nF    3.000000\ndtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "267": {"Title": "Operations", "Text": "Same operation on the other axis:", "Code_snippet": "In [62]: df.mean(1)\nOut[62]: \n2013-01-01    0.872735\n2013-01-02    1.431621\n2013-01-03    0.707731\n2013-01-04    1.395042\n2013-01-05    1.883656\n2013-01-06    1.592306\nFreq: D, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "268": {"Title": "Operations", "Text": "Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension.", "Code_snippet": "In [63]: s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\n\nIn [64]: s\nOut[64]: \n2013-01-01    NaN\n2013-01-02    NaN\n2013-01-03    1.0\n2013-01-04    3.0\n2013-01-05    5.0\n2013-01-06    NaN\nFreq: D, dtype: float64\n\nIn [65]: df.sub(s, axis='index')\nOut[65]: \n                   A         B         C    D    F\n2013-01-01       NaN       NaN       NaN  NaN  NaN\n2013-01-02       NaN       NaN       NaN  NaN  NaN\n2013-01-03 -1.861849 -3.104569 -1.494929  4.0  1.0\n2013-01-04 -2.278445 -3.706771 -4.039575  2.0  0.0\n2013-01-05 -5.424972 -4.432980 -4.723768  0.0 -1.0\n2013-01-06       NaN       NaN       NaN  NaN  NaN", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "269": {"Title": "Operations", "Text": "Applying functions to the data:", "Code_snippet": "In [66]: df.apply(np.cumsum)\nOut[66]: \n                   A         B         C   D     F\n2013-01-01  0.000000  0.000000 -1.509059   5   NaN\n2013-01-02  1.212112 -0.173215 -1.389850  10   1.0\n2013-01-03  0.350263 -2.277784 -1.884779  15   3.0\n2013-01-04  1.071818 -2.984555 -2.924354  20   6.0\n2013-01-05  0.646846 -2.417535 -2.648122  25  10.0\n2013-01-06 -0.026844 -2.303886 -4.126549  30  15.0\n\nIn [67]: df.apply(lambda x: x.max() - x.min())\nOut[67]: \nA    2.073961\nB    2.671590\nC    1.785291\nD    0.000000\nF    4.000000\ndtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "270": {"Title": "Operations", "Text": "See more at Histogramming and Discretization.", "Code_snippet": "In [68]: s = pd.Series(np.random.randint(0, 7, size=10))\n\nIn [69]: s\nOut[69]: \n0    4\n1    2\n2    1\n3    2\n4    6\n5    4\n6    4\n7    6\n8    4\n9    4\ndtype: int64\n\nIn [70]: s.value_counts()\nOut[70]: \n4    5\n6    2\n2    2\n1    1\ndtype: int64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "271": {"Title": "Operations", "Text": "Series is equipped with a set of string processing methods in the str attribute that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them). See more at Vectorized String Methods.", "Code_snippet": "In [71]: s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])\n\nIn [72]: s.str.lower()\nOut[72]: \n0       a\n1       b\n2       c\n3    aaba\n4    baca\n5     NaN\n6    caba\n7     dog\n8     cat\ndtype: object", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "272": {"Title": "Merge", "Text": "pandas provides various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.See the Merging section.Concatenating pandas objects together with concat():", "Code_snippet": "In [73]: df = pd.DataFrame(np.random.randn(10, 4))\n\nIn [74]: df\nOut[74]: \n          0         1         2         3\n0 -0.548702  1.467327 -1.015962 -0.483075\n1  1.637550 -1.217659 -0.291519 -1.745505\n2 -0.263952  0.991460 -0.919069  0.266046\n3 -0.709661  1.669052  1.037882 -1.705775\n4 -0.919854 -0.042379  1.247642 -0.009920\n5  0.290213  0.495767  0.362949  1.548106\n6 -1.131345 -0.089329  0.337863 -0.945867\n7 -0.932132  1.956030  0.017587 -0.016692\n8 -0.575247  0.254161 -1.143704  0.215897\n9  1.193555 -0.077118 -0.408530 -0.862495\n\n# break it into pieces\nIn [75]: pieces = [df[:3], df[3:7], df[7:]]\n\nIn [76]: pd.concat(pieces)\nOut[76]: \n          0         1         2         3\n0 -0.548702  1.467327 -1.015962 -0.483075\n1  1.637550 -1.217659 -0.291519 -1.745505\n2 -0.263952  0.991460 -0.919069  0.266046\n3 -0.709661  1.669052  1.037882 -1.705775\n4 -0.919854 -0.042379  1.247642 -0.009920\n5  0.290213  0.495767  0.362949  1.548106\n6 -1.131345 -0.089329  0.337863 -0.945867\n7 -0.932132  1.956030  0.017587 -0.016692\n8 -0.575247  0.254161 -1.143704  0.215897\n9  1.193555 -0.077118 -0.408530 -0.862495", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "273": {"Title": "Merge", "Text": "SQL style merges. See the Database style joining section.", "Code_snippet": "In [77]: left = pd.DataFrame({'key': ['foo', 'foo'], 'lval': [1, 2]})\n\nIn [78]: right = pd.DataFrame({'key': ['foo', 'foo'], 'rval': [4, 5]})\n\nIn [79]: left\nOut[79]: \n   key  lval\n0  foo     1\n1  foo     2\n\nIn [80]: right\nOut[80]: \n   key  rval\n0  foo     4\n1  foo     5\n\nIn [81]: pd.merge(left, right, on='key')\nOut[81]: \n   key  lval  rval\n0  foo     1     4\n1  foo     1     5\n2  foo     2     4\n3  foo     2     5", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "274": {"Title": "Merge", "Text": "Another example that can be given is:", "Code_snippet": "In [82]: left = pd.DataFrame({'key': ['foo', 'bar'], 'lval': [1, 2]})\n\nIn [83]: right = pd.DataFrame({'key': ['foo', 'bar'], 'rval': [4, 5]})\n\nIn [84]: left\nOut[84]: \n   key  lval\n0  foo     1\n1  bar     2\n\nIn [85]: right\nOut[85]: \n   key  rval\n0  foo     4\n1  bar     5\n\nIn [86]: pd.merge(left, right, on='key')\nOut[86]: \n   key  lval  rval\n0  foo     1     4\n1  bar     2     5", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "275": {"Title": "Merge", "Text": "Append rows to a dataframe. See the Appending section.", "Code_snippet": "In [87]: df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])\n\nIn [88]: df\nOut[88]: \n          A         B         C         D\n0  1.346061  1.511763  1.627081 -0.990582\n1 -0.441652  1.211526  0.268520  0.024580\n2 -1.577585  0.396823 -0.105381 -0.532532\n3  1.453749  1.208843 -0.080952 -0.264610\n4 -0.727965 -0.589346  0.339969 -0.693205\n5 -0.339355  0.593616  0.884345  1.591431\n6  0.141809  0.220390  0.435589  0.192451\n7 -0.096701  0.803351  1.715071 -0.708758\n\nIn [89]: s = df.iloc[3]\n\nIn [90]: df.append(s, ignore_index=True)\nOut[90]: \n          A         B         C         D\n0  1.346061  1.511763  1.627081 -0.990582\n1 -0.441652  1.211526  0.268520  0.024580\n2 -1.577585  0.396823 -0.105381 -0.532532\n3  1.453749  1.208843 -0.080952 -0.264610\n4 -0.727965 -0.589346  0.339969 -0.693205\n5 -0.339355  0.593616  0.884345  1.591431\n6  0.141809  0.220390  0.435589  0.192451\n7 -0.096701  0.803351  1.715071 -0.708758\n8  1.453749  1.208843 -0.080952 -0.264610", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "276": {"Title": "Grouping", "Text": "By \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:See the Grouping section.", "Code_snippet": "In [91]: df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n   ....:                          'foo', 'bar', 'foo', 'foo'],\n   ....:                    'B': ['one', 'one', 'two', 'three',\n   ....:                          'two', 'two', 'one', 'three'],\n   ....:                    'C': np.random.randn(8),\n   ....:                    'D': np.random.randn(8)})\n   ....: \n\nIn [92]: df\nOut[92]: \n     A      B         C         D\n0  foo    one -1.202872 -0.055224\n1  bar    one -1.814470  2.395985\n2  foo    two  1.018601  1.552825\n3  bar  three -0.595447  0.166599\n4  foo    two  1.395433  0.047609\n5  bar    two -0.392670 -0.136473\n6  foo    one  0.007207 -0.561757\n7  foo  three  1.928123 -1.623033", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "277": {"Title": "Grouping", "Text": "Grouping and then applying the sum() function to the resulting groups.", "Code_snippet": "In [93]: df.groupby('A').sum()\nOut[93]: \n            C        D\nA                     \nbar -2.802588  2.42611\nfoo  3.146492 -0.63958", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "278": {"Title": "Grouping", "Text": "Grouping by multiple columns forms a hierarchical index, and again we can apply the sum function.", "Code_snippet": "In [94]: df.groupby(['A', 'B']).sum()\nOut[94]: \n                  C         D\nA   B                        \nbar one   -1.814470  2.395985\n    three -0.595447  0.166599\n    two   -0.392670 -0.136473\nfoo one   -1.195665 -0.616981\n    three  1.928123 -1.623033\n    two    2.414034  1.600434", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "279": {"Title": "Reshaping", "Text": "See the sections on Hierarchical Indexing and Reshaping.", "Code_snippet": "In [95]: tuples = list(zip(*[['bar', 'bar', 'baz', 'baz',\n   ....:                      'foo', 'foo', 'qux', 'qux'],\n   ....:                     ['one', 'two', 'one', 'two',\n   ....:                      'one', 'two', 'one', 'two']]))\n   ....: \n\nIn [96]: index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n\nIn [97]: df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])\n\nIn [98]: df2 = df[:4]\n\nIn [99]: df2\nOut[99]: \n                     A         B\nfirst second                    \nbar   one     0.029399 -0.542108\n      two     0.282696 -0.087302\nbaz   one    -1.575170  1.771208\n      two     0.816482  1.100230", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "280": {"Title": "Reshaping", "Text": "The stack() method \u201ccompresses\u201d a level in the DataFrame\u2019s columns.", "Code_snippet": "In [100]: stacked = df2.stack()\n\nIn [101]: stacked\nOut[101]: \nfirst  second   \nbar    one     A    0.029399\n               B   -0.542108\n       two     A    0.282696\n               B   -0.087302\nbaz    one     A   -1.575170\n               B    1.771208\n       two     A    0.816482\n               B    1.100230\ndtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "281": {"Title": "Reshaping", "Text": "With a \u201cstacked\u201d DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:", "Code_snippet": "In [102]: stacked.unstack()\nOut[102]: \n                     A         B\nfirst second                    \nbar   one     0.029399 -0.542108\n      two     0.282696 -0.087302\nbaz   one    -1.575170  1.771208\n      two     0.816482  1.100230\n\nIn [103]: stacked.unstack(1)\nOut[103]: \nsecond        one       two\nfirst                      \nbar   A  0.029399  0.282696\n      B -0.542108 -0.087302\nbaz   A -1.575170  0.816482\n      B  1.771208  1.100230\n\nIn [104]: stacked.unstack(0)\nOut[104]: \nfirst          bar       baz\nsecond                      \none    A  0.029399 -1.575170\n       B -0.542108  1.771208\ntwo    A  0.282696  0.816482\n       B -0.087302  1.100230", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "282": {"Title": "Reshaping", "Text": "See the section on Pivot Tables.", "Code_snippet": "In [105]: df = pd.DataFrame({'A': ['one', 'one', 'two', 'three'] * 3,\n   .....:                    'B': ['A', 'B', 'C'] * 4,\n   .....:                    'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,\n   .....:                    'D': np.random.randn(12),\n   .....:                    'E': np.random.randn(12)})\n   .....: \n\nIn [106]: df\nOut[106]: \n        A  B    C         D         E\n0     one  A  foo  1.418757 -0.179666\n1     one  B  foo -1.879024  1.291836\n2     two  C  foo  0.536826 -0.009614\n3   three  A  bar  1.006160  0.392149\n4     one  B  bar -0.029716  0.264599\n5     one  C  bar -1.146178 -0.057409\n6     two  A  foo  0.100900 -1.425638\n7   three  B  foo -1.035018  1.024098\n8     one  C  foo  0.314665 -0.106062\n9     one  A  bar -0.773723  1.824375\n10    two  B  bar -1.170653  0.595974\n11  three  C  bar  0.648740  1.167115", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "283": {"Title": "Reshaping", "Text": "We can produce pivot tables from this data very easily:", "Code_snippet": "In [107]: pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])\nOut[107]: \nC             bar       foo\nA     B                    \none   A -0.773723  1.418757\n      B -0.029716 -1.879024\n      C -1.146178  0.314665\nthree A  1.006160       NaN\n      B       NaN -1.035018\n      C  0.648740       NaN\ntwo   A       NaN  0.100900\n      B -1.170653       NaN\n      C       NaN  0.536826", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "284": {"Title": "Time Series", "Text": "pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications. See the Time Series section.", "Code_snippet": "In [108]: rng = pd.date_range('1/1/2012', periods=100, freq='S')\n\nIn [109]: ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n\nIn [110]: ts.resample('5Min').sum()\nOut[110]: \n2012-01-01    25083\nFreq: 5T, dtype: int64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "285": {"Title": "Time Series", "Text": "Time zone representation:", "Code_snippet": "In [111]: rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')\n\nIn [112]: ts = pd.Series(np.random.randn(len(rng)), rng)\n\nIn [113]: ts\nOut[113]: \n2012-03-06    0.464000\n2012-03-07    0.227371\n2012-03-08   -0.496922\n2012-03-09    0.306389\n2012-03-10   -2.290613\nFreq: D, dtype: float64\n\nIn [114]: ts_utc = ts.tz_localize('UTC')\n\nIn [115]: ts_utc\nOut[115]: \n2012-03-06 00:00:00+00:00    0.464000\n2012-03-07 00:00:00+00:00    0.227371\n2012-03-08 00:00:00+00:00   -0.496922\n2012-03-09 00:00:00+00:00    0.306389\n2012-03-10 00:00:00+00:00   -2.290613\nFreq: D, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "286": {"Title": "Time Series", "Text": "Converting to another time zone:", "Code_snippet": "In [116]: ts_utc.tz_convert('US/Eastern')\nOut[116]: \n2012-03-05 19:00:00-05:00    0.464000\n2012-03-06 19:00:00-05:00    0.227371\n2012-03-07 19:00:00-05:00   -0.496922\n2012-03-08 19:00:00-05:00    0.306389\n2012-03-09 19:00:00-05:00   -2.290613\nFreq: D, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "287": {"Title": "Time Series", "Text": "Converting between time span representations:", "Code_snippet": "In [117]: rng = pd.date_range('1/1/2012', periods=5, freq='M')\n\nIn [118]: ts = pd.Series(np.random.randn(len(rng)), index=rng)\n\nIn [119]: ts\nOut[119]: \n2012-01-31   -1.134623\n2012-02-29   -1.561819\n2012-03-31   -0.260838\n2012-04-30    0.281957\n2012-05-31    1.523962\nFreq: M, dtype: float64\n\nIn [120]: ps = ts.to_period()\n\nIn [121]: ps\nOut[121]: \n2012-01   -1.134623\n2012-02   -1.561819\n2012-03   -0.260838\n2012-04    0.281957\n2012-05    1.523962\nFreq: M, dtype: float64\n\nIn [122]: ps.to_timestamp()\nOut[122]: \n2012-01-01   -1.134623\n2012-02-01   -1.561819\n2012-03-01   -0.260838\n2012-04-01    0.281957\n2012-05-01    1.523962\nFreq: MS, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "288": {"Title": "Time Series", "Text": "Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:", "Code_snippet": "In [123]: prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')\n\nIn [124]: ts = pd.Series(np.random.randn(len(prng)), prng)\n\nIn [125]: ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9\n\nIn [126]: ts.head()\nOut[126]: \n1990-03-01 09:00   -0.902937\n1990-06-01 09:00    0.068159\n1990-09-01 09:00   -0.057873\n1990-12-01 09:00   -0.368204\n1991-03-01 09:00   -1.144073\nFreq: H, dtype: float64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "289": {"Title": "Categoricals", "Text": "pandas can include categorical data in a DataFrame. For full docs, see the categorical introduction and the API documentation.", "Code_snippet": "In [127]: df = pd.DataFrame({\"id\": [1, 2, 3, 4, 5, 6],\n   .....:                    \"raw_grade\": ['a', 'b', 'b', 'a', 'a', 'e']})\n   .....: ", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "290": {"Title": "Categoricals", "Text": "Convert the raw grades to a categorical data type.", "Code_snippet": "In [128]: df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\n\nIn [129]: df[\"grade\"]\nOut[129]: \n0    a\n1    b\n2    b\n3    a\n4    a\n5    e\nName: grade, dtype: category\nCategories (3, object): [a, b, e]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "291": {"Title": "Categoricals", "Text": "Rename the categories to more meaningful names (assigning to Series.cat.categories is inplace!).", "Code_snippet": "In [130]: df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "292": {"Title": "Categoricals", "Text": "Reorder the categories and simultaneously add the missing categories (methods under Series .cat return a new Series by default).", "Code_snippet": "In [131]: df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\",\n   .....:                                               \"good\", \"very good\"])\n   .....: \n\nIn [132]: df[\"grade\"]\nOut[132]: \n0    very good\n1         good\n2         good\n3    very good\n4    very good\n5     very bad\nName: grade, dtype: category\nCategories (5, object): [very bad, bad, medium, good, very good]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "293": {"Title": "Categoricals", "Text": "Sorting is per order in the categories, not lexical order.", "Code_snippet": "In [133]: df.sort_values(by=\"grade\")\nOut[133]: \n   id raw_grade      grade\n5   6         e   very bad\n1   2         b       good\n2   3         b       good\n0   1         a  very good\n3   4         a  very good\n4   5         a  very good", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "294": {"Title": "Categoricals", "Text": "Grouping by a categorical column also shows empty categories.", "Code_snippet": "In [134]: df.groupby(\"grade\").size()\nOut[134]: \ngrade\nvery bad     1\nbad          0\nmedium       0\ngood         2\nvery good    3\ndtype: int64", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "295": {"Title": "Plotting", "Text": "See the Plotting docs.", "Code_snippet": "In [135]: ts = pd.Series(np.random.randn(1000),\n   .....:                index=pd.date_range('1/1/2000', periods=1000))\n   .....: \n\nIn [136]: ts = ts.cumsum()\n\nIn [137]: ts.plot()\nOut[137]: <matplotlib.axes._subplots.AxesSubplot at 0x7f2b5771ac88>", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "296": {"Title": "Plotting", "Text": "On a DataFrame, the plot() method is a convenience to plot all of the columns with labels:", "Code_snippet": "In [138]: df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index,\n   .....:                   columns=['A', 'B', 'C', 'D'])\n   .....: \n\nIn [139]: df = df.cumsum()\n\nIn [140]: plt.figure()\nOut[140]: <Figure size 640x480 with 0 Axes>\n\nIn [141]: df.plot()\nOut[141]: <matplotlib.axes._subplots.AxesSubplot at 0x7f2b53a2d7f0>\n\nIn [142]: plt.legend(loc='best')\nOut[142]: <matplotlib.legend.Legend at 0x7f2b539728d0>", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": ["https://pandas.pydata.org/pandas-docs/stable/_images/series_plot_basic.png"]}, "297": {"Title": "Getting Data In/Out", "Text": "Writing to a csv file.", "Code_snippet": "In [143]: df.to_csv('foo.csv')", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "298": {"Title": "Getting Data In/Out", "Text": "Reading from a csv file.", "Code_snippet": "In [144]: pd.read_csv('foo.csv')\nOut[144]: \n     Unnamed: 0          A          B         C          D\n0    2000-01-01   0.266457  -0.399641 -0.219582   1.186860\n1    2000-01-02  -1.170732  -0.345873  1.653061  -0.282953\n2    2000-01-03  -1.734933   0.530468  2.060811  -0.515536\n3    2000-01-04  -1.555121   1.452620  0.239859  -1.156896\n4    2000-01-05   0.578117   0.511371  0.103552  -2.428202\n5    2000-01-06   0.478344   0.449933 -0.741620  -1.962409\n6    2000-01-07   1.235339  -0.091757 -1.543861  -1.084753\n..          ...        ...        ...       ...        ...\n993  2002-09-20 -10.628548  -9.153563 -7.883146  28.313940\n994  2002-09-21 -10.390377  -8.727491 -6.399645  30.914107\n995  2002-09-22  -8.985362  -8.485624 -4.669462  31.367740\n996  2002-09-23  -9.558560  -8.781216 -4.499815  30.518439\n997  2002-09-24  -9.902058  -9.340490 -4.386639  30.105593\n998  2002-09-25 -10.216020  -9.480682 -3.933802  29.758560\n999  2002-09-26 -11.856774 -10.671012 -3.216025  29.369368\n\n[1000 rows x 5 columns]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "299": {"Title": "Getting Data In/Out", "Text": "Reading and writing to HDFStores.Writing to a HDF5 Store.", "Code_snippet": "In [145]: df.to_hdf('foo.h5', 'df')", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "300": {"Title": "Getting Data In/Out", "Text": "Reading from a HDF5 Store.", "Code_snippet": "In [146]: pd.read_hdf('foo.h5', 'df')\nOut[146]: \n                    A          B         C          D\n2000-01-01   0.266457  -0.399641 -0.219582   1.186860\n2000-01-02  -1.170732  -0.345873  1.653061  -0.282953\n2000-01-03  -1.734933   0.530468  2.060811  -0.515536\n2000-01-04  -1.555121   1.452620  0.239859  -1.156896\n2000-01-05   0.578117   0.511371  0.103552  -2.428202\n2000-01-06   0.478344   0.449933 -0.741620  -1.962409\n2000-01-07   1.235339  -0.091757 -1.543861  -1.084753\n...               ...        ...       ...        ...\n2002-09-20 -10.628548  -9.153563 -7.883146  28.313940\n2002-09-21 -10.390377  -8.727491 -6.399645  30.914107\n2002-09-22  -8.985362  -8.485624 -4.669462  31.367740\n2002-09-23  -9.558560  -8.781216 -4.499815  30.518439\n2002-09-24  -9.902058  -9.340490 -4.386639  30.105593\n2002-09-25 -10.216020  -9.480682 -3.933802  29.758560\n2002-09-26 -11.856774 -10.671012 -3.216025  29.369368\n\n[1000 rows x 4 columns]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "301": {"Title": "Getting Data In/Out", "Text": "Reading and writing to MS Excel.Writing to an excel file.", "Code_snippet": "In [147]: df.to_excel('foo.xlsx', sheet_name='Sheet1')", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}, "302": {"Title": "Getting Data In/Out", "Text": "Reading from an excel file.", "Code_snippet": "In [148]: pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])\nOut[148]: \n    Unnamed: 0          A          B         C          D\n0   2000-01-01   0.266457  -0.399641 -0.219582   1.186860\n1   2000-01-02  -1.170732  -0.345873  1.653061  -0.282953\n2   2000-01-03  -1.734933   0.530468  2.060811  -0.515536\n3   2000-01-04  -1.555121   1.452620  0.239859  -1.156896\n4   2000-01-05   0.578117   0.511371  0.103552  -2.428202\n5   2000-01-06   0.478344   0.449933 -0.741620  -1.962409\n6   2000-01-07   1.235339  -0.091757 -1.543861  -1.084753\n..         ...        ...        ...       ...        ...\n993 2002-09-20 -10.628548  -9.153563 -7.883146  28.313940\n994 2002-09-21 -10.390377  -8.727491 -6.399645  30.914107\n995 2002-09-22  -8.985362  -8.485624 -4.669462  31.367740\n996 2002-09-23  -9.558560  -8.781216 -4.499815  30.518439\n997 2002-09-24  -9.902058  -9.340490 -4.386639  30.105593\n998 2002-09-25 -10.216020  -9.480682 -3.933802  29.758560\n999 2002-09-26 -11.856774 -10.671012 -3.216025  29.369368\n\n[1000 rows x 5 columns]", "Url": "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html", "Attachment_Url": []}}